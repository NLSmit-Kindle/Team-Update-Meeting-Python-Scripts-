{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e90cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta, date\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gspread\n",
    "from sqlalchemy import create_engine\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload, HttpRequest\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10283b70",
   "metadata": {},
   "source": [
    "# SCRIPT DIRECTORY MADE DYNAMIC \n",
    "extrapolated to other sections since folders created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60229be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicola\\repositories & enviroments\\Python\\Automation\\Weekly Team Update Meeting\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.getcwd()\n",
    "print(script_dir)\n",
    "\n",
    "#background_data_dir = script_dir + '\\\\' + 'Background Data Folder'\n",
    "\"\"\"os.path.join adds the appropriate path separator (\\ or /) based on the operating system\"\"\"\n",
    "background_data_dir = os.path.join(script_dir, 'Background Data Folder')\n",
    "if not os.path.exists(background_data_dir):\n",
    "    os.makedirs(background_data_dir)\n",
    "\n",
    "#report_dir = script_dir + '\\\\' + 'Reports Folder'\n",
    "report_dir = os.path.join(script_dir, 'Reports Folder') \n",
    "if not os.path.exists(report_dir):\n",
    "    os.makedirs(report_dir)\n",
    "\n",
    "data_supplied_dir = os.path.join(script_dir, 'Data Supplied Folder')\n",
    "if not os.path.exists(data_supplied_dir):\n",
    "    os.makedirs(data_supplied_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_folder(script_dir):\n",
    "\n",
    "    if not os.path.exists(script_dir):\n",
    "        print(f\"Directory not found: {script_dir}\")\n",
    "        return []\n",
    "\n",
    "    files = [f for f in os.listdir(script_dir) if os.path.isfile(os.path.join(script_dir, f))]\n",
    "    \n",
    "    print(f\"Files in {script_dir}:\")\n",
    "    for file in files:\n",
    "        print(file)\n",
    "    \n",
    "    return #files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce2774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Data Supplied Folder:\n",
      "KIT 3 Online Reporting Data 01 05 2025.xlsx\n",
      "previous_errors_n_reponse_times_data.db\n"
     ]
    }
   ],
   "source": [
    "list_files_in_data_supplied_folder(data_supplied_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb5cca",
   "metadata": {},
   "source": [
    "# CONNECT TO GOOGLE DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_gdrive():\n",
    "    try:\n",
    "        creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        gc = gspread.authorize(creds)\n",
    "        drive_service = build('drive', 'v3', credentials=creds)\n",
    "        return gc, drive_service\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to authenticate with Google API: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_id: str, output_filename: str):\n",
    "    try:\n",
    "        _, drive_service = authenticate_gdrive()\n",
    "        request = drive_service.files().export_media(fileId=file_id,mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')\n",
    "    \n",
    "        with io.FileIO(output_filename, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            logging.info(f\"Download {int(status.progress() * 100)}% complete.\")\n",
    "\n",
    "        logging.info(f\"File downloaded as {output_filename}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download file: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc508255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_with_backoff(func,*args,max_retries=5,**kwargs):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result=func(*args,**kwargs)\n",
    "            if isinstance(result,HttpRequest):\n",
    "                result=result.execute()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except(HttpError,gspread.exceptions.APIError) as e:\n",
    "            if 'Quota exceeded' in str(e) or getattr(e,'status',0)==429:\n",
    "\n",
    "                if attempt==max_retries-1:\n",
    "                    logging.error(f\"Max retries reached for {func.__name__}: {e}\")\n",
    "                    raise\n",
    "\n",
    "                sleep_time=(2**attempt)+(random.randint(0,1000)/1000)\n",
    "                logging.warning(f\"Quota exceeded, retrying in {sleep_time:.2f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error in {func.__name__}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee0f991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets','https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Print script directory and its contents for debugging\n",
    "logging.info(f\"Notebook directory: {background_data_dir}\")\n",
    "logging.info(f\"Directory contents: {os.listdir(background_data_dir)}\")\n",
    "\n",
    "\n",
    "# Define the service account file path\n",
    "SERVICE_ACCOUNT_FILE = os.getenv('SERVICE_ACCOUNT_FILE',os.path.join(background_data_dir, 'GoogleAuth.json'))\n",
    "\n",
    "# Print the path being used\n",
    "logging.info(f\"Attempting to use service account file: {SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda252b",
   "metadata": {},
   "source": [
    "# GET DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba8a6f",
   "metadata": {},
   "source": [
    "## REPORTING DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ae5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_and_types(directory: str) -> None:\n",
    "    try:\n",
    "        # Establish connection to SQLite database\n",
    "        conn = sqlite3.connect(f\"{directory}/master_data.db\")\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Successfully connected to the database\")\n",
    "\n",
    "        # Get all tables in the database\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            print(f\"\\nTable: {table_name}\")\n",
    "\n",
    "            # Get column information for the table\n",
    "            cursor.execute(f\"PRAGMA table_info('{table_name}')\")\n",
    "            columns = cursor.fetchall()\n",
    "\n",
    "            # Print column names and their data types\n",
    "            for column in columns:\n",
    "                column_name = column[1]  # Name is in the second field\n",
    "                data_type = column[2]   # Data type is in the third field\n",
    "                print(f\"  Column: {column_name}, Data Type: {data_type}\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error connecting to SQLite: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"\\nSQLite connection closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5751fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    get_columns_and_types(background_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1eb1978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02\n",
      "02 May 2025\n"
     ]
    }
   ],
   "source": [
    "reporting_date = date(2025, 5, 2)\n",
    "print(reporting_date)\n",
    "reporting_date_formatted = reporting_date.strftime(\"%d %b %Y\")\n",
    "print(reporting_date_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28ed8603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-05-24'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd7d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 5, 24, 17, 17, 17, 863287)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.today()\n",
    "\n",
    "reporting_date = date(2025, 5, 2)\n",
    "\n",
    "reporting_date_formatted = reporting_date.strftime(\"%d %b %Y\")\n",
    "\n",
    "# CONNECT\n",
    "conn = sqlite3.connect(f\"{background_data_dir}/master_data.db\")\n",
    "cursor = conn.cursor()\n",
    "## RETRIEVE ROW \n",
    "cursor.execute(\"SELECT * FROM MasterData WHERE ReportingDate = ?\", (reporting_date_formatted,))\n",
    "row = cursor.fetchone()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "adding YearWeek and altering the if then loop logic to account for the new column\n",
    "DID NOT WORK since YearWeek is INTEGER and not a string\n",
    "\n",
    "if row:\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    for key, value in zip(column_names, row):\n",
    "        if key in [\"StartDate\", \"EndDate\", \"YearWeek\", \"ReportingDate\"] and isinstance(value, str):\n",
    "            try: \n",
    "                if key in [\"StartDate\", \"EndDate\"]:\n",
    "                    value = datetime.strptime(value, \"%Y-%m-%d\").date() \n",
    "                elif key == \"ReportingDate\": \n",
    "                    # assuming ReportingDate is in 'dd MMM YYYY' format\n",
    "                    value = datetime.strptime(value, \"%d %b %Y\").date()\n",
    "                else: \n",
    "                    value = str(value)\n",
    "            except ValueError:\n",
    "                value = None \n",
    "                # store the value as a global variable (if needed)\n",
    "            globals()[key] = value\n",
    "            # print out the key, value, and its type\n",
    "            print(f\"{key} = {value} ({type(value).__name__})\")\n",
    "else:\n",
    "    print(f\"No row found for ReportingDate = {reporting_date_formatted}\")\n",
    "\"\"\"\n",
    "\n",
    "if row:\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    for key, value in zip(column_names, row):\n",
    "        if key in [\"StartDate\", \"EndDate\", \"ReportingDate\"] and isinstance(value, str):\n",
    "            try:\n",
    "                if key in [\"StartDate\", \"EndDate\"]:\n",
    "                    value = datetime.strptime(value, \"%Y-%m-%d\").date()\n",
    "                elif key == \"ReportingDate\":\n",
    "                    value = datetime.strptime(value, \"%d %b %Y\").date()\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not parse date for {key}: {value}\")\n",
    "                value = None\n",
    "        elif key == \"YearWeek\":\n",
    "            # Handle YearWeek as an integer (or convert if needed)\n",
    "            value = int(value) if value is not None else None\n",
    "        else:\n",
    "            # Keep other columns as-is\n",
    "            pass\n",
    "\n",
    "        if value is not None or key in [\"StartDate\", \"EndDate\", \"YearWeek\", \"ReportingDate\"]:\n",
    "                globals()[key] = value\n",
    "                print(f\"{key} = {value} ({type(value).__name__})\")\n",
    "else:\n",
    "    print(f\"No row found for ReportingDate = {reporting_date_formatted}\")\n",
    "  \n",
    "# CLOSE CONNECTION\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29adffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_date = date(2025, 5, 2)\n",
    "\n",
    "reporting_date_formatted = reporting_date.strftime(\"%d %b %Y\")\n",
    "\n",
    "# CONNECT\n",
    "conn = sqlite3.connect(f\"{background_data_dir}/master_data.db\")\n",
    "cursor = conn.cursor()\n",
    "## RETRIEVE ROW \n",
    "cursor.execute(\"SELECT * FROM MasterData WHERE ReportingDate = ?\", (reporting_date_formatted,))\n",
    "row = cursor.fetchone()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "adding YearWeek and altering the if then loop logic to account for the new column\n",
    "DID NOT WORK since YearWeek is INTEGER and not a string\n",
    "\n",
    "if row:\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    for key, value in zip(column_names, row):\n",
    "        if key in [\"StartDate\", \"EndDate\", \"YearWeek\", \"ReportingDate\"] and isinstance(value, str):\n",
    "            try: \n",
    "                if key in [\"StartDate\", \"EndDate\"]:\n",
    "                    value = datetime.strptime(value, \"%Y-%m-%d\").date() \n",
    "                elif key == \"ReportingDate\": \n",
    "                    # assuming ReportingDate is in 'dd MMM YYYY' format\n",
    "                    value = datetime.strptime(value, \"%d %b %Y\").date()\n",
    "                else: \n",
    "                    value = str(value)\n",
    "            except ValueError:\n",
    "                value = None \n",
    "                # store the value as a global variable (if needed)\n",
    "            globals()[key] = value\n",
    "            # print out the key, value, and its type\n",
    "            print(f\"{key} = {value} ({type(value).__name__})\")\n",
    "else:\n",
    "    print(f\"No row found for ReportingDate = {reporting_date_formatted}\")\n",
    "\"\"\"\n",
    "\n",
    "if row:\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    for key, value in zip(column_names, row):\n",
    "        if key in [\"StartDate\", \"EndDate\", \"ReportingDate\"] and isinstance(value, str):\n",
    "            try:\n",
    "                if key in [\"StartDate\", \"EndDate\"]:\n",
    "                    value = datetime.strptime(value, \"%Y-%m-%d\").date()\n",
    "                elif key == \"ReportingDate\":\n",
    "                    value = datetime.strptime(value, \"%d %b %Y\").date()\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not parse date for {key}: {value}\")\n",
    "                value = None\n",
    "        elif key == \"YearWeek\":\n",
    "            # Handle YearWeek as an integer (or convert if needed)\n",
    "            value = int(value) if value is not None else None\n",
    "        else:\n",
    "            # Keep other columns as-is\n",
    "            pass\n",
    "\n",
    "        if value is not None or key in [\"StartDate\", \"EndDate\", \"YearWeek\", \"ReportingDate\"]:\n",
    "                globals()[key] = value\n",
    "                print(f\"{key} = {value} ({type(value).__name__})\")\n",
    "else:\n",
    "    print(f\"No row found for ReportingDate = {reporting_date_formatted}\")\n",
    "  \n",
    "# CLOSE CONNECTION\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fae77",
   "metadata": {},
   "source": [
    "## ADDITIONAL DATA ON GOOGLE DRIVE\n",
    "NOTICE: \n",
    "Additional Data worksheet has been changed without keeping record of previous versions (ie. what this script was based on)\n",
    "\n",
    "<strong> in future CRUCIAL to keep track of ALL alterations to script AND source</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_leave(reporting_date=reporting_date):\n",
    "    reporting_date = pd.Timestamp(reporting_date)\n",
    "    try:\n",
    "        #gc, _ = authenticate_gdrive()\n",
    "        creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Leave Sheet')\n",
    "        data = pd.DataFrame(sheet.get_all_records())\n",
    "\n",
    "        data['Leave - Start Date'] = pd.to_datetime(data['Leave - Start Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "        data['Leave - End Date'] = pd.to_datetime(data['Leave - End Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "        data['On Leave'] = False\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            start = row['Leave - Start Date']\n",
    "            end = row['Leave - End Date']\n",
    "\n",
    "            if pd.notnull(start) and start < reporting_date <= end:\n",
    "                data.at[index, 'On Leave'] = True\n",
    "\n",
    "            elif pd.isnull(start) and pd.notnull(end) and end > reporting_date:\n",
    "                data.at[index, 'On Leave'] = True\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Leave Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    return str(s).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_client_names():\n",
    "    try:\n",
    "    #gc, _ = authenticate_gdrive()\n",
    "        creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Client Names Sheet')\n",
    "        # Get all records from the sheet\n",
    "        data = sheet.get_all_records()\n",
    "        # Convert to DataFrame\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Client Names Sheet: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kindle_employees():\n",
    "    try:\n",
    "        #gc, _ = authenticate_gdrive()\n",
    "        creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Person Responsible Sheet')\n",
    "        data = sheet.get_all_records()\n",
    "        data = pd.DataFrame(data)\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Person Responsible Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef099f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_person_responsible(reporting_date=reporting_date):\n",
    "    reporting_date = pd.Timestamp(reporting_date)\n",
    "    Employee_data = fetch_kindle_employees()\n",
    "    LeaveStatus_data = fetch_leave(reporting_date)\n",
    "\n",
    "    def assign_developer_turn(row):\n",
    "        rotation_freq = row['Rotation Frequency (Weeks)']\n",
    "\n",
    "        if pd.isna(rotation_freq) or rotation_freq == '':\n",
    "        # Return Developer 1's name if valid, otherwise return empty string\n",
    "            dev1 = row['Developer 1']\n",
    "            if dev1 and dev1 != '?' and not pd.isna(dev1):\n",
    "                return dev1.strip()\n",
    "            \n",
    "            return ''\n",
    "\n",
    "        # Convert rotation_freq to an integer\n",
    "        try:\n",
    "            rotation_freq = int(rotation_freq)\n",
    "\n",
    "        except (ValueError, TypeError):\n",
    "            return ''\n",
    "\n",
    "        if rotation_freq <= 0:\n",
    "            return ''\n",
    "\n",
    "        # Determine which developer columns to consider (Developer 1 to Developer n)\n",
    "        developer_columns = ['Developer 1', 'Developer 2', 'Developer 3', 'Developer 4'][:rotation_freq]\n",
    "\n",
    "        # Get the list of developers from the relevant columns\n",
    "        developers = []\n",
    "        for col in developer_columns:\n",
    "            dev = row[col]\n",
    "            if dev and dev != '?' and not pd.isna(dev):\n",
    "\n",
    "                # Handle multiple developers in the same column (e.g., \"Joel;Brendan\")\n",
    "                developers.extend([d.strip() for d in dev.split(';') if d.strip()])\n",
    "\n",
    "        if not developers:\n",
    "            return ''\n",
    "\n",
    "        # Ensure current_week_number is an integer\n",
    "        if not isinstance(WeekNumber, int):\n",
    "            raise ValueError(f\"current_week_number must be an integer, got {type(WeekNumber)}: {WeekNumber}\")\n",
    "\n",
    "        # Calculate the developer index\n",
    "        developer_index = (WeekNumber % rotation_freq) % len(developers)\n",
    "\n",
    "        # Verify developer_index is an integer\n",
    "        if not isinstance(developer_index, int):\n",
    "            raise ValueError(f\"developer_index is not an integer, got {type(developer_index)}: {developer_index}\")\n",
    "\n",
    "        return developers[developer_index]\n",
    "\n",
    "    Employee_data['Developer'] = Employee_data.apply(assign_developer_turn, axis=1)\n",
    "    on_leave_list = LeaveStatus_data.loc[LeaveStatus_data['On Leave'] == True, 'Kindle Employee'].dropna().tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    SUGGESTED FOR LARGER DATASETS to use vectorized operations\n",
    "    def determine_ba_responsible_df(df):\n",
    "        conditions = [\n",
    "            (df['Business Analyst 1'].notna() & ~df['Business Analyst 1'].isin(on_leave_list)),\n",
    "            (df['Business Analyst 2'].notna() & ~df['Business Analyst 2'].isin(on_leave_list)),\n",
    "            (df['In Case of Emergency'].notna())\n",
    "        ]\n",
    "        choices = [\n",
    "            df['Business Analyst 1'],\n",
    "            df['Business Analyst 2'],\n",
    "            df['In Case of Emergency']\n",
    "        ]\n",
    "        return np.select(conditions, choices, default=None)\n",
    "    \"\"\"\n",
    "\n",
    "    def determine_ba_responsible(row):\n",
    "        ba1 = row['Business Analyst 1']\n",
    "        ba2 = row['Business Analyst 2']\n",
    "        emergency = row['In Case of Emergency']\n",
    "        ba1_on_leave = pd.notna(ba1) and ba1 in on_leave_list\n",
    "        ba2_on_leave = pd.notna(ba2) and ba2 in on_leave_list\n",
    "\n",
    "        if pd.notna(ba1) and not ba1_on_leave:\n",
    "            return ba1\n",
    "\n",
    "        elif pd.notna(ba2) and not ba2_on_leave:\n",
    "            return ba2\n",
    "\n",
    "        elif pd.notna(emergency):\n",
    "            return emergency\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    Employee_data['BA Responsible'] = Employee_data.apply(determine_ba_responsible, axis=1)\n",
    "\n",
    "    def determine_dev_responsible(row):\n",
    "\n",
    "        dev = row['Developer']\n",
    "        dev2 = row['Developer 2']\n",
    "        dev3 = row['Developer 3']\n",
    "        dev4 = row['Developer 4']\n",
    "        emergency = row['In Case of Emergency']\n",
    "\n",
    "        dev_on_leave = pd.notna(dev) and dev in on_leave_list\n",
    "        dev2_on_leave = pd.notna(dev2) and dev2 in on_leave_list\n",
    "        dev3_on_leave = pd.notna(dev3) and dev3 in on_leave_list\n",
    "        dev4_on_leave = pd.notna(dev4) and dev4 in on_leave_list\n",
    "\n",
    "        if pd.notna(dev) and not dev_on_leave:\n",
    "            return dev\n",
    "\n",
    "        elif pd.notna(dev2) and not dev2_on_leave:\n",
    "            return dev2\n",
    "\n",
    "        elif pd.notna(dev3) and not dev3_on_leave:\n",
    "            return dev3\n",
    "\n",
    "        elif pd.notna(dev4) and not dev4_on_leave:\n",
    "            return dev4\n",
    "\n",
    "        elif pd.notna(emergency):\n",
    "            return emergency\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    Employee_data['DEV Responsible'] = Employee_data.apply(determine_dev_responsible, axis=1)\n",
    "    Employee_data = Employee_data.drop(['Business Analyst 1', 'Business Analyst 2', 'Key Resource','Developer 1', 'Developer 2', 'Developer 3', 'Developer 4','Rotation Frequency (Weeks)', 'In Case of Emergency', 'Developer'], axis=1)\n",
    "\n",
    "    return Employee_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9aa96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_current_errors():\n",
    "    try:\n",
    "        #gc, _ = authenticate_gdrive()\n",
    "        creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('ERROR REPORT Current Week')\n",
    "        current_sheet = spreadsheet.worksheet(CurrentSheetName)\n",
    "        data = current_sheet.get_all_records()\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab77a45",
   "metadata": {},
   "source": [
    "## PREVIOUS DATA from db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_previous(YearWeek=YearWeek):\n",
    "    db_path = os.path.join(background_data_dir, 'previous_errors_n_reponse_times_data.db')\n",
    "    engine = create_engine(f'sqlite:///{db_path}')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM PreviousErrorData\n",
    "        WHERE CAST(SUBSTR(Period, 1, 6) AS INTEGER) < {YearWeek}\n",
    "        \"\"\"\n",
    "    return pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abb3f7",
   "metadata": {},
   "source": [
    "## RESPONSE TIMES from local device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ca3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_response_times(data_folder = data_supplied_dir):\n",
    "    run_date = reporting_date - timedelta(days=1)\n",
    "    date_string = f\"{run_date.day:02d} {run_date.month:02d} {run_date.year}\"\n",
    "    target_filename = f\"KIT 3 Online Reporting Data {date_string}.xlsx\"\n",
    "\n",
    "    files = os.listdir(data_folder)\n",
    "    matched_file = None\n",
    "\n",
    "    for file in files:\n",
    "        if clean_string(file) == clean_string(target_filename):\n",
    "            matched_file = file\n",
    "            break\n",
    "\n",
    "    if not matched_file:\n",
    "        available_files = \", \".join(files)\n",
    "        raise FileNotFoundError(f\"File '{target_filename}' not found. Available files: {available_files}\")\n",
    "\n",
    "\n",
    "    file_path = os.path.join(data_folder, matched_file)\n",
    "\n",
    "\n",
    "    try:\n",
    "        xl = pd.ExcelFile(file_path)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"File found but couldn't be accessed: {str(e)}\")\n",
    "  \n",
    "\n",
    "    sheet_name = \"This week response times\"\n",
    "    if sheet_name not in xl.sheet_names:\n",
    "        sheet_name = xl.sheet_names[1]\n",
    "        # Default to last sheet\n",
    "\n",
    "    try:\n",
    "        data = xl.parse(sheet_name, header=0)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"No valid data found in the '{sheet_name}' sheet: {str(e)}\")\n",
    "\n",
    "\n",
    "    client_name_lookup = {}\n",
    "    client_names_data = fetch_client_names()\n",
    "\n",
    "    for _, row in client_names_data.iterrows():\n",
    "        target = row['Client Names'].strip() if pd.notnull(row['Client Names']) else ''\n",
    "        if pd.notnull(row['Other']) and row['Other'].lower() != 'null':\n",
    "            aliases = [alias.strip() for alias in str(row['Other']).split(',')]\n",
    "            \n",
    "            for alias in aliases:\n",
    "                client_name_lookup[clean_string(alias)] = target\n",
    "\n",
    "        client_name_lookup[clean_string(target)] = target\n",
    "\n",
    "\n",
    "    def standardize_company_name(ColumnName):\n",
    "        if pd.isnull(ColumnName):\n",
    "            return ''\n",
    "\n",
    "        cleaned = clean_string(ColumnName)\n",
    "        return client_name_lookup.get(cleaned, ColumnName)\n",
    "        # fallback to original if not found\n",
    "\n",
    "    data['Company'] = data['Company'].apply(standardize_company_name)\n",
    "    desired_columns = ['Company', 'Average Response Time', 'Minimum Response Time','Maximum Response Time','Period', 'NewOnlineCount']\n",
    "    existing_columns = [col for col in desired_columns if col in data.columns]\n",
    "    data = data[existing_columns]\n",
    "\n",
    "    # data.to_excel(\"ResponseTimesQuery.xlsx\", index=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16241100",
   "metadata": {},
   "source": [
    "# DETERMINE CURRENT ERROR COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420215cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_count():\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    data = fetch_current_errors()\n",
    "    client_names_data = fetch_client_names()\n",
    "    client_name_lookup = {}\n",
    "\n",
    "    for _, row in client_names_data.iterrows():\n",
    "        target = row['Client Names'].strip() if pd.notnull(row['Client Names']) else ''\n",
    "        # Handle aliases in 'Other' column\n",
    "        if pd.notnull(row['Other']):\n",
    "            aliases = [alias.strip() for alias in str(row['Other']).split(',') if alias.strip()]\n",
    "            for alias in aliases:\n",
    "                client_name_lookup[clean_string(alias)] = target\n",
    "\n",
    "        # Map the target name itself\n",
    "        client_name_lookup[clean_string(target)] = target\n",
    "\n",
    "    # Define function to standardize company names\n",
    "    def standardize_company_name(name):\n",
    "        if pd.isna(name):\n",
    "            return np.nan\n",
    "        \n",
    "        cleaned = clean_string(name)\n",
    "        return client_name_lookup.get(cleaned, name)\n",
    "        # Fallback to original if not found\n",
    "\n",
    "    # Apply standardization to 'Client' column\n",
    "    data['Client'] = data['Client'].apply(standardize_company_name)\n",
    "    data['Client'] = data['Client'].replace('', np.nan)\n",
    "    data['Period'] = data['Period'].replace('', np.nan)\n",
    "    data['Client'] = data['Client'].ffill().bfill()\n",
    "    data['Period'] = data['Period'].ffill().bfill()\n",
    "    data = data.infer_objects(copy=False)\n",
    "    data['Functional Error Y/N'] = data['Functional Error Y/N'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    grouped = data.groupby(['Client', 'Period', 'Functional Error Y/N'], as_index=False)['No of times error occurred'].sum()\n",
    "    grouped.rename(columns={'No of times error occurred': 'Total Errors'}, inplace=True)\n",
    "\n",
    "    # Step 2: Rename column to remove spaces/slashes\n",
    "    grouped.rename(columns={'Functional Error Y/N': 'FunctionalError_Y/N'}, inplace=True)\n",
    "\n",
    "    # Step 3: Pivot Y/N values into columns\n",
    "    pivoted = grouped.pivot_table(\n",
    "        index=['Client', 'Period'],\n",
    "        columns='FunctionalError_Y/N',\n",
    "        values='Total Errors',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0).reset_index()\n",
    "\n",
    "    pivoted = pivoted.infer_objects(copy=False)\n",
    "    # Flatten the column index created by the pivot (required!)\n",
    "    pivoted.columns.name = None\n",
    "    # Remove the \"FunctionalError_Y/N\" header\n",
    "    pivoted = pivoted.drop(columns=[''])\n",
    "\n",
    "    # Step 4: Rename pivoted columns for clarity\n",
    "    pivoted.rename(columns={'Y': 'FunctionalErrors', 'N': 'NonFunctionalErrors'}, inplace=True)\n",
    "\n",
    "    return pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7092cb",
   "metadata": {},
   "source": [
    "## JOIN WITH PREVIOUS AND RESPONSE TIMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(text):\n",
    "    match = re.search(r'(\\d{1,2} \\w{3}) \\d{4} - (\\d{1,2} \\w{3}) \\d{4}', text)\n",
    "    if match:\n",
    "        return f\"{match.group(1)} - {match.group(2)}\"\n",
    "\n",
    "    return None\n",
    "    # or return text if no match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_errors():\n",
    "    ResponseTimes_data = fetch_response_times()\n",
    "    ResponseTimes_data = ResponseTimes_data[~ResponseTimes_data['Period'].isna()] #remove total row\n",
    "\n",
    "    ErrorCount_data = error_count()\n",
    "    ErrorCount_data = ErrorCount_data.rename(columns={'Client': 'Company'})\n",
    "    ErrorCount_data = ErrorCount_data.drop(['Period'], axis=1) # can later be used as checK\n",
    "\n",
    "    ErrorReport_data = pd.merge(ResponseTimes_data, ErrorCount_data, on=['Company'], how='left')\n",
    "    columns_to_convert = ['FunctionalErrors', 'NonFunctionalErrors']\n",
    "    for col in columns_to_convert:\n",
    "        ErrorReport_data[col] = pd.to_numeric(ErrorReport_data[col], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Database path\n",
    "    db_path = os.path.join(background_data_dir, 'previous_errors_n_reponse_times_data.db')\n",
    "\n",
    "    # Create SQLAlchemy engine for SQLite\n",
    "    engine = create_engine(f'sqlite:///{db_path}')\n",
    "\n",
    "    # Verify the Period of new data against the last entry\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Get the last entry's Period from PreviousErrorData (most recent by Period)\n",
    "            last_entry = pd.read_sql(\"SELECT Period FROM PreviousErrorData ORDER BY Period DESC LIMIT 1\", conn)\n",
    "            if last_entry.empty:\n",
    "                print(\"Warning: PreviousErrorData table is empty. Cannot verify Period.\")\n",
    "            else:\n",
    "                last_period = last_entry['Period'].iloc[0]\n",
    "                last_period_prefix = int(last_period[:6])\n",
    "\n",
    "                # Get the Period prefix from ErrorReport_data (assuming all rows have the same prefix)\n",
    "                new_period_prefix = int(ErrorReport_data['Period'].iloc[0][:6])\n",
    "                # First 6 characters\n",
    "\n",
    "                # Verify that the new year is exactly one more than the last year\n",
    "                if new_period_prefix == last_period_prefix + 1:\n",
    "                    print(f\"Verification successful: New Period ({new_period_prefix}) is one month after last Period ({last_period_prefix}).\")\n",
    "                else:\n",
    "                    print(f\"Verification failed: New Period ({new_period_prefix}) is not one more than last Period ({last_period_prefix}).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {e}\")\n",
    "\n",
    "    # Check for duplicates and append only unique records\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Read existing records from PreviousErrorData to check for duplicates\n",
    "            existing_records = pd.read_sql(\"SELECT Period, Company FROM PreviousErrorData\", conn)\n",
    "            # Create a merged DataFrame to identify new records\n",
    "            # Convert relevant columns to string to ensure consistent comparison\n",
    "            ErrorReport_data['Period'] = ErrorReport_data['Period'].astype(str)\n",
    "            ErrorReport_data['Company'] = ErrorReport_data['Company'].astype(str)\n",
    "            existing_records['Period'] = existing_records['Period'].astype(str)\n",
    "            existing_records['Company'] = existing_records['Company'].astype(str)\n",
    "            # Merge to find records in ErrorReport_data that don't exist in the database\n",
    "            new_records = ErrorReport_data.merge(\n",
    "                existing_records,\n",
    "                on=['Period', 'Company'],\n",
    "                how='left',\n",
    "                indicator=True).query('_merge == \"left_only\"').drop(columns='_merge')\n",
    "\n",
    "            if new_records.empty:\n",
    "                print(\"No new records to append; all records already exist in PreviousErrorData.\")\n",
    "            else:\n",
    "                # Append only the new records to the PreviousErrorData table\n",
    "                new_records.to_sql('PreviousErrorData', con=engine, if_exists='append', index=False)\n",
    "\n",
    "            print(f\"Successfully appended {len(new_records)} new records to PreviousErrorData table.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking/appending data: {e}\")\n",
    "\n",
    "\n",
    "    PreviousErrors_data = fetch_previous()\n",
    "    PreviousErrors_data = PreviousErrors_data.rename(columns={'AverageResponseTime': 'Average Response Time', 'MinimumResponseTime': 'Minimum Response Time', 'MaximumResponseTime':'Maximum Response Time'})\n",
    "\n",
    "    ErrorReport_data = pd.concat([PreviousErrors_data, ErrorReport_data], ignore_index=True)\n",
    "\n",
    "    return ErrorReport_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3126070",
   "metadata": {},
   "source": [
    "# COPY DATA TO REPORTING WORKBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5531077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def write_dataframe_to_sheet():\n",
    "DataErrors_n_ResponseTimes_data = concat_errors()\n",
    "DataErrors_n_ResponseTimes_data = DataErrors_n_ResponseTimes_data.where(pd.notnull(DataErrors_n_ResponseTimes_data), None)\n",
    "\n",
    "try:\n",
    "    creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    gc = gspread.authorize(creds)\n",
    "    spreadsheet = gc.open('KIT 3 Online Reporting Data')\n",
    "    sheet = spreadsheet.worksheet('Data Errors and Reponse Times')\n",
    "        # Directly convert NaN to None without fillna\n",
    "        # data = [DataErrors_n_ResponseTimes_data.columns.tolist()] + DataErrors_n_ResponseTimes_data.where(pd.notnull(DataErrors_n_ResponseTimes_data), None).values.tolist()\n",
    "        # Fill numeric columns with 0 and non-numeric with ''\n",
    "    data_filled = DataErrors_n_ResponseTimes_data.copy()\n",
    "    for col in data_filled.columns:\n",
    "        if data_filled[col].dtype == 'Float64':\n",
    "            data_filled[col] = data_filled[col].fillna(0)\n",
    "        else:\n",
    "            data_filled[col] = data_filled[col].fillna('')\n",
    "\n",
    "    data = [data_filled.columns.tolist()] + data_filled.astype(object).where(pd.notnull(data_filled), None).values.tolist()\n",
    "    execute_with_backoff(sheet.clear)\n",
    "    execute_with_backoff(sheet.update, data, 'A1', value_input_option='RAW')\n",
    "\n",
    "    logging.info(f\"Overwrote {len(DataErrors_n_ResponseTimes_data)} rows in {sheet} in 'Data Errors and Reponse Times'\")\n",
    "    #return\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to write DataFrame to sheet: {e}\")\n",
    "    raise\n",
    "\n",
    "#write_dataframe_to_sheet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREADSHEET_ID = os.getenv('SPREADSHEET_ID', '1m49V3mKZiuTatraSSGqo1xHLIhcQRmJZO90S4Y72vcc') # CURRENT\n",
    "DEST_SPREADSHEET_ID = os.getenv('DEST_SPREADSHEET_ID', '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI') # KIT 3 Online Reporting Data\n",
    "\n",
    "# Validate service account file\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "# Authenticate with Google Sheets\n",
    "try:\n",
    "    creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    gc = gspread.authorize(creds)\n",
    "    sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to authenticate with Google Sheets: {e}\")\n",
    "    raise\n",
    "\n",
    "# Open source and destination spreadsheets\n",
    "spreadsheet = execute_with_backoff(gc.open_by_key, SPREADSHEET_ID)\n",
    "dest_spreadsheet = execute_with_backoff(gc.open_by_key, DEST_SPREADSHEET_ID)\n",
    "\n",
    "# Cache worksheets to reduce API calls\n",
    "source_worksheets = execute_with_backoff(spreadsheet.worksheets)\n",
    "dest_worksheets = execute_with_backoff(dest_spreadsheet.worksheets)\n",
    "\n",
    "source_worksheet_ids = {sheet.title: sheet.id for sheet in source_worksheets}\n",
    "dest_worksheet_ids = {sheet.title: sheet.id for sheet in dest_worksheets}\n",
    "\n",
    "# Define sheet names\n",
    "source_sheet_name = CurrentSheetName\n",
    "dest_sheet_name = 'Current Week Error'\n",
    "\n",
    "# Copy and rename sheet if needed\n",
    "if source_sheet_name in source_worksheet_ids and source_sheet_name not in dest_worksheet_ids:\n",
    "    copy_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().sheets().copyTo,\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        sheetId=source_worksheet_ids[source_sheet_name],\n",
    "        body={'destinationSpreadsheetId': DEST_SPREADSHEET_ID})\n",
    "\n",
    "    copied_sheet_id = copy_response['sheetId']\n",
    "    logging.info(f\"Copied sheet '{source_sheet_name}' to destination.\")\n",
    "\n",
    "    # Delete existing destination sheet if it exists\n",
    "    if dest_sheet_name in dest_worksheet_ids:\n",
    "        delete_request = {'requests': [{'deleteSheet': {'sheetId': dest_worksheet_ids[dest_sheet_name]}}]}\n",
    "        \n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "            body=delete_request)\n",
    "\n",
    "        logging.info(f\"Deleted existing sheet '{dest_sheet_name}' from destination.\")\n",
    "\n",
    "    # Rename the copied sheet\n",
    "    rename_request = {'requests': [{'updateSheetProperties': {'properties': {'sheetId': copied_sheet_id,'title': dest_sheet_name},'fields': 'title'}}]}\n",
    "\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "        body=rename_request)\n",
    "\n",
    "    logging.info(f\"Renamed copied sheet to '{dest_sheet_name}' in destination.\")\n",
    "\n",
    "else:\n",
    "    logging.info(f\"No sheet copied. Either '{source_sheet_name}' doesn't exist in source or '{dest_sheet_name}' already exists in destination.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREADSHEET_ID = os.getenv('SPREADSHEET_ID', '13IpNi1rFg8luMX8t_OlhCbAQcEhkZkZUmwFWP3C_8N8') # FOLLOW UP\n",
    "DEST_SPREADSHEET_ID = os.getenv('DEST_SPREADSHEET_ID', '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI')\n",
    "\n",
    "# Validate service account file\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "# Authenticate with Google Sheets\n",
    "try:\n",
    "    creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    gc = gspread.authorize(creds)\n",
    "    sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to authenticate with Google Sheets: {e}\")\n",
    "    raise\n",
    "\n",
    "# Open source and destination spreadsheets\n",
    "spreadsheet = execute_with_backoff(gc.open_by_key, SPREADSHEET_ID)\n",
    "dest_spreadsheet = execute_with_backoff(gc.open_by_key, DEST_SPREADSHEET_ID)\n",
    " \n",
    "\n",
    "# Cache worksheets to reduce API calls\n",
    "source_worksheets = execute_with_backoff(spreadsheet.worksheets)\n",
    "dest_worksheets = execute_with_backoff(dest_spreadsheet.worksheets)\n",
    "\n",
    "source_worksheet_ids = {sheet.title: sheet.id for sheet in source_worksheets}\n",
    "dest_worksheet_ids = {sheet.title: sheet.id for sheet in dest_worksheets}\n",
    "\n",
    "# Define sheet names\n",
    "source_sheet_name = PreviousSheetName\n",
    "dest_sheet_name = 'Feedback on Previous Week Error'\n",
    "\n",
    "# Copy and rename sheet if needed\n",
    "if source_sheet_name in source_worksheet_ids and source_sheet_name not in dest_worksheet_ids:\n",
    "    copy_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().sheets().copyTo,\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        sheetId=source_worksheet_ids[source_sheet_name],\n",
    "        body={'destinationSpreadsheetId': DEST_SPREADSHEET_ID})\n",
    "\n",
    "    copied_sheet_id = copy_response['sheetId']\n",
    "    logging.info(f\"Copied sheet '{source_sheet_name}' to destination.\")\n",
    "\n",
    "    # Delete existing destination sheet if it exists\n",
    "    if dest_sheet_name in dest_worksheet_ids:\n",
    "        delete_request = {'requests': [{'deleteSheet': {'sheetId': dest_worksheet_ids[dest_sheet_name]}}]}\n",
    "\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "            body=delete_request)\n",
    "\n",
    "        logging.info(f\"Deleted existing sheet '{dest_sheet_name}' from destination.\")\n",
    "  \n",
    "\n",
    "    # Rename the copied sheet\n",
    "    rename_request = {'requests': [{'updateSheetProperties': {'properties': {'sheetId': copied_sheet_id,'title': dest_sheet_name},'fields': 'title'}}]}\n",
    "\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "        body=rename_request)\n",
    "\n",
    "    logging.info(f\"Renamed copied sheet to '{dest_sheet_name}' in destination.\")\n",
    "\n",
    "else:\n",
    "    logging.info(f\"No sheet copied. Either '{source_sheet_name}' doesn't exist in source or '{dest_sheet_name}' already exists in destination.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e16d6df",
   "metadata": {},
   "source": [
    "# ADD and COPY SHEETS FOR NEXT WEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hide_requests(worksheet_ids: Dict[str, int], visible_titles: List[str], spreadsheet_id: str) -> List[Dict[str, Any]]:\n",
    "    try:\n",
    "        metadata = execute_with_backoff(\n",
    "            sheets_service.spreadsheets().get,\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            fields='sheets.properties')\n",
    "\n",
    "        current_sheets = metadata['sheets']\n",
    "        visible_sheet_ids = [\n",
    "            s['properties']['sheetId']\n",
    "            for s in current_sheets\n",
    "            if not s['properties'].get('hidden', False)]\n",
    "\n",
    "        hide_requests = []\n",
    "        for title, sheet_id in worksheet_ids.items():\n",
    "            if title not in visible_titles:\n",
    "                hide_requests.append({\"updateSheetProperties\": {\"properties\": {\"sheetId\": sheet_id,\"hidden\": True},\"fields\": \"hidden\"}})\n",
    "\n",
    "        if len(hide_requests) >= len(visible_sheet_ids):\n",
    "            logging.warning(f\"Prevented hiding all sheets in spreadsheet {spreadsheet_id}.\")\n",
    "            if hide_requests:\n",
    "                hide_requests.pop()\n",
    "        return hide_requests\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch or process current sheet visibility: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the service account file path\n",
    "SERVICE_ACCOUNT_FILE = os.getenv('SERVICE_ACCOUNT_FILE',os.path.join(background_data_dir, 'GoogleAuth.json'))\n",
    "\n",
    "# Print the path being useD\n",
    "logging.info(f\"Attempting to use service account file: {SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "logging.info(\"File found, proceeding with authentication...\")\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Google Sheets setup\n",
    "REDEFINED_SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "CURRENT_SPREADSHEET_ID = os.getenv('SPREADSHEET_ID', '1m49V3mKZiuTatraSSGqo1xHLIhcQRmJZO90S4Y72vcc') # CURRENT\n",
    "FOLLOWUP_SPREADSHEET_ID = os.getenv('SPREADSHEET_ID', '13IpNi1rFg8luMX8t_OlhCbAQcEhkZkZUmwFWP3C_8N8') # FOLLOW UP\n",
    "TEMPLATE_NAME = 'TEMPLATE - Do not edit'\n",
    "\n",
    "# Validate service account file\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {SERVICE_ACCOUNT_FILE}\") \n",
    "\n",
    "# Authenticate with Google Sheets\n",
    "try:\n",
    "    creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=REDEFINED_SCOPES)\n",
    "    gc = gspread.authorize(creds)\n",
    "    sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to authenticate with Google Sheets: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Open spreadsheets\n",
    "current_spreadsheet = execute_with_backoff(gc.open_by_key, CURRENT_SPREADSHEET_ID)\n",
    "template_sheet = current_spreadsheet.get_worksheet_by_id(552319826)\n",
    "followup_spreadsheet = execute_with_backoff(gc.open_by_key, FOLLOWUP_SPREADSHEET_ID)\n",
    "\n",
    "# Cache worksheets to reduce API calls\n",
    "current_worksheets = execute_with_backoff(current_spreadsheet.worksheets)\n",
    "followup_worksheets = execute_with_backoff(followup_spreadsheet.worksheets)\n",
    "\n",
    "source_worksheet_ids = {sheet.title: sheet.id for sheet in current_worksheets}\n",
    "dest_worksheet_ids = {sheet.title: sheet.id for sheet in followup_worksheets}\n",
    "\n",
    "\n",
    "# Define sheet names (replace with actual values or logic)\n",
    "next_sheet_name = NextSheetName\n",
    "followup_sheet_name = CurrentSheetName\n",
    "\n",
    "# Create or open new worksheet\n",
    "if next_sheet_name in source_worksheet_ids:\n",
    "    logging.info(f\"Worksheet '{next_sheet_name}' already exists. Deleting it.\")\n",
    "    sheet_id = source_worksheet_ids[next_sheet_name]\n",
    "    delete_request = {'requests': [{'deleteSheet': {'sheetId': sheet_id}}]}\n",
    "\n",
    "    try:\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "            body=delete_request)\n",
    "\n",
    "        # Remove from cache\n",
    "        del source_worksheet_ids[next_sheet_name]\n",
    "        # Brief delay to ensure deletion is processed\n",
    "\n",
    "        time.sleep(1)\n",
    "        # Refresh worksheet list to confirm deletion\n",
    "        current_worksheets = execute_with_backoff(current_spreadsheet.worksheets)\n",
    "        source_worksheet_ids = {sheet.title: sheet.id for sheet in current_worksheets}\n",
    "        if next_sheet_name in source_worksheet_ids:\n",
    "            raise Exception(f\"Failed to delete worksheet '{next_sheet_name}'.\")\n",
    "        logging.info(f\"Deleted worksheet '{next_sheet_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error deleting worksheet '{next_sheet_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "  \n",
    "# Create new sheet\n",
    "try:\n",
    "    new_sheet = execute_with_backoff(\n",
    "        current_spreadsheet.duplicate_sheet,\n",
    "        source_sheet_id=template_sheet.id,\n",
    "        new_sheet_name=next_sheet_name)\n",
    "    \n",
    "    source_worksheet_ids[next_sheet_name] = new_sheet.id\n",
    "    logging.info(f\"Created worksheet '{next_sheet_name}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating worksheet '{next_sheet_name}': {e}\")\n",
    "    raise\n",
    "\n",
    "# Update 'Period' column\n",
    "values = execute_with_backoff(new_sheet.get_all_values)\n",
    "header = values[0]\n",
    "\n",
    "try:\n",
    "    period_col_index = header.index(\"Period\")\n",
    "    period_col_letter = chr(ord('A') + period_col_index)\n",
    "\n",
    "except ValueError:\n",
    "    raise Exception(\"Column 'Period' not found.\")\n",
    "\n",
    "updates = [{'range': f\"{period_col_letter}{row_idx}\", 'values': [[next_sheet_name]]} \n",
    "           for row_idx, row in enumerate(values[1:], start=2) \n",
    "           if len(row) > period_col_index and row[period_col_index].strip()]\n",
    "\n",
    "if updates:\n",
    "    execute_with_backoff(new_sheet.batch_update, updates)\n",
    "    logging.info(f\"Updated {len(updates)} 'Period' cells.\")\n",
    "\n",
    "# Copy followup sheet to destination spreadsheet if needed\n",
    "if followup_sheet_name in source_worksheet_ids and followup_sheet_name not in dest_worksheet_ids:\n",
    "    try:\n",
    "        copy_response = execute_with_backoff(\n",
    "            sheets_service.spreadsheets().sheets().copyTo,\n",
    "            spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "            sheetId=source_worksheet_ids[followup_sheet_name],\n",
    "            body={'destinationSpreadsheetId': FOLLOWUP_SPREADSHEET_ID})\n",
    "\n",
    "        copied_sheet_id = copy_response['sheetId']\n",
    "        rename_request = {'requests': [{'updateSheetProperties': {'properties': {'sheetId': copied_sheet_id,'title': followup_sheet_name},'fields': 'title'}}]}\n",
    "\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "            body=rename_request)\n",
    "\n",
    "        dest_worksheet_ids[followup_sheet_name] = copied_sheet_id\n",
    "        logging.info(f\"Copied and renamed sheet '{followup_sheet_name}' to destination.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error copying sheet '{followup_sheet_name}': {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddc93d2",
   "metadata": {},
   "source": [
    "#### HttpError                                 \n",
    "Traceback (most recent call last)\n",
    "Cell In[65], line 63\n",
    "     60 delete_request = {'requests': [{'deleteSheet': {'sheetId': sheet_id}}]}\n",
    "     62 try:\n",
    "---> 63     execute_with_backoff(\n",
    "     64         sheets_service.spreadsheets().batchUpdate,\n",
    "     65         spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "     66         body=delete_request)\n",
    "     68     # Remove from cache\n",
    "     69     del source_worksheet_ids[next_sheet_name]\n",
    "\n",
    "Cell In[13], line 6, in execute_with_backoff(func, max_retries, *args, **kwargs)\n",
    "      4     result=func(*args,**kwargs)\n",
    "      5     if isinstance(result,HttpRequest):\n",
    "----> 6         result=result.execute()\n",
    "      8     return result\n",
    "     10 except(HttpError,gspread.exceptions.APIError) as e:\n",
    "\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\googleapiclient\\_helpers.py:130, in positional.<locals>.positional_decorator.<locals>.positional_wrapper(*args, **kwargs)\n",
    "    128     elif positional_parameters_enforcement == POSITIONAL_WARNING:\n",
    "    129         logger.warning(message)\n",
    "--> 130 return wrapped(*args, **kwargs)\n",
    "\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\googleapiclient\\http.py:938, in HttpRequest.execute(self, http, num_retries)\n",
    "    936     callback(resp)\n",
    "    937 if resp.status >= 300:\n",
    "--> 938     raise HttpError(resp, content, uri=self.uri)\n",
    "    939 return self.postproc(resp, content)\n",
    "\n",
    "HttpError: <HttpError 400 when requesting https://sheets.googleapis.com/v4/spreadsheets/1m49V3mKZiuTatraSSGqo1xHLIhcQRmJZO90S4Y72vcc:batchUpdate?alt=json returned \"Invalid requests[0].deleteSheet: You can't remove all the visible sheets in a document.\". Details: \"Invalid requests[0].deleteSheet: You can't remove all the visible sheets in a document.\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_visible_sheets = [next_sheet_name]\n",
    "followup_visible_sheets = [followup_sheet_name]\n",
    "\n",
    "source_requests = filter_hide_requests(source_worksheet_ids, current_visible_sheets, CURRENT_SPREADSHEET_ID)\n",
    "dest_requests = filter_hide_requests(dest_worksheet_ids, followup_visible_sheets, FOLLOWUP_SPREADSHEET_ID)\n",
    "\n",
    "if source_requests:\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "        body={\"requests\": source_requests})\n",
    "\n",
    "    logging.info(f\"Hid {len(source_requests)} sheets in source spreadsheet.\")\n",
    "\n",
    "if dest_requests:\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body={\"requests\": dest_requests})\n",
    "\n",
    "    logging.info(f\"Hid {len(dest_requests)} sheets in destination spreadsheet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
