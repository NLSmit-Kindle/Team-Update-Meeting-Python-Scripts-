{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f70b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from googleapiclient.errors import HttpError\n",
    "from gspread_formatting import CellFormat, TextFormat, Color, Borders, Border, set_column_width, format_cell_range\n",
    "from gspread_formatting import batch_updater\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.http import HttpRequest\n",
    "import ipywidgets as widgets\n",
    "import tkinter as tk\n",
    "from IPython.display import display, clear_output\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from tkinter import messagebox\n",
    "#from google.oauth2.credentials import Credentials as UserCredentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.auth.exceptions import RefreshError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7186e5",
   "metadata": {},
   "source": [
    "<strong>LESSON LEARNED</strong>\n",
    "\n",
    "each time code updated or changes made \n",
    "<strong>START OVER</strong>\n",
    "duplicate file and update name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5c661",
   "metadata": {},
   "source": [
    "# SETUP DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()\n",
    "print(script_dir)\n",
    "\n",
    "#background_data_dir = script_dir + '\\\\' + 'Background Data Folder'\n",
    "\"\"\"os.path.join adds the appropriate path separator (\\ or /) based on the operating system\"\"\"\n",
    "background_data_dir = os.path.join(script_dir, 'Background Data Folder')\n",
    "if not os.path.exists(background_data_dir):\n",
    "    os.makedirs(background_data_dir)\n",
    "\n",
    "#report_dir = script_dir + '\\\\' + 'Reports Folder'\n",
    "reports_dir = os.path.join(script_dir, 'Reports Folder') \n",
    "if not os.path.exists(reports_dir):\n",
    "    os.makedirs(reports_dir)\n",
    "\n",
    "data_supplied_dir = os.path.join(script_dir, 'Data Supplied Folder')\n",
    "if not os.path.exists(data_supplied_dir):\n",
    "    os.makedirs(data_supplied_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12390bc2",
   "metadata": {},
   "source": [
    "c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ecc95",
   "metadata": {},
   "source": [
    "# DETERMINE REPORTING DATE\n",
    "display as dropdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MasterData_db_path = os.path.join(background_data_dir, 'master_data.db')\n",
    "conn = sqlite3.connect(MasterData_db_path) #f\"{background_data_dir}/master_data.db\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT DISTINCT ReportingDate FROM MasterData\")   \n",
    "periods = [row[0] for row in cursor.fetchall()]\n",
    "conn.close()\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=periods,\n",
    "    description='Reporting Date:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global selected_reporting_date\n",
    "        selected_reporting_date = change['new']\n",
    "        with out:\n",
    "            clear_output()\n",
    "            print(f\"Selected Period: {selected_reporting_date}\")\n",
    "\n",
    "dropdown.observe(on_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with out:\n",
    "    display(dropdown)\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7828332",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_reporting_date # just a check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf77fd",
   "metadata": {},
   "source": [
    "'02 May 2025'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ReportingDate = selected_reporting_date\n",
    "datetime_ReportingDate = datetime.strptime(selected_reporting_date, '%d %b %Y').date()\n",
    "formatted_ReportingDate = datetime_ReportingDate.strftime('%d %m %Y')\n",
    "\n",
    "RunDate = datetime_ReportingDate - timedelta(days=1)\n",
    "formatted_RunDate = f\"{RunDate .day:02d} {RunDate .month:02d} {RunDate .year}\"\n",
    "#formatted_RunDate = RunDate.strftime('%d %m %Y')\n",
    "\n",
    "#Designated_Filename = 'KIT 3 Online Reporting {}.xlsx'.format(formatted_RunDate)\n",
    "Designated_Filename = f\"KIT 3 Online Reporting Data {formatted_RunDate}.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09567015",
   "metadata": {},
   "source": [
    "1. Using f-string with manual formatting: \n",
    "\n",
    "    formatted_RunDate = f\"{RunDate.day:02d} {RunDate.month:02d} {RunDate.year}\"\n",
    "\n",
    "        RunDate.day:02d formats the day with two digits (e.g., 01, 09, 23).\n",
    "\n",
    "        RunDate.month:02d does the same for the month.\n",
    "\n",
    "        RunDate.year just inserts the full year (e.g., 2025).\n",
    "\n",
    "    This approach is explicit and lets you control each component individually.\n",
    "\n",
    "2. Using .strftime()\n",
    "\n",
    "    formatted_RunDate = RunDate.strftime('%d %m %Y')\n",
    "\n",
    "\n",
    "        %d formats the day as a two-digit number.\n",
    "\n",
    "        %m formats the month as a two-digit number.\n",
    "\n",
    "        %Y formats the year as a four-digit number.\n",
    "\n",
    "    This method is cleaner and more concise, especially useful when dealing with more complex date formats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae57fab",
   "metadata": {},
   "source": [
    "1. Using .format()\n",
    "\n",
    "    Designated_Filename = 'KIT 3 Online Reporting {}.xlsx'.format(formatted_RunDate)\n",
    "\n",
    "    This uses Python's older .format() method to insert the value of formatted_RunDate into the {} placeholder.\n",
    "\n",
    "    If you had more placeholders, you could insert multiple values like .format(a, b, c).\n",
    "\n",
    "2. Using an f-string\n",
    "\n",
    "    Designated_Filename = f\"KIT 3 Online Reporting Data {formatted_RunDate}.xlsx\"\n",
    "\n",
    "    This uses a formatted string literal (f-string), available from Python 3.6+.\n",
    "\n",
    "    You embed variables directly in the string with {}.\n",
    "\n",
    "    Itâ€™s generally cleaner and more readable, especially when combining multiple variables or expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94467cf5",
   "metadata": {},
   "source": [
    "## ESTABLISH CORRESPONDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(MasterData_db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query the row where ReportingDate matches\n",
    "cursor.execute(\"SELECT * FROM MasterData WHERE ReportingDate = ?\", (str_ReportingDate,))\n",
    "row = cursor.fetchone()\n",
    "\n",
    "if row:\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    \n",
    "    for key, value in zip(column_names, row):\n",
    "        if key in [\"StartDate\", \"EndDate\", \"ReportingDate\"] and isinstance(value, str):\n",
    "            # Convert string to datetime.date\n",
    "            try:\n",
    "                # For StartDate and EndDate, use ISO format (YYYY-MM-DD)\n",
    "                if key in [\"StartDate\", \"EndDate\"]:\n",
    "                    value = datetime.strptime(value, \"%Y-%m-%d\").date()\n",
    "                else:\n",
    "                    # ReportingDate is in 'dd MMM YYYY' format\n",
    "                    value = datetime.strptime(value, \"%d %b %Y\").date()\n",
    "            except ValueError:\n",
    "                value = None  # Handle parsing error gracefully\n",
    "        \n",
    "        # Store the value as a global variable (if needed)\n",
    "        globals()[key] = value\n",
    "        \n",
    "        # Print out the key, value, and its type\n",
    "        print(f\"{key} = {value} ({type(value).__name__})\")\n",
    "else:\n",
    "    print(f\"No row found for ReportingDate = {formatted_ReportingDate}\")\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee922d4",
   "metadata": {},
   "source": [
    "id = 17 (int)\n",
    "StartDate = 2025-04-24 (date)\n",
    "EndDate = 2025-05-01 (date)\n",
    "WeekNumber = 17 (int)\n",
    "YearWeek = 202517 (int)\n",
    "CurrentSheetName = 24 Apr - 30 Apr (str)\n",
    "CurrentPeriod = 24 Apr 2025 - 01 May 2025 (str)\n",
    "PeriodName = 202517 - 24 Apr 2025 - 01 May 2025 (str)\n",
    "PreviousSheetName = 17 Apr - 23 Apr (str)\n",
    "NextSheetName = 01 May - 07 May (str)\n",
    "ReportingDate = 2025-05-02 (date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check execution \n",
    "YeakWeek = YearWeek\n",
    "WeekNumber = WeekNumber\n",
    "CurrentSheetName = CurrentSheetName\n",
    "CurrentPeriod = CurrentPeriod\n",
    "PeriodName = PeriodName\n",
    "PreviousSheetName = PreviousSheetName\n",
    "NextSheetName = NextSheetName\n",
    "ReportingDate = ReportingDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c22d1",
   "metadata": {},
   "source": [
    "# CHECK GOOGLE DRIVE and GMAIL CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Print script directory and its contents for debugging\n",
    "logging.info(f\"Notebook directory: {background_data_dir}\")\n",
    "logging.info(f\"Directory contents: {os.listdir(background_data_dir)}\")\n",
    "\n",
    "# Define the service account file path\n",
    "GOOGLE_SERVICE_ACCOUNT_FILE = os.getenv(\n",
    "    'SERVICE_ACCOUNT_FILE',\n",
    "    os.path.join(background_data_dir, 'GoogleAuth.json')\n",
    ")\n",
    "\n",
    "GMAIL_SERVICE_ACCOUNT_FILE = os.getenv(\n",
    "    'SERVICE_ACCOUNT_FILE',\n",
    "    os.path.join(background_data_dir, 'GmailAuth.json')\n",
    ")\n",
    "\n",
    "# Print the path being used\n",
    "logging.info(f\"Attempting to use service account file: {GOOGLE_SERVICE_ACCOUNT_FILE}\")\n",
    "logging.info(f\"Attempting to use service account file: {GMAIL_SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(GOOGLE_SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {GOOGLE_SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {GOOGLE_SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "logging.info(\"File found, proceeding with authentication...\")\n",
    "# Add your Google Sheets authentication code here\n",
    "\n",
    "if not os.path.exists(GMAIL_SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {GMAIL_SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {GMAIL_SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "logging.info(\"File found, proceeding with authentication...\")\n",
    "# Add your Google Sheets authentication code here\n",
    "\n",
    "def authenticate_gdrive():\n",
    "    #SCOPES = ['https://www.googleapis.com/auth/spreadsheets','https://www.googleapis.com/auth/drive']\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    try:\n",
    "        creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        gc = gspread.authorize(creds)\n",
    "        drive_service = build('drive', 'v3', credentials=creds)\n",
    "        return gc, drive_service\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to authenticate with Google API: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def authenticate_gsheets():\n",
    "    \"\"\"Authenticate and return gspread client and Google Sheets API service.\"\"\"\n",
    "    # For gspread\n",
    "    #SCOPES = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    #creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', SCOPES)\n",
    "    #gc = gspread.authorize(creds)\n",
    "    \n",
    "    # For Google Sheets API\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "    try:\n",
    "        creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        gc = gspread.authorize(creds)\n",
    "        sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "        return gc, sheets_service\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to authenticate with Google Sheets: {e}\")\n",
    "\n",
    "\n",
    "def download_file(file_id: str, output_filename: str):\n",
    "    try:\n",
    "        _, drive_service = authenticate_gdrive()\n",
    "        request = drive_service.files().export_media(\n",
    "            fileId=file_id,\n",
    "            mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "        )\n",
    "\n",
    "        with io.FileIO(output_filename, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                logging.info(f\"Download {int(status.progress() * 100)}% complete.\")\n",
    "\n",
    "        logging.info(f\"File downloaded as {output_filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download file: {e}\")\n",
    "        raise\n",
    "\n",
    "def execute_with_backoff(func,*args,max_retries=5,**kwargs):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result=func(*args,**kwargs)\n",
    "            if isinstance(result,HttpRequest):\n",
    "                result=result.execute()\n",
    "            return result\n",
    "        except(HttpError,gspread.exceptions.APIError) as e:\n",
    "            if 'Quota exceeded' in str(e) or getattr(e,'status',0)==429:\n",
    "                if attempt==max_retries-1:\n",
    "                    logging.error(f\"Max retries reached for {func.__name__}: {e}\")\n",
    "                    raise\n",
    "                sleep_time=(2**attempt)+(random.randint(0,1000)/1000)\n",
    "                logging.warning(f\"Quota exceeded, retrying in {sleep_time:.2f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error in {func.__name__}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def add_or_overwrite_sheet(spreadsheet_id: str, sheet_name: str, template_sheet_id: int = None, data: list = None):\n",
    "    try:\n",
    "        # Authenticate\n",
    "        gc, sheets_service = authenticate_gsheets()\n",
    "        \n",
    "        # Open the spreadsheet\n",
    "        spreadsheet = execute_with_backoff(gc.open_by_key, spreadsheet_id)\n",
    "        \n",
    "        # Get current worksheets\n",
    "        worksheets = execute_with_backoff(spreadsheet.worksheets)\n",
    "        worksheet_titles = [sheet.title for sheet in worksheets]\n",
    "        worksheet_ids = {sheet.title: sheet.id for sheet in worksheets}\n",
    "\n",
    "        # Check if the sheet already exists\n",
    "        if sheet_name in worksheet_titles:\n",
    "            logging.info(f\"Sheet '{sheet_name}' already exists. Deleting it.\")\n",
    "            sheet_id = worksheet_ids[sheet_name]\n",
    "            delete_request = {\n",
    "                'requests': [{\n",
    "                    'deleteSheet': {\n",
    "                        'sheetId': sheet_id\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "            execute_with_backoff(\n",
    "                sheets_service.spreadsheets().batchUpdate,\n",
    "                spreadsheetId=spreadsheet_id,\n",
    "                body=delete_request\n",
    "            )\n",
    "            logging.info(f\"Deleted existing sheet '{sheet_name}'.\")\n",
    "            time.sleep(1)  # Brief delay to ensure deletion is processed\n",
    "\n",
    "        # Create a new sheet by duplicating a template (if provided) or adding a blank sheet\n",
    "        if template_sheet_id:\n",
    "            new_sheet = execute_with_backoff(\n",
    "                spreadsheet.duplicate_sheet,\n",
    "                source_sheet_id=template_sheet_id,\n",
    "                new_sheet_name=sheet_name\n",
    "            )\n",
    "            logging.info(f\"Created new sheet '{sheet_name}' by duplicating template.\")\n",
    "        else:\n",
    "            new_sheet = execute_with_backoff(\n",
    "                spreadsheet.add_worksheet,\n",
    "                title=sheet_name,\n",
    "                rows=100,\n",
    "                cols=20\n",
    "            )\n",
    "            logging.info(f\"Created new blank sheet '{sheet_name}'.\")\n",
    "\n",
    "        # If data is provided, update the sheet with new data\n",
    "        if data:\n",
    "            new_sheet = execute_with_backoff(spreadsheet.worksheet, sheet_name)\n",
    "            execute_with_backoff(\n",
    "                new_sheet.update,\n",
    "                values=data,\n",
    "                range_name='A1'\n",
    "            )\n",
    "            logging.info(f\"Updated sheet '{sheet_name}' with new data.\")\n",
    "\n",
    "        return new_sheet, worksheet_ids\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to add or overwrite sheet '{sheet_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf87448",
   "metadata": {},
   "source": [
    "2025-05-21 15:34:34,139 - INFO - Notebook directory: c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Background Data Folder\n",
    "2025-05-21 15:34:34,142 - INFO - Directory contents: ['Construct_MasterData.ipynb', 'Construct_Previous.ipynb', 'Construct_WeekDates.ipynb', 'error_log.txt', 'GmailAuth.json', 'GmailToken.json', 'GoogleAuth.json', 'master_data.db', 'previous_errors_n_reponse_times_data.db', 'Previous_Error_n_ResponseTimes202516.sql', 'Previous_Error_n_ResponseTimes202517.sql', 'week_dates.db']\n",
    "2025-05-21 15:34:34,143 - INFO - Attempting to use service account file: c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Background Data Folder\\GoogleAuth.json\n",
    "2025-05-21 15:34:34,144 - INFO - Attempting to use service account file: c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Background Data Folder\\GmailAuth.json\n",
    "2025-05-21 15:34:34,146 - INFO - File found, proceeding with authentication...\n",
    "2025-05-21 15:34:34,148 - INFO - File found, proceeding with authentication..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b77ba",
   "metadata": {},
   "source": [
    "## standardize client names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "\n",
    "def extract_dates(text):\n",
    "    match = re.search(r'(\\d{1,2} \\w{3}) \\d{4} - (\\d{1,2} \\w{3}) \\d{4}', text)\n",
    "    if match:\n",
    "        return f\"{match.group(1)} - {match.group(2)}\"\n",
    "    return None  # or return text if no match is found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clients_df_to_custom_dict(df):\n",
    "    result = {}\n",
    "    for _, row in df.iterrows():\n",
    "        # Initialize list with 'Other' value (assuming it's always non-null)\n",
    "        values = [row['Other1']]\n",
    "        # Append non-null values from Other2 and Other3\n",
    "        for col in ['Other2', 'Other3']:\n",
    "            if pd.notnull(row[col]) and row[col] != '':\n",
    "                values.append(row[col])\n",
    "        # Set dictionary entry\n",
    "        result[row['Client Name']] = values\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c000bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_client_names():\n",
    "    try:\n",
    "        gc, _ = authenticate_gdrive()\n",
    "        #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "        #gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Client Names Sheet')\n",
    "        # Get all records from the sheet\n",
    "        data = sheet.get_all_records()\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df[~df['Other1'].isnull() & (df['Other1'].str.strip() != '')]  #ignore those for which no alternative name is listed\n",
    "        ClientNames_dict = clients_df_to_custom_dict(df)\n",
    "        return ClientNames_dict # return as dictionary \n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Client Names Sheet: {e}\")\n",
    "        raise   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d53396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_client_names(df, df_name):\n",
    "\n",
    "    ClientNames_dict = fetch_client_names()\n",
    "    Nonstrd_ClientNames_list = [item for value in ClientNames_dict.values() for item in (value if isinstance(value, (list, tuple)) else [value])]\n",
    "\n",
    "    Nonstrd_ClientNames_list = [clean_string(i) for i in Nonstrd_ClientNames_list]\n",
    "    def check_match(text):\n",
    "        if pd.isna(text):  # Handle NaN/None values\n",
    "            return False\n",
    "        cleaned_text = clean_string(text)\n",
    "        return cleaned_text in Nonstrd_ClientNames_list\n",
    "    \n",
    "    orginal_colname = df.columns[0]\n",
    "    orginal_cols = list(df.columns)\n",
    "\n",
    "    # Standardize column\n",
    "    df = df.rename(columns={df.columns[0]: 'temp col'})\n",
    "    # Add a new column 'Match' to the DataFrame\n",
    "    df['Match'] = df['temp col'].apply(check_match)\n",
    "\n",
    "    value_map = {}\n",
    "    for key, values in ClientNames_dict.items():\n",
    "        for value in values:\n",
    "            value_map[clean_string(value)] = key\n",
    "    \n",
    "    # Function to apply to each row\n",
    "    def get_standard_value(row):\n",
    "        if row['Match']:  # If Match is True\n",
    "            cleaned_col = clean_string(row['temp col'])\n",
    "            return value_map.get(cleaned_col, row['temp col'])  # Return key if found, else original\n",
    "        return row['temp col']  # Return original if Match is False\n",
    "    \n",
    "    # Add 'standard' column to DataFrame\n",
    "    df['standard'] = df.apply(get_standard_value, axis=1)\n",
    "    df = df.drop(columns=['temp col'])\n",
    "    df = df.rename(columns={'standard': orginal_colname})\n",
    "    df = df.reindex(columns=orginal_cols) # reorder\n",
    "    print('Client Names standardized for {}'.format(df_name))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f4a24",
   "metadata": {},
   "source": [
    "## determine individuals repsonsible for reporting \n",
    "determine which Kindle EMployees are (would be) on leave on day of reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_leave(reporting_date=ReportingDate):\n",
    "    reporting_date = pd.Timestamp(reporting_date)\n",
    "    try:\n",
    "        gc, _ = authenticate_gdrive()\n",
    "        #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "        #gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Leave Sheet')\n",
    "        data = pd.DataFrame(sheet.get_all_records())\n",
    "\n",
    "        data['Leave - Start Date'] = pd.to_datetime(data['Leave - Start Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "        data['Leave - End Date'] = pd.to_datetime(data['Leave - End Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "        data['On Leave'] = False\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            start = row['Leave - Start Date']\n",
    "            end = row['Leave - End Date']\n",
    "\n",
    "            if pd.notnull(start) and start < reporting_date <= end:\n",
    "                data.at[index, 'On Leave'] = True\n",
    "            elif pd.isnull(start) and pd.notnull(end) and end > reporting_date:\n",
    "                data.at[index, 'On Leave'] = True\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Leave Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kindle_employees():\n",
    "    try:\n",
    "        gc, _ = authenticate_gdrive()\n",
    "        #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "        #gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Person Responsible Sheet')\n",
    "        data = sheet.get_all_records()\n",
    "        data = pd.DataFrame(data)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Person Responsible Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_person_responsible(reporting_date=ReportingDate):\n",
    "    reporting_date = pd.Timestamp(reporting_date)\n",
    "    Employee_data = fetch_kindle_employees()\n",
    "    LeaveStatus_data = fetch_leave(reporting_date)\n",
    "\n",
    "    def determine_devs_turn(row):\n",
    "        rotation_freq = row['Rotation Frequency (Weeks)']\n",
    "        if pd.isna(rotation_freq) or rotation_freq == '':\n",
    "        # Return Developer 1's name if valid, otherwise return empty string\n",
    "            dev1 = row['Developer 1']\n",
    "            if dev1 and dev1 != '?' and not pd.isna(dev1):\n",
    "                return dev1.strip()\n",
    "            return ''\n",
    "    \n",
    "        # Convert rotation_freq to an integer\n",
    "        try:\n",
    "            rotation_freq = int(rotation_freq)\n",
    "        except (ValueError, TypeError):\n",
    "            return ''\n",
    "    \n",
    "        if rotation_freq <= 0:\n",
    "            return ''\n",
    "\n",
    "        # Determine which developer columns to consider (Developer 1 to Developer n)\n",
    "        developer_columns = ['Developer 1', 'Developer 2', 'Developer 3', 'Developer 4'][:rotation_freq]\n",
    "    \n",
    "        # Get the list of developers from the relevant columns\n",
    "        developers = []\n",
    "        for col in developer_columns:\n",
    "            dev = row[col]\n",
    "            if dev and dev != '?' and not pd.isna(dev):\n",
    "            # Handle multiple developers in the same column (e.g., \"Joel;Brendan\")\n",
    "                developers.extend([d.strip() for d in dev.split(';') if d.strip()])\n",
    "    \n",
    "        if not developers:\n",
    "            return ''\n",
    "    \n",
    "        # Ensure current_week_number is an integer\n",
    "        if not isinstance(WeekNumber, int):\n",
    "            raise ValueError(f\"current_week_number must be an integer, got {type(WeekNumber)}: {WeekNumber}\")\n",
    "    \n",
    "        # Calculate the developer index\n",
    "        developer_index = (WeekNumber % rotation_freq) % len(developers)\n",
    "    \n",
    "        # Verify developer_index is an integer\n",
    "        if not isinstance(developer_index, int):\n",
    "            raise ValueError(f\"developer_index is not an integer, got {type(developer_index)}: {developer_index}\")\n",
    "    \n",
    "        return developers[developer_index]\n",
    "\n",
    "    Employee_data['Developer'] = Employee_data.apply(determine_devs_turn, axis=1)\n",
    "\n",
    "    on_leave_list = LeaveStatus_data.loc[LeaveStatus_data['On Leave'] == True, 'Kindle Employee'].dropna().tolist()\n",
    "\n",
    "    def assign_functionalerror_reporter(row):\n",
    "        dev = row['Developer']\n",
    "        dev2 = row['Developer 2']\n",
    "        dev3 = row['Developer 3']\n",
    "        dev4 = row['Developer 4']\n",
    "        emergency = row['In Case of Emergency']\n",
    "    \n",
    "        dev_on_leave = pd.notna(dev) and dev in on_leave_list\n",
    "        dev2_on_leave = pd.notna(dev2) and dev2 in on_leave_list\n",
    "        dev3_on_leave = pd.notna(dev3) and dev3 in on_leave_list\n",
    "        dev4_on_leave = pd.notna(dev4) and dev4 in on_leave_list\n",
    "    \n",
    "        if pd.notna(dev) and not dev_on_leave:\n",
    "            return dev\n",
    "        elif pd.notna(dev2) and not dev2_on_leave:\n",
    "            return dev2\n",
    "        elif pd.notna(dev3) and not dev3_on_leave:\n",
    "            return dev3\n",
    "        elif pd.notna(dev4) and not dev4_on_leave:\n",
    "            return dev4\n",
    "        elif pd.notna(emergency):\n",
    "            return emergency\n",
    "        else:\n",
    "            return None  # or raise an error if no responsible person can be assigned\n",
    "        \n",
    "    Employee_data['Functional Errors Reporter'] = Employee_data.apply(assign_functionalerror_reporter, axis=1)\n",
    "\n",
    "\n",
    "    def assign_clientmovement_reporter(row):\n",
    "        if row['Designated Reporter on Client Movement'] == 'Developer':\n",
    "            return assign_functionalerror_reporter(row)\n",
    "\n",
    "        if row['Designated Reporter on Client Movement'] == 'Business Analyst':\n",
    "            ba1 = row['Business Analyst 1']\n",
    "            ba2 = row['Business Analyst 2']\n",
    "            emergency = row['In Case of Emergency']\n",
    "\n",
    "            ba1_on_leave = pd.notna(ba1) and ba1 in on_leave_list\n",
    "            ba2_on_leave = pd.notna(ba2) and ba2 in on_leave_list\n",
    "\n",
    "            if pd.notna(ba1) and not ba1_on_leave:\n",
    "                return ba1\n",
    "            elif pd.notna(ba2) and not ba2_on_leave:\n",
    "                return ba2\n",
    "            elif pd.notna(emergency):\n",
    "                return emergency\n",
    "            else:\n",
    "                return None  # or raise an error if no responsible person can be assigned\n",
    "\n",
    "        return None  # If \"Designated Reporter on Client Movement\" is neither BA nor Developer\n",
    "\n",
    "    \n",
    "    Employee_data['Client Movement Reporter'] = Employee_data.apply(assign_clientmovement_reporter, axis=1)\n",
    "\n",
    "    PersonsResponsible_data = Employee_data.drop(['Designated Reporter on Client Movement', 'Business Analyst 1', 'Business Analyst 2', 'Key Resource',\n",
    "       'Developer 1', 'Developer 2', 'Developer 3', 'Developer 4',\n",
    "       'Rotation Frequency (Weeks)', 'In Case of Emergency', 'Developer'], axis=1)\n",
    "    \n",
    "    PersonsResponsible_data = standardize_client_names(PersonsResponsible_data, 'PersonsResponsible_data')\n",
    "    \n",
    "    return PersonsResponsible_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3778f",
   "metadata": {},
   "source": [
    "#### apply to Client Movement and Current Weeks' Functional Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd89691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_current_errors():\n",
    "    try:\n",
    "        gc, _ = authenticate_gdrive()\n",
    "        #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "        #gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('ERROR REPORT Current Week')\n",
    "        current_sheet = spreadsheet.worksheet(CurrentSheetName)\n",
    "        data = current_sheet.get_all_records()\n",
    "        return pd.DataFrame(data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e376b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_error_count():\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    \n",
    "    Current_Error_data = fetch_current_errors()\n",
    "    \n",
    "    Current_Error_data['Client'] = Current_Error_data['Client'].replace('', np.nan)\n",
    "    Current_Error_data['Period'] = Current_Error_data['Period'].replace('', np.nan)\n",
    "    Current_Error_data['Client'] = Current_Error_data['Client'].ffill().bfill()\n",
    "    Current_Error_data['Period'] = Current_Error_data['Period'].ffill().bfill()\n",
    "    Current_Error_data = Current_Error_data.infer_objects(copy=False)\n",
    "    Current_Error_data['Functional Error Y/N'] = Current_Error_data['Functional Error Y/N'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    Current_Error_grouped = Current_Error_data.groupby(['Client', 'Period', 'Functional Error Y/N'], as_index=False)['No of times error occurred'].sum()\n",
    "    Current_Error_grouped.rename(columns={'No of times error occurred': 'Total Errors'}, inplace=True)\n",
    "\n",
    "    # Step 2: Rename column to remove spaces/slashes\n",
    "    Current_Error_grouped.rename(columns={'Functional Error Y/N': 'FunctionalError_Y/N'}, inplace=True)\n",
    "\n",
    "    # Step 3: Pivot Y/N values into columns\n",
    "    Current_ErrorCount_data = Current_Error_grouped.pivot_table(\n",
    "        index=['Client', 'Period'],\n",
    "        columns='FunctionalError_Y/N',\n",
    "        values='Total Errors',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "        ).reset_index()\n",
    "    Current_ErrorCount_data = Current_ErrorCount_data.infer_objects(copy=False)\n",
    "    # Flatten the column index created by the pivot (required!)\n",
    "    \n",
    "    Current_ErrorCount_data.columns.name = None  # Remove the \"FunctionalError_Y/N\" header\n",
    "    Current_ErrorCount_data = Current_ErrorCount_data.drop(columns=[''])\n",
    "\n",
    "    # Step 4: Rename pivoted columns for clarity\n",
    "    Current_ErrorCount_data.rename(columns={'Y': 'FunctionalErrors', 'N': 'NonFunctionalErrors'}, inplace=True)\n",
    "\n",
    "    Current_ErrorCount_data = standardize_client_names(Current_ErrorCount_data, 'Current_ErrorCount_data')\n",
    "\n",
    "    return Current_ErrorCount_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_client_movement():\n",
    "    try:\n",
    "        gc, _ = authenticate_gdrive()\n",
    "        #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "        #gc = gspread.authorize(creds)\n",
    "        spreadsheet = gc.open('Weekly Movement')\n",
    "        sheet = spreadsheet.worksheet('Export')\n",
    "        \n",
    "        data = sheet.get_all_records()\n",
    "        \n",
    "       \n",
    "        ClientMovement_data = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "        # Remove all rows after the row where 'Tenant Name' has 'Total'\n",
    "        if 'Tenant Name' in ClientMovement_data.columns:\n",
    "            total_index = ClientMovement_data[ClientMovement_data['Tenant Name'].str.contains('Total', na=False)].index\n",
    "            if not total_index.empty:\n",
    "                ClientMovement_data = ClientMovement_data.loc[:total_index[0]-1]\n",
    "\n",
    "        ClientMovement_data = standardize_client_names(ClientMovement_data, 'ClientMovement_data')\n",
    "\n",
    "        return ClientMovement_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Client Movement: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30229985",
   "metadata": {},
   "outputs": [],
   "source": [
    "PersonsResponsible_data = determine_person_responsible()\n",
    "BAResponsible_data = PersonsResponsible_data[['Clients','Client Movement Reporter']].rename(columns={'Clients':'Tenant Name'})\n",
    "DEVResponsible_data = PersonsResponsible_data[['Clients','Functional Errors Reporter']].rename(columns={'Clients':'Company'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a55ad3",
   "metadata": {},
   "source": [
    "2025-05-21 15:44:23,224 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:44:27,234 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:44:30,401 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Client Names standardized for PersonsResponsible_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbea1d",
   "metadata": {},
   "source": [
    "### AttributeError                            \n",
    "Traceback (most recent call last)\n",
    "Cell In[22], line 1\n",
    "----> 1 PersonsResponsible_data = determine_person_responsible()\n",
    "      2 BAResponsible_data = PersonsResponsible_data[['Clients','Client Movement Reporter']].rename(columns={'Clients':'Tenant Name'})\n",
    "      3 DEVResponsible_data = PersonsResponsible_data[['Clients','Functional Errors Reporter']].rename(columns={'Clients':'Company'})\n",
    "\n",
    "Cell In[18], line 3, in determine_person_responsible(reporting_date)\n",
    "      1 def determine_person_responsible(reporting_date=ReportingDate):\n",
    "      2     reporting_date = pd.Timestamp(reporting_date)\n",
    "----> 3     Employee_data = fetch_kindle_employees()\n",
    "      4     LeaveStatus_data = fetch_leave(reporting_date)\n",
    "      6     def determine_devs_turn(row):\n",
    "\n",
    "Cell In[17], line 3, in fetch_kindle_employees()\n",
    "      1 def fetch_kindle_employees():\n",
    "      2     try:\n",
    "----> 3         gc, _ = authenticate_gdrive()\n",
    "      4         #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "      5         #gc = gspread.authorize(creds)\n",
    "      6         spreadsheet = gc.open('Additional Data')\n",
    "\n",
    "Cell In[11], line 42, in authenticate_gdrive()\n",
    "     40 SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "     41 try:\n",
    "---> 42     creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "     43     gc = gspread.authorize(creds)\n",
    "     44     drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "AttributeError: type object 'Credentials' has no attribute 'from_service_account_file'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718c52d",
   "metadata": {},
   "source": [
    "#### \n",
    "from google.oauth2.credentials import Credentials <strong> as UserCredentials </strong>\n",
    "\n",
    "since conflicts with \n",
    "\n",
    "from google.oauth2.service_account import Credentials \n",
    "\n",
    "defined earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d1832",
   "metadata": {},
   "source": [
    "##### Client Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd985bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClientMovement_data = fetch_client_movement()\n",
    "Onlines_data = pd.merge(ClientMovement_data, BAResponsible_data, on='Tenant Name', how='left')\n",
    "Onlines_data.loc[Onlines_data['Difference'] <= 0, 'Client Movement Reporter'] = np.nan\n",
    "\n",
    "try:\n",
    "    gc, _ = authenticate_gdrive()\n",
    "    #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "    #gc = gspread.authorize(creds)\n",
    "    spreadsheet = gc.open('Persons Responsible')\n",
    "\n",
    "    worksheet_title = 'Onlines'\n",
    "    try:\n",
    "        sheet = spreadsheet.worksheet(worksheet_title)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        sheet = spreadsheet.add_worksheet(title=worksheet_title, rows=100, cols=26)\n",
    "        logging.info(f\"Created new worksheet: {worksheet_title}\")\n",
    "\n",
    "    data = [Onlines_data.columns.tolist()] + Onlines_data.astype(object).where(pd.notnull(Onlines_data), None).values.tolist()\n",
    "        \n",
    "    # Clear the sheet and write the data\n",
    "    execute_with_backoff(sheet.clear)\n",
    "    execute_with_backoff(sheet.update, data, 'A1', value_input_option='RAW')\n",
    "\n",
    "    # Set the width of the first column (column A)\n",
    "    set_column_width(sheet, 'A', 250)  \n",
    "    set_column_width(sheet, 'F', 100) \n",
    "    set_column_width(sheet, 'G', 150) \n",
    "\n",
    "    # Format the data as a table\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(data[0])\n",
    "    range_notation = f'A1:{chr(65 + num_cols - 1)}{num_rows}'\n",
    "\n",
    "    # Set header row height using batch update\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateDimensionProperties\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet.id,\n",
    "                            \"dimension\": \"ROWS\",\n",
    "                            \"startIndex\": 0,  # Row 1 (0-based index)\n",
    "                            \"endIndex\": 1     # Only the first row\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"pixelSize\": 30  # Set height to 30 pixels\n",
    "                        },\n",
    "                        \"fields\": \"pixelSize\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Bold headers and format\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            f'A1:{chr(65 + num_cols - 1)}1',\n",
    "            CellFormat(\n",
    "                textFormat=TextFormat(bold=True),\n",
    "                horizontalAlignment='CENTER',\n",
    "                verticalAlignment='MIDDLE',\n",
    "                backgroundColor=Color(0.9, 0.9, 0.9)  # Light gray background for headers\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add borders to create a table effect\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            range_notation,\n",
    "            CellFormat(\n",
    "                borders=Borders(\n",
    "                    top=Border('SOLID'),\n",
    "                    bottom=Border('SOLID'),\n",
    "                    left=Border('SOLID'),\n",
    "                    right=Border('SOLID')\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add filter buttons to the table\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"setBasicFilter\": {\n",
    "                        \"filter\": {\n",
    "                            \"range\": {\n",
    "                                \"sheetId\": sheet.id,\n",
    "                                \"startRowIndex\": 0,  # Start at row 1\n",
    "                                \"endRowIndex\": num_rows,  # End at last row\n",
    "                                \"startColumnIndex\": 0,  # Start at column A\n",
    "                                \"endColumnIndex\": num_cols  # End at last column\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Data successfully added to {worksheet_title} worksheet of Persons Responsible workbook\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to write DataFrame to sheet, apply formatting, or add filter: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d83782",
   "metadata": {},
   "source": [
    "##### Functional Errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64657788",
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_ErrorCount_data = determine_error_count()\n",
    "Current_ErrorCount_data = Current_ErrorCount_data.rename(columns={'Client': 'Company'}) \n",
    "Current_ErrorCount_data = Current_ErrorCount_data.drop(['Period'], axis=1)\n",
    "FunctionalErrors_data = pd.merge(Current_ErrorCount_data, DEVResponsible_data, on='Company', how='left')\n",
    "FunctionalErrors_data.loc[FunctionalErrors_data['FunctionalErrors'] > 0, 'Functional Errors Reporter'] = np.nan\n",
    "\n",
    "try:\n",
    "    gc, _ = authenticate_gdrive()\n",
    "    #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "    #gc = gspread.authorize(creds)\n",
    "    spreadsheet = gc.open('Persons Responsible')\n",
    "\n",
    "    # Check if worksheet exists, create if it doesn't\n",
    "    worksheet_title = 'Functional Errors'\n",
    "    try:\n",
    "        sheet = spreadsheet.worksheet(worksheet_title)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        sheet = spreadsheet.add_worksheet(title=worksheet_title, rows=100, cols=26)\n",
    "        logging.info(f\"Created new worksheet: {worksheet_title}\")\n",
    "\n",
    "    data = [FunctionalErrors_data.columns.tolist()] + FunctionalErrors_data.astype(object).where(pd.notnull(FunctionalErrors_data), None).values.tolist()\n",
    "        \n",
    "    # Clear the sheet and write the data\n",
    "    execute_with_backoff(sheet.clear)\n",
    "    execute_with_backoff(sheet.update, data, 'A1', value_input_option='RAW')\n",
    "\n",
    "        # Set the width of the first column (column A)\n",
    "    set_column_width(sheet, 'A', 300)\n",
    "    set_column_width(sheet, 'B', 50)  \n",
    "    set_column_width(sheet, 'C', 50) \n",
    "    set_column_width(sheet, 'D', 150)\n",
    "\n",
    "        # Format the data as a table\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(data[0])\n",
    "    range_notation = f'A1:{chr(65 + num_cols - 1)}{num_rows}'\n",
    "\n",
    "        # Set header row height using batch update\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateDimensionProperties\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet.id,\n",
    "                            \"dimension\": \"ROWS\",\n",
    "                            \"startIndex\": 0,  # Row 1 (0-based index)\n",
    "                            \"endIndex\": 1     # Only the first row\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"pixelSize\": 30  # Set height to 30 pixels\n",
    "                        },\n",
    "                        \"fields\": \"pixelSize\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Bold headers and format\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            f'A1:{chr(65 + num_cols - 1)}1',\n",
    "            CellFormat(\n",
    "                textFormat=TextFormat(bold=True),\n",
    "                horizontalAlignment='CENTER',\n",
    "                verticalAlignment='MIDDLE',\n",
    "                backgroundColor=Color(0.9, 0.9, 0.9)  # Light gray background for headers\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add borders to create a table effect\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            range_notation,\n",
    "            CellFormat(\n",
    "                borders=Borders(\n",
    "                    top=Border('SOLID'),\n",
    "                    bottom=Border('SOLID'),\n",
    "                    left=Border('SOLID'),\n",
    "                    right=Border('SOLID')\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    # Add filter buttons to the table\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"setBasicFilter\": {\n",
    "                        \"filter\": {\n",
    "                            \"range\": {\n",
    "                                \"sheetId\": sheet.id,\n",
    "                                \"startRowIndex\": 0,  # Start at row 1\n",
    "                                \"endRowIndex\": num_rows,  # End at last row\n",
    "                                \"startColumnIndex\": 0,  # Start at column A\n",
    "                                \"endColumnIndex\": num_cols  # End at last column\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Data sucessfully added to {worksheet_title} worksheet of Persons Responsible workbook\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to write DataFrame to sheet, apply formatting, or add filter: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3627a31",
   "metadata": {},
   "source": [
    "2025-05-21 15:45:05,222 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:45:10,135 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:45:13,936 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Client Names standardized for Current_ErrorCount_data\n",
    "2025-05-21 15:45:22,686 - INFO - Data sucessfully added to Functional Errors worksheet of Persons Responsible workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54705c",
   "metadata": {},
   "source": [
    "# MERGE PREVIOUS WITH NEWEST RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e70af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_previous(YearWeek=YearWeek): \n",
    "    db_path = os.path.join(background_data_dir, 'previous_errors_n_reponse_times_data.db')\n",
    "    engine = create_engine(f'sqlite:///{db_path}')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM PreviousErrorData\n",
    "        WHERE CAST(SUBSTR(Period, 1, 6) AS INTEGER) < {YearWeek}\n",
    "            \"\"\"\n",
    "    Previous_Errors_n_ResponseTimes_data = pd.read_sql(query, engine)\n",
    "    Previous_Errors_n_ResponseTimes_data = standardize_client_names(Previous_Errors_n_ResponseTimes_data,'Previous_Errors_n_ResponseTimes_data')\n",
    "    \n",
    "    return Previous_Errors_n_ResponseTimes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e39093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_response_times(data_folder = data_supplied_dir): # change to allow user input \n",
    "    #date_string = f\"{RunDate .day:02d} {RunDate .month:02d} {RunDate .year}\"\n",
    "    #target_filename = f\"KIT 3 Online Reporting Data {date_string}.xlsx\"\n",
    "\n",
    "    files = os.listdir(data_folder)\n",
    "    matched_file = None\n",
    "    for file in files:\n",
    "        if clean_string(file) == clean_string(Designated_Filename):\n",
    "            matched_file = file\n",
    "            break\n",
    "\n",
    "    if not matched_file:\n",
    "        available_files = \", \".join(files)\n",
    "        raise FileNotFoundError(f\"File '{Designated_Filename}' not found. Available files: {available_files}\")\n",
    "\n",
    "    file_path = os.path.join(data_folder, matched_file)\n",
    "\n",
    "    try:\n",
    "        xl = pd.ExcelFile(file_path)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"File found but couldn't be accessed: {str(e)}\")\n",
    "\n",
    "    sheet_name = \"This week response times\"\n",
    "    if sheet_name not in xl.sheet_names:\n",
    "        sheet_name = xl.sheet_names[1]  # Default to last sheet\n",
    "\n",
    "    try:\n",
    "        data = xl.parse(sheet_name, header=0)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"No valid data found in the '{sheet_name}' sheet: {str(e)}\")\n",
    "\n",
    "    Current_ResponseTimes_data  = standardize_client_names(data, 'Current_ResponseTimes_data')\n",
    "    Current_ResponseTimes_data = Current_ResponseTimes_data.drop(['FunctionalError', 'NonFunctionalError'], axis=1)\n",
    "    \n",
    "    #Current_ResponseTimes_data.to_excel(\"ResponseTimesQuery.xlsx\", index=False)\n",
    "\n",
    "    return Current_ResponseTimes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17563689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_errors_n_responsetimes():\n",
    "    \"\"\"\n",
    "    Concatenate current response times and error counts, removing total row and duplicates,\n",
    "    and append new records to the database.\n",
    "    \"\"\"\n",
    "    # Fetch data\n",
    "    Current_ResponseTimes_data = fetch_response_times()\n",
    "    Current_ResponseTimes_data = Current_ResponseTimes_data[~Current_ResponseTimes_data['Period'].isna()]\n",
    "    Current_ResponseTimes_data['Period'] = Current_ResponseTimes_data['Period'].astype(str)\n",
    "    Current_ResponseTimes_data['Company'] = Current_ResponseTimes_data['Company'].astype(str)\n",
    "\n",
    "    Current_ErrorCount_data = determine_error_count()\n",
    "    Current_ErrorCount_data = Current_ErrorCount_data.rename(columns={'Client': 'Company'})\n",
    "    Current_ErrorCount_data = Current_ErrorCount_data.drop(['Period'], axis=1)\n",
    "    Current_ErrorCount_data['Company'] = Current_ErrorCount_data['Company'].astype(str)\n",
    "\n",
    "    # Merge data\n",
    "    Current_Errors_n_ResponseTimes_data = pd.merge(\n",
    "        Current_ResponseTimes_data, Current_ErrorCount_data, on=['Company'], how='left'\n",
    "    )\n",
    "    \n",
    "    # Convert error columns to Int64\n",
    "    columns_to_convert = ['FunctionalErrors', 'NonFunctionalErrors']\n",
    "    for col in columns_to_convert:\n",
    "        Current_Errors_n_ResponseTimes_data[col] = pd.to_numeric(\n",
    "            Current_Errors_n_ResponseTimes_data[col], errors='coerce'\n",
    "        ).astype('Int64')\n",
    "\n",
    "    # Remove duplicates\n",
    "    duplicates = Current_Errors_n_ResponseTimes_data[\n",
    "        Current_Errors_n_ResponseTimes_data.duplicated(subset=['Period', 'Company'], keep=False)\n",
    "    ]\n",
    "    if not duplicates.empty:\n",
    "        print(\"Warning: Removing duplicates based on Period and Company:\")\n",
    "        print(duplicates[['Period', 'Company']])\n",
    "        Current_Errors_n_ResponseTimes_data = Current_Errors_n_ResponseTimes_data.drop_duplicates(\n",
    "            subset=['Period', 'Company']\n",
    "        )\n",
    "\n",
    "    # Database setup\n",
    "    db_path = os.path.join(background_data_dir, 'previous_errors_n_reponse_times_data.db')\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    engine = create_engine(f'sqlite:///{db_path}', echo=True)\n",
    "\n",
    "    # Verify period\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            inspector = inspect(engine)\n",
    "            if not inspector.has_table('PreviousErrorData'):\n",
    "                print(\"Warning: PreviousErrorData table does not exist. Creating it.\")\n",
    "                Current_Errors_n_ResponseTimes_data.to_sql(\n",
    "                    'PreviousErrorData', con=engine, if_exists='replace', index=False\n",
    "                )\n",
    "            else:\n",
    "                last_entry = pd.read_sql(\n",
    "                    \"SELECT Period FROM PreviousErrorData ORDER BY Period DESC LIMIT 1\", conn\n",
    "                )\n",
    "                if last_entry.empty:\n",
    "                    print(\"Warning: PreviousErrorData table is empty. Cannot verify Period.\")\n",
    "                else:\n",
    "                    last_period = last_entry['Period'].iloc[0]\n",
    "                    last_period_prefix = int(last_period[:6])\n",
    "                    new_period_prefix = int(Current_ResponseTimes_data['Period'].iloc[0][:6])\n",
    "                    if new_period_prefix == last_period_prefix + 1:\n",
    "                        print(\n",
    "                            f\"Verification successful: New Period ({new_period_prefix}) is one week after last Period ({last_period_prefix}).\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Verification failed: New Period ({new_period_prefix}) is not one more than last Period ({last_period_prefix}).\"\n",
    "                        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {e}\")\n",
    "        with open('error_log.txt', 'a') as f:\n",
    "            f.write(f\"Error during verification: {e}\\n\")\n",
    "\n",
    "    # Append new records\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            existing_records = pd.read_sql(\n",
    "                \"SELECT Period, Company FROM PreviousErrorData WHERE Period IN :periods\",\n",
    "                conn,\n",
    "                params={'periods': tuple(Current_Errors_n_ResponseTimes_data['Period'].unique())}\n",
    "            )\n",
    "\n",
    "            # Ensure consistent data types\n",
    "            for df in [Current_Errors_n_ResponseTimes_data, existing_records]:\n",
    "                df['Period'] = df['Period'].astype(str)\n",
    "                df['Company'] = df['Company'].astype(str)\n",
    "\n",
    "            new_records = Current_Errors_n_ResponseTimes_data.merge(\n",
    "                existing_records,\n",
    "                on=['Period', 'Company'],\n",
    "                how='left',\n",
    "                indicator=True\n",
    "            ).query('_merge == \"left_only\"').drop(columns='_merge')\n",
    "\n",
    "            if new_records.empty:\n",
    "                print(\"No new records to append; all records already exist in PreviousErrorData.\")\n",
    "            else:\n",
    "                new_records.to_sql('PreviousErrorData', con=engine, if_exists='append', index=False)\n",
    "                print(f\"Successfully appended {len(new_records)} new records to PreviousErrorData table.\")\n",
    "                print(\"Appended records:\")\n",
    "                print(new_records[['Period', 'Company']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking/appending data: {e}\")\n",
    "        with open('error_log.txt', 'a') as f:\n",
    "            f.write(f\"Error checking/appending data: {e}\\n\")\n",
    "\n",
    "    # Fetch previous data and concatenate\n",
    "    Previous_Errors_n_ResponseTimes_data = fetch_previous()\n",
    "    Updated_Errors_n_ResponseTimes_data = pd.concat(\n",
    "        [Previous_Errors_n_ResponseTimes_data, Current_Errors_n_ResponseTimes_data],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    Updated_Errors_n_ResponseTimes_data = Updated_Errors_n_ResponseTimes_data.drop_duplicates(\n",
    "        subset=['Period', 'Company'], keep='last'\n",
    "    )\n",
    "\n",
    "    return Updated_Errors_n_ResponseTimes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c1557",
   "metadata": {},
   "source": [
    "#### write to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Updated_Errors_n_ResponseTimes_data = concat_errors_n_responsetimes()\n",
    "Updated_Errors_n_ResponseTimes_data = Updated_Errors_n_ResponseTimes_data.where(pd.notnull(Updated_Errors_n_ResponseTimes_data), None)\n",
    "ClientNames_dict = fetch_client_names()\n",
    "Nonstrd_ClientNames_list = [item for value in ClientNames_dict.values() for item in (value if isinstance(value, (list, tuple)) else [value])]\n",
    "if not Updated_Errors_n_ResponseTimes_data['Company'].isin(Nonstrd_ClientNames_list).any():\n",
    "    print(\"No matches found.\")\n",
    "else:\n",
    "    print(\"There is at least one match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f8ecf",
   "metadata": {},
   "source": [
    "2025-05-21 15:46:01,159 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
    "2025-05-21 15:46:01,159 - INFO - BEGIN (implicit)\n",
    "2025-05-21 15:46:01,167 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"SELECT Period, Company FROM PreviousErrorData WHERE Period IN :periods\")\n",
    "2025-05-21 15:46:01,167 - INFO - PRAGMA main.table_info(\"SELECT Period, Company FROM PreviousErrorData WHERE Period IN :periods\")\n",
    "2025-05-21 15:46:01,173 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
    "2025-05-21 15:46:01,173 - INFO - [raw sql] ()\n",
    "2025-05-21 15:46:01,174 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"SELECT Period, Company FROM PreviousErrorData WHERE Period IN :periods\")\n",
    "2025-05-21 15:46:01,174 - INFO - PRAGMA temp.table_info(\"SELECT Period, Company FROM PreviousErrorData WHERE Period IN :periods\")\n",
    "2025-05-21 15:46:01,176 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
    "2025-05-21 15:46:01,176 - INFO - [raw sql] ()\n",
    "2025-05-21 15:46:01,179 INFO sqlalchemy.engine.Engine SELECT Period, Company FROM PreviousErrorData WHERE Period IN :periods\n",
    "2025-05-21 15:46:01,179 - INFO - SELECT Period, Company FROM PreviousErrorData WHERE Period IN :periods\n",
    "2025-05-21 15:46:01,180 INFO sqlalchemy.engine.Engine [raw sql] {'periods': ('202517 - 24 Apr 2025 - 01 May 2025',)}\n",
    "2025-05-21 15:46:01,180 - INFO - [raw sql] {'periods': ('202517 - 24 Apr 2025 - 01 May 2025',)}\n",
    "2025-05-21 15:46:01,182 INFO sqlalchemy.engine.Engine ROLLBACK\n",
    "2025-05-21 15:46:01,182 - INFO - ROLLBACK\n",
    "2025-05-21 15:46:01,229 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Error checking/appending data: (sqlite3.OperationalError) near \":periods\": syntax error\n",
    "[SQL: SELECT Period, Company FROM PreviousErrorData WHERE Period IN :periods]\n",
    "[parameters: {'periods': ('202517 - 24 Apr 2025 - 01 May 2025',)}]\n",
    "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
    "2025-05-21 15:46:03,902 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Client Names standardized for Previous_Errors_n_ResponseTimes_data\n",
    "No matches found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gc, _ = authenticate_gdrive()\n",
    "    #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "    #gc = gspread.authorize(creds)\n",
    "    spreadsheet = gc.open('KIT 3 Online Reporting Data')\n",
    "\n",
    "    # Check if worksheet exists, create if it doesn't\n",
    "    worksheet_title = 'Data Errors and Reponse Times'\n",
    "    try:\n",
    "        sheet = spreadsheet.worksheet(worksheet_title)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        sheet = spreadsheet.add_worksheet(title=worksheet_title, rows=100, cols=26)  # Initial size: 100 rows, 26 cols (A-Z)\n",
    "        logging.info(f\"Created new worksheet: {worksheet_title}\")\n",
    "\n",
    "    # Prepare data: Fill NaN values based on column type\n",
    "    data_filled = Updated_Errors_n_ResponseTimes_data.copy()\n",
    "\n",
    "    for col in data_filled.columns:\n",
    "        if data_filled[col].dtype == 'Float64':\n",
    "            data_filled[col] = data_filled[col].fillna(0)\n",
    "        else:\n",
    "            data_filled[col] = data_filled[col].fillna('')\n",
    "\n",
    "    # Convert DataFrame to list for writing to sheet\n",
    "    data = [data_filled.columns.tolist()] + data_filled.astype(object).where(pd.notnull(data_filled), None).values.tolist()\n",
    "\n",
    "    # Clear the sheet and write the data\n",
    "    execute_with_backoff(sheet.clear)\n",
    "    execute_with_backoff(sheet.update, data, 'A1', value_input_option='RAW')\n",
    "\n",
    "    # Format the data as a table\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(data[0])\n",
    "    range_notation = f'A1:{chr(65 + num_cols - 1)}{num_rows}'\n",
    "\n",
    "    # Set header row height using batch update\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateDimensionProperties\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet.id,\n",
    "                            \"dimension\": \"ROWS\",\n",
    "                            \"startIndex\": 0,  # Row 1 (0-based index)\n",
    "                            \"endIndex\": 1     # Only the first row\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"pixelSize\": 30  # Set height to 30 pixels\n",
    "                        },\n",
    "                        \"fields\": \"pixelSize\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Bold headers and format\n",
    "    format_cell_range(\n",
    "        sheet,\n",
    "        f'A1:{chr(65 + num_cols - 1)}1',\n",
    "        CellFormat(\n",
    "                textFormat=TextFormat(bold=True),\n",
    "                horizontalAlignment='CENTER',\n",
    "                verticalAlignment='MIDDLE',\n",
    "                backgroundColor=Color(0.9, 0.9, 0.9)  # Light gray background for headers\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add borders to create a table effect\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            range_notation,\n",
    "            CellFormat(\n",
    "                borders=Borders(\n",
    "                    top=Border('SOLID'),\n",
    "                    bottom=Border('SOLID'),\n",
    "                    left=Border('SOLID'),\n",
    "                    right=Border('SOLID')\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Auto-resize columns for better readability\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"autoResizeDimensions\": {\n",
    "                        \"dimensions\": {\n",
    "                            \"sheetId\": sheet.id,\n",
    "                            \"dimension\": \"COLUMNS\",\n",
    "                            \"startIndex\": 0,\n",
    "                            \"endIndex\": num_cols\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Add filter buttons to the table\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"setBasicFilter\": {\n",
    "                        \"filter\": {\n",
    "                            \"range\": {\n",
    "                                \"sheetId\": sheet.id,\n",
    "                                \"startRowIndex\": 0,  # Start at row 1\n",
    "                                \"endRowIndex\": num_rows,  # End at last row\n",
    "                                \"startColumnIndex\": 0,  # Start at column A\n",
    "                                \"endColumnIndex\": num_cols  # End at last column\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Overwrote {len(Updated_Errors_n_ResponseTimes_data)} rows in {sheet}, formatted as a table, and added filter buttons\")\n",
    "       \n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to write DataFrame to sheet, apply formatting, or add filter: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd6a37",
   "metadata": {},
   "source": [
    "2025-05-21 15:46:20,187 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:46:34,533 - INFO - Overwrote 5031 rows in <Worksheet 'Data Errors and Reponse Times' id:1289794990>, formatted as a table, and added filter buttons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42c78c",
   "metadata": {},
   "source": [
    "#### create accompanying pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb33225",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gc, _ = authenticate_gdrive()\n",
    "    #creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "    #gc = gspread.authorize(creds)\n",
    "    spreadsheet = gc.open('KIT 3 Online Reporting Data')\n",
    "\n",
    "    # Access the data sheet\n",
    "    data_sheet = spreadsheet.worksheet('Data Errors and Reponse Times')\n",
    "\n",
    "    # Create or access a new sheet for pivot table, slicers, and chart\n",
    "    pivot_sheet_title = 'Errors Report'\n",
    "    try:\n",
    "        pivot_sheet = spreadsheet.worksheet(pivot_sheet_title)\n",
    "        execute_with_backoff(pivot_sheet.clear)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        pivot_sheet = spreadsheet.add_worksheet(title=pivot_sheet_title, rows=100, cols=26)\n",
    "        logging.info(f\"Created new worksheet: {pivot_sheet_title}\")\n",
    "\n",
    "    _ , sheets_service = authenticate_gsheets()\n",
    "    #sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "    sheet_id = spreadsheet.id\n",
    "    pivot_sheet_id = pivot_sheet.id\n",
    "\n",
    "    # Determine column indices based on your data headers\n",
    "    headers = data_sheet.row_values(1)  # Assuming headers are in the first row\n",
    "    company_idx = headers.index('Company') if 'Company' in headers else 0\n",
    "    functional_error_idx = headers.index('Functional Error Count') if 'Functional Error Count' in headers else 1\n",
    "    non_functional_error_idx = headers.index('Non Functional Error Count') if 'Non Functional Error Count' in headers else 2\n",
    "    period_idx = headers.index('Period') if 'Period' in headers else 3\n",
    "\n",
    "    # Define the pivot table with filter for PeriodName\n",
    "    pivot_table_request = {\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateCells\": {\n",
    "                        \"rows\": [\n",
    "                            {\n",
    "                                \"values\": [\n",
    "                                    {\n",
    "                                        \"pivotTable\": {\n",
    "                                            \"source\": {\n",
    "                                                \"sheetId\": data_sheet.id,\n",
    "                                                \"startRowIndex\": 0,\n",
    "                                                \"startColumnIndex\": 0,\n",
    "                                                \"endRowIndex\": len(data_sheet.get_all_values()),\n",
    "                                                \"endColumnIndex\": len(data_sheet.get_all_values()[0])\n",
    "                                            },\n",
    "                                            \"rows\": [\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": company_idx,\n",
    "                                                    \"showTotals\": True,\n",
    "                                                    \"sortOrder\": \"ASCENDING\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"values\": [\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": functional_error_idx,\n",
    "                                                    \"summarizeFunction\": \"SUM\",\n",
    "                                                    \"name\": \"Functional Error Count\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": non_functional_error_idx,\n",
    "                                                    \"summarizeFunction\": \"SUM\",\n",
    "                                                    \"name\": \"Non Functional Error Count\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"criteria\": {\n",
    "                                                str(period_idx): {\n",
    "                                                    \"visibleValues\": [PeriodName]\n",
    "                                                }\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        ],\n",
    "                        \"start\": {\n",
    "                            \"sheetId\": pivot_sheet_id,\n",
    "                            \"rowIndex\": 0,\n",
    "                            \"columnIndex\": 0\n",
    "                        },\n",
    "                        \"fields\": \"*\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # Execute pivot table creation\n",
    "    sheets_service.spreadsheets().batchUpdate(spreadsheetId=sheet_id, body=pivot_table_request).execute()\n",
    "    logging.info(\"Created pivot table in 'Errors Report' sheet\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create pivot table or slicers: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82a074",
   "metadata": {},
   "source": [
    "2025-05-21 15:46:42,892 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:46:47,133 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:46:50,595 - INFO - Created pivot table in 'Errors Report' sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56666df7",
   "metadata": {},
   "source": [
    "# COPY DATA AND PASTE DATA BETWEEN WORKBOOKS \n",
    "copy data from Current Errors and Follow Up sheets and add to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREADSHEET_ID = os.getenv('SPREADSHEET_ID', '1m49V3mKZiuTatraSSGqo1xHLIhcQRmJZO90S4Y72vcc') # CURRENT\n",
    "DEST_SPREADSHEET_ID = os.getenv('DEST_SPREADSHEET_ID', '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI') # KIT 3 ONLINE REPORT\n",
    "\n",
    "gc, sheets_service = authenticate_gsheets()\n",
    "\n",
    "# Open source and destination spreadsheets\n",
    "spreadsheet = execute_with_backoff(gc.open_by_key, SPREADSHEET_ID)\n",
    "dest_spreadsheet = execute_with_backoff(gc.open_by_key, DEST_SPREADSHEET_ID)\n",
    "\n",
    "# Cache worksheets to reduce API calls\n",
    "source_worksheets = execute_with_backoff(spreadsheet.worksheets)\n",
    "dest_worksheets = execute_with_backoff(dest_spreadsheet.worksheets)\n",
    "\n",
    "source_worksheet_ids = {sheet.title: sheet.id for sheet in source_worksheets}\n",
    "dest_worksheet_ids = {sheet.title: sheet.id for sheet in dest_worksheets}\n",
    "\n",
    "# Define sheet names\n",
    "source_sheet_name = CurrentSheetName  \n",
    "dest_sheet_name = 'Current Week Error'\n",
    "\n",
    "# Copy and rename sheet if needed\n",
    "if source_sheet_name in source_worksheet_ids and source_sheet_name not in dest_worksheet_ids:\n",
    "    # Copy the sheet\n",
    "    copy_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().sheets().copyTo,\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        sheetId=source_worksheet_ids[source_sheet_name],\n",
    "        body={'destinationSpreadsheetId': DEST_SPREADSHEET_ID}\n",
    "    )\n",
    "    copied_sheet_id = copy_response['sheetId']\n",
    "    logging.info(f\"Copied sheet '{source_sheet_name}' to destination.\")\n",
    "\n",
    "    # Delete existing destination sheet if it exists\n",
    "    if dest_sheet_name in dest_worksheet_ids:\n",
    "        delete_request = {\n",
    "            'requests': [{\n",
    "                'deleteSheet': {\n",
    "                    'sheetId': dest_worksheet_ids[dest_sheet_name]\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "            body=delete_request\n",
    "        )\n",
    "        logging.info(f\"Deleted existing sheet '{dest_sheet_name}' from destination.\")\n",
    "\n",
    "    # Rename the copied sheet and ensure it's not hidden\n",
    "    rename_and_unhide_request = {\n",
    "    'requests': [{\n",
    "        'updateSheetProperties': {\n",
    "            'properties': {\n",
    "                'sheetId': copied_sheet_id,\n",
    "                'title': dest_sheet_name,\n",
    "                'hidden': False\n",
    "            },\n",
    "            'fields': 'title,hidden'\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    execute_with_backoff(\n",
    "    sheets_service.spreadsheets().batchUpdate,\n",
    "    spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "    body=rename_and_unhide_request\n",
    "    )\n",
    "    logging.info(f\"Renamed copied sheet to '{dest_sheet_name}' and ensured it is visible.\")\n",
    "\n",
    "\n",
    "    # Adjust column widths\n",
    "    resize_columns_request = {\n",
    "    'requests': [\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 0,  # Column A\n",
    "                    'endIndex': 1\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 170  # Slightly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 1,  # Column B\n",
    "                    'endIndex': 2\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 120  # Slightly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 2,  # Column C\n",
    "                    'endIndex': 3\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 400  # Significantly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "        body=resize_columns_request\n",
    "    )\n",
    "    logging.info(\"Adjusted column widths for columns A, B (slightly), and C (significantly).\")\n",
    "\n",
    "else:\n",
    "    logging.info(f\"No sheet copied. Either '{source_sheet_name}' doesn't exist in source or '{dest_sheet_name}' already exists in destination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfcd0d",
   "metadata": {},
   "source": [
    "2025-05-20 10:06:49,144 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-20 10:06:53,797 - INFO - Copied sheet '24 Apr - 30 Apr' to destination.\n",
    "2025-05-20 10:06:54,907 - INFO - Deleted existing sheet 'Current Week Error' from destination.\n",
    "2025-05-20 10:06:55,495 - INFO - Renamed copied sheet to 'Current Week Error' and ensured it is visible.\n",
    "2025-05-20 10:06:56,035 - INFO - Adjusted column widths for columns A, B (slightly), and C (significantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREADSHEET_ID = os.getenv('SPREADSHEET_ID', '13IpNi1rFg8luMX8t_OlhCbAQcEhkZkZUmwFWP3C_8N8') # FOLLOW UP\n",
    "DEST_SPREADSHEET_ID = os.getenv('DEST_SPREADSHEET_ID', '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI') # KIT3 REPORT\n",
    "\n",
    "gc, sheets_service = authenticate_gsheets()\n",
    "\n",
    "# Open source and destination spreadsheets\n",
    "spreadsheet = execute_with_backoff(gc.open_by_key, SPREADSHEET_ID)\n",
    "dest_spreadsheet = execute_with_backoff(gc.open_by_key, DEST_SPREADSHEET_ID)\n",
    "\n",
    "# Cache worksheets to reduce API calls\n",
    "source_worksheets = execute_with_backoff(spreadsheet.worksheets)\n",
    "dest_worksheets = execute_with_backoff(dest_spreadsheet.worksheets)\n",
    "\n",
    "source_worksheet_ids = {sheet.title: sheet.id for sheet in source_worksheets}\n",
    "dest_worksheet_ids = {sheet.title: sheet.id for sheet in dest_worksheets}\n",
    "\n",
    "# Define sheet names\n",
    "source_sheet_name = PreviousSheetName\n",
    "dest_sheet_name = 'Feedback on Previous Week Error'\n",
    "\n",
    "# Copy and rename sheet if needed\n",
    "if source_sheet_name in source_worksheet_ids and source_sheet_name not in dest_worksheet_ids:\n",
    "    # Copy the sheet\n",
    "    copy_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().sheets().copyTo,\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        sheetId=source_worksheet_ids[source_sheet_name],\n",
    "        body={'destinationSpreadsheetId': DEST_SPREADSHEET_ID}\n",
    "    )\n",
    "    copied_sheet_id = copy_response['sheetId']\n",
    "    logging.info(f\"Copied sheet '{source_sheet_name}' to destination.\")\n",
    "\n",
    "    # Delete existing destination sheet if it exists\n",
    "    if dest_sheet_name in dest_worksheet_ids:\n",
    "        delete_request = {\n",
    "            'requests': [{\n",
    "                'deleteSheet': {\n",
    "                    'sheetId': dest_worksheet_ids[dest_sheet_name]\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "            body=delete_request\n",
    "        )\n",
    "        logging.info(f\"Deleted existing sheet '{dest_sheet_name}' from destination.\")\n",
    "\n",
    "    # Rename the copied sheet and ensure it's not hidden\n",
    "    rename_and_unhide_request = {\n",
    "    'requests': [{\n",
    "        'updateSheetProperties': {\n",
    "            'properties': {\n",
    "                'sheetId': copied_sheet_id,\n",
    "                'title': dest_sheet_name,\n",
    "                'hidden': False\n",
    "            },\n",
    "            'fields': 'title,hidden'\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "        body=rename_and_unhide_request\n",
    "    )\n",
    "    logging.info(f\"Renamed copied sheet to '{dest_sheet_name}' and ensured it is visible.\")\n",
    "\n",
    "\n",
    "    # Adjust column widths\n",
    "    resize_columns_request = {\n",
    "    'requests': [\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 0,  # Column A\n",
    "                    'endIndex': 1\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 170  # Slightly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 1,  # Column B\n",
    "                    'endIndex': 2\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 120  # Slightly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 2,  # Column C\n",
    "                    'endIndex': 3\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 400  # Significantly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "        body=resize_columns_request\n",
    "    )\n",
    "    logging.info(\"Adjusted column widths for columns A, B (slightly), and C (significantly).\")\n",
    "\n",
    "else:\n",
    "    logging.info(f\"No sheet copied. Either '{source_sheet_name}' doesn't exist in source or '{dest_sheet_name}' already exists in destination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65ab70",
   "metadata": {},
   "source": [
    "2025-05-20 10:06:56,074 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-20 10:07:02,647 - INFO - Copied sheet '17 Apr - 23 Apr' to destination.\n",
    "2025-05-20 10:07:03,264 - INFO - Deleted existing sheet 'Feedback on Previous Week Error' from destination.\n",
    "2025-05-20 10:07:03,869 - INFO - Renamed copied sheet to 'Feedback on Previous Week Error' and ensured it is visible.\n",
    "2025-05-20 10:07:04,503 - INFO - Adjusted column widths for columns A, B (slightly), and C (significantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start of alteration\"\"\"\n",
    "designated_file_path = os.path.join(reports_dir, Designated_Filename) # specify using os.path.join for futureproffing with pyw file\n",
    "#designated_file_path = f'Reports Folder/{Designated_Filename}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe35e8a",
   "metadata": {},
   "source": [
    "### download file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, drive_service = authenticate_gdrive()\n",
    "\n",
    "# Spreadsheet ID\n",
    "SPREADSHEET_ID = '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI'\n",
    "\n",
    "# Export Google Sheet as Excel\n",
    "request = drive_service.files().export_media(\n",
    "    fileId=SPREADSHEET_ID,\n",
    "    mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    ")\n",
    "\n",
    "\n",
    "fh = io.FileIO(designated_file_path, 'wb')\n",
    "downloader = MediaIoBaseDownload(fh, request)\n",
    "done = False\n",
    "while not done:\n",
    "    status, done = downloader.next_chunk()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511f846",
   "metadata": {},
   "source": [
    "2025-05-20 10:28:29,605 - INFO - file_cache is only supported with oauth2client<4.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8439f5",
   "metadata": {},
   "source": [
    "### open folder location for manual alternation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(script_dir, 'Reports Folder') #r\"C:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Reports Folder\"\n",
    "os.startfile(folder_path)\n",
    "\n",
    "#file_path = folder_path + '\\\\' + Designated_Filename\n",
    "file_path = os.path.join(folder_path, Designated_Filename)\n",
    "os.startfile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8157bb7",
   "metadata": {},
   "source": [
    "### confirm manual alteration\n",
    "needs troubleshooting - gets stuck in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user_confirmation():\n",
    "    root = tk.Tk()\n",
    "    result = messagebox.askyesno(\"Confirmation\", \"Has the file been checked and graph added?\")\n",
    "    root.destroy()  # Destroy the hidden main window\n",
    "    return result\n",
    "\n",
    "ask_user_confirmation()\n",
    "if not ask_user_confirmation():\n",
    "    print(\"User did not confirm. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Rest of your code goes here\n",
    "print(\"User confirmed. Continuing with the rest of the program.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def show_confirmation():\n",
    "    # Create a root window and hide it (needed for messagebox to work)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    # Show the Yes/No confirmation popup\n",
    "    confirmed = messagebox.askyesno(\"Confirmation\", \"Has the file been checked and graph added?\")\n",
    "    \n",
    "    root.destroy()  # Destroy the hidden root window after the dialog\n",
    "\n",
    "    return confirmed\n",
    "\n",
    "# Show the confirmation dialog\n",
    "if show_confirmation():\n",
    "    print(\"User confirmed. Continuing with the rest of the code...\")\n",
    "else:\n",
    "    print(\"User did not confirm. Exiting or taking alternative action.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a27619",
   "metadata": {},
   "source": [
    "User confirmed. Continuing with the rest of the code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f68d2",
   "metadata": {},
   "source": [
    "### email "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GMAIL_SCOPES = ['https://www.googleapis.com/auth/gmail.send']\n",
    "TOKEN_FILE = os.path.join(background_data_dir, 'GmailToken.json') #'Background Data/GmailToken.json' # change to os.path method\n",
    "CREDENTIALS_FILE = os.path.join(background_data_dir, 'GmailAuth.json') #'Background Data/GmailAuth.json' # change to os.path method\n",
    "\n",
    "creds = None\n",
    "if os.path.exists(TOKEN_FILE):\n",
    "    try:\n",
    "        creds = Credentials.from_authorized_user_file(TOKEN_FILE, GMAIL_SCOPES)\n",
    "        print(f\"Loaded token, valid: {creds.valid}, expired: {creds.expired}, expiry: {creds.expiry}, refresh_token: {creds.refresh_token}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading token: {e}\")\n",
    "        os.remove(TOKEN_FILE)\n",
    "\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        try:\n",
    "            creds.refresh(Request())\n",
    "            print(\"Token refreshed successfully.\")\n",
    "        except RefreshError as e:\n",
    "            print(f\"Refresh failed: {e}\")\n",
    "            print(f\"Error details: {e.args}\")\n",
    "            os.remove(TOKEN_FILE)\n",
    "            creds = None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected refresh error: {e}\")\n",
    "            os.remove(TOKEN_FILE)\n",
    "            creds = None\n",
    "    if not creds:\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_FILE, GMAIL_SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "            print(\"OAuth flow completed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"OAuth flow failed: {e}\")\n",
    "            exit()\n",
    "\n",
    "    try:\n",
    "        with open(TOKEN_FILE, 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "        print(f\"Token saved to {os.path.abspath(TOKEN_FILE)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving token: {e}\")\n",
    "        exit()\n",
    "\n",
    "print(\"Authentication complete.\")\n",
    "\n",
    "\n",
    "\"\"\"end alteration\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3668e",
   "metadata": {},
   "source": [
    "Loaded token, valid: True, expired: False, expiry: 2025-05-20 10:04:46, refresh_token: 1//03Gq_DGcAqaPrCgYIARAAGAMSNwF-L9IrMdWx2R8GaUtcV8ZgX7Bib0l82DVsyRYYHW_WIR-vVhwqcTnbG5-tDhrg1Tu3X7fypls\n",
    "Authentication complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e9161",
   "metadata": {},
   "source": [
    "# CREATE NEW SHEETS \n",
    "add new column to Follow Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded column widths (in pixels) for specific columns (0-based index)\n",
    "COLUMN_WIDTHS = {\n",
    "    0: 200,  # Column A\n",
    "    1: 150,  # Column B\n",
    "    2: 300,  # Column C\n",
    "    3: 150,  # Column D\n",
    "    4: 150, #E\n",
    "    5: 150, #F\n",
    "    6: 150, #G\n",
    "    7: 150, \n",
    "    8: 150,\n",
    "    9: 150\n",
    "}\n",
    "\n",
    "CURRENT_SPREADSHEET_ID = os.getenv('CURRENT_SPREADSHEET_ID', '1m49V3mKZiuTatraSSGqo1xHLIhcQRmJZO90S4Y72vcc')  # CURRENT\n",
    "FOLLOWUP_SPREADSHEET_ID = os.getenv('FOLLOWUP_SPREADSHEET_ID', '13IpNi1rFg8luMX8t_OlhCbAQcEhkZkZUmwFWP3C_8N8')  # FOLLOW UP\n",
    "TEMPLATE_NAME = 'TEMPLATE - Do not edit'\n",
    "\n",
    "gc, sheets_service = authenticate_gsheets()\n",
    "\n",
    "# Open spreadsheets\n",
    "current_spreadsheet = execute_with_backoff(gc.open_by_key, CURRENT_SPREADSHEET_ID)\n",
    "template_sheet = current_spreadsheet.get_worksheet_by_id(552319826)\n",
    "followup_spreadsheet = execute_with_backoff(gc.open_by_key, FOLLOWUP_SPREADSHEET_ID)\n",
    "\n",
    "# Cache worksheets to reduce API calls\n",
    "current_worksheets = execute_with_backoff(current_spreadsheet.worksheets)\n",
    "followup_worksheets = execute_with_backoff(followup_spreadsheet.worksheets)\n",
    "\n",
    "current_worksheet_ids = {sheet.title: sheet.id for sheet in current_worksheets}\n",
    "followup_worksheet_ids = {sheet.title: sheet.id for sheet in followup_worksheets}\n",
    "\n",
    "# Define sheet names \n",
    "next_sheet_name = NextSheetName  \n",
    "followup_sheet_name = CurrentSheetName  \n",
    "\n",
    "if TEMPLATE_NAME in current_worksheet_ids:\n",
    "    template_sheet_id = current_worksheet_ids[TEMPLATE_NAME]\n",
    "    unhide_request = {\n",
    "        'requests': [{\n",
    "            'updateSheetProperties': {\n",
    "                'properties': {\n",
    "                    'sheetId': template_sheet_id,\n",
    "                    'hidden': False\n",
    "                },\n",
    "                'fields': 'hidden'\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    try:\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "            body=unhide_request\n",
    "        )\n",
    "        logging.info(f\"Unhid template sheet '{TEMPLATE_NAME}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error unhiding template sheet '{TEMPLATE_NAME}': {e}\")\n",
    "        raise\n",
    "else:\n",
    "    logging.error(f\"Template sheet '{TEMPLATE_NAME}' not found.\")\n",
    "    raise Exception(f\"Template sheet '{TEMPLATE_NAME}' not found.\")\n",
    "\n",
    "if next_sheet_name in current_worksheet_ids:\n",
    "    logging.info(f\"Worksheet '{next_sheet_name}' already exists. Deleting it.\")\n",
    "    sheet_id = current_worksheet_ids[next_sheet_name]\n",
    "    delete_request = {\n",
    "        'requests': [{\n",
    "            'deleteSheet': {\n",
    "                'sheetId': sheet_id\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "        body=delete_request\n",
    "    )\n",
    "    \n",
    "    try:   \n",
    "        del current_worksheet_ids[next_sheet_name]\n",
    "        time.sleep(1)\n",
    "        updated_current_worksheets = execute_with_backoff(current_spreadsheet.worksheets)\n",
    "        updated_current_worksheet_ids = {sheet.title: sheet.id for sheet in updated_current_worksheets}\n",
    "        \n",
    "        if next_sheet_name in updated_current_worksheet_ids:\n",
    "            raise Exception(f\"Failed to delete worksheet '{next_sheet_name}'.\")\n",
    "        logging.info(f\"Deleted worksheet '{next_sheet_name}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error deleting worksheet '{next_sheet_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    new_sheet = execute_with_backoff(\n",
    "        current_spreadsheet.duplicate_sheet,\n",
    "        source_sheet_id=template_sheet.id,\n",
    "        new_sheet_name=next_sheet_name)\n",
    "    \n",
    "    current_worksheet_ids[next_sheet_name] = new_sheet.id\n",
    "    \n",
    "    logging.info(f\"Created worksheet '{next_sheet_name}'.\")\n",
    "\n",
    "    # Set column widths for the new sheet\n",
    "    width_requests = [\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': new_sheet.id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': col_index,\n",
    "                    'endIndex': col_index + 1\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': width\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        } for col_index, width in COLUMN_WIDTHS.items()\n",
    "    ]\n",
    "    if width_requests:\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "            body={'requests': width_requests}\n",
    "        )\n",
    "        logging.info(f\"Set column widths for '{next_sheet_name}'.\")\n",
    "\n",
    "    # Update 'Period' column\n",
    "    values = execute_with_backoff(new_sheet.get_all_values)\n",
    "    header = values[0]\n",
    "    try:\n",
    "        period_col_index = header.index(\"Period\")\n",
    "        period_col_letter = chr(ord('A') + period_col_index)\n",
    "    except ValueError:\n",
    "        raise Exception(\"Column 'Period' not found.\")\n",
    "    updates = [\n",
    "        {'range': f\"{period_col_letter}{row_idx}\", 'values': [[next_sheet_name]]}\n",
    "        for row_idx, row in enumerate(values[1:], start=2)\n",
    "        if len(row) > period_col_index and row[period_col_index].strip()\n",
    "    ]\n",
    "    if updates:\n",
    "        execute_with_backoff(new_sheet.batch_update, updates)\n",
    "        logging.info(f\"Updated {len(updates)} 'Period' cells.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating worksheet '{next_sheet_name}': {e}\")\n",
    "    raise\n",
    "\n",
    "if followup_sheet_name in current_worksheet_ids and followup_sheet_name in followup_worksheet_ids:\n",
    "    logging.info(f\"Worksheet '{followup_sheet_name}' already exists. Overwriting it.\")\n",
    "    sheet_id = followup_worksheet_ids[followup_sheet_name]\n",
    "\n",
    "    # Clear the existing sheet content\n",
    "    try:\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().values().clear,\n",
    "            spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "            range=f\"{followup_sheet_name}!A1:Z\",  # Pass range directly\n",
    "            body={}  # Empty body, as no additional metadata is needed\n",
    "        )\n",
    "        logging.info(f\"Cleared content of worksheet '{followup_sheet_name}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error clearing worksheet '{followup_sheet_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "    # Copy data from the source sheet\n",
    "    try:\n",
    "        # Get data from the source sheet\n",
    "        source_data = execute_with_backoff(\n",
    "            sheets_service.spreadsheets().values().get,\n",
    "            spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "            range=f\"{followup_sheet_name}!A1:Z\"\n",
    "        ).get('values', [])\n",
    "\n",
    "        # Write data to the existing sheet\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().values().update,\n",
    "            spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "            range=f\"{followup_sheet_name}!A1\",\n",
    "            valueInputOption=\"RAW\",\n",
    "            body={\"values\": source_data}\n",
    "        )\n",
    "\n",
    "        # Update column widths\n",
    "        width_requests = [\n",
    "            {\n",
    "                'updateDimensionProperties': {\n",
    "                    'range': {\n",
    "                        'sheetId': sheet_id,\n",
    "                        'dimension': 'COLUMNS',\n",
    "                        'startIndex': col_index,\n",
    "                        'endIndex': col_index + 1\n",
    "                    },\n",
    "                    'properties': {\n",
    "                        'pixelSize': width\n",
    "                    },\n",
    "                    'fields': 'pixelSize'\n",
    "                }\n",
    "            } for col_index, width in COLUMN_WIDTHS.items()\n",
    "        ]\n",
    "\n",
    "        update_request = {'requests': width_requests}\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "            body=update_request\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Overwrote content and set column widths for sheet '{followup_sheet_name}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error overwriting sheet '{followup_sheet_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "else:\n",
    "    # If the sheet doesn't exist, proceed with copying as before\n",
    "    try:\n",
    "        copy_response = execute_with_backoff(\n",
    "            sheets_service.spreadsheets().sheets().copyTo,\n",
    "            spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "            sheetId=current_worksheet_ids[followup_sheet_name],\n",
    "            body={'destinationSpreadsheetId': FOLLOWUP_SPREADSHEET_ID}\n",
    "        )\n",
    "        \n",
    "        copied_sheet_id = copy_response['sheetId']\n",
    "        rename_request = {\n",
    "            'requests': [{\n",
    "                'updateSheetProperties': {\n",
    "                    'properties': {\n",
    "                        'sheetId': copied_sheet_id,\n",
    "                        'title': followup_sheet_name\n",
    "                    },\n",
    "                    'fields': 'title'\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        # Add column width updates to the rename request\n",
    "        width_requests = [\n",
    "            {\n",
    "                'updateDimensionProperties': {\n",
    "                    'range': {\n",
    "                        'sheetId': copied_sheet_id,\n",
    "                        'dimension': 'COLUMNS',\n",
    "                        'startIndex': col_index,\n",
    "                        'endIndex': col_index + 1\n",
    "                    },\n",
    "                    'properties': {\n",
    "                        'pixelSize': width\n",
    "                    },\n",
    "                    'fields': 'pixelSize'\n",
    "                }\n",
    "            } for col_index, width in COLUMN_WIDTHS.items()\n",
    "        ]\n",
    "        rename_request['requests'].extend(width_requests)\n",
    "        \n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "            body=rename_request\n",
    "        )\n",
    "        \n",
    "        followup_worksheet_ids[followup_sheet_name] = copied_sheet_id\n",
    "        logging.info(f\"Copied, renamed, and set column widths for sheet '{followup_sheet_name}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error copying sheet '{followup_sheet_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45531181",
   "metadata": {},
   "source": [
    "2025-05-21 15:47:13,834 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:47:20,908 - INFO - Unhid template sheet 'TEMPLATE - Do not edit'.\n",
    "2025-05-21 15:47:20,911 - INFO - Worksheet '01 May - 07 May' already exists. Deleting it.\n",
    "2025-05-21 15:47:24,535 - INFO - Deleted worksheet '01 May - 07 May'.\n",
    "2025-05-21 15:47:26,200 - INFO - Created worksheet '01 May - 07 May'.\n",
    "2025-05-21 15:47:27,230 - INFO - Set column widths for '01 May - 07 May'.\n",
    "2025-05-21 15:47:29,166 - INFO - Updated 25 'Period' cells.\n",
    "2025-05-21 15:47:29,171 - INFO - Worksheet '24 Apr - 30 Apr' already exists. Overwriting it.\n",
    "2025-05-21 15:47:30,349 - INFO - Cleared content of worksheet '24 Apr - 30 Apr'.\n",
    "2025-05-21 15:47:33,818 - INFO - Overwrote content and set column widths for sheet '24 Apr - 30 Apr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec722369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_index_to_letter(index):\n",
    "    result = ''\n",
    "    index += 1\n",
    "    while index:\n",
    "        index, rem = divmod(index - 1, 26)\n",
    "        result = chr(65 + rem) + result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id = followup_worksheet_ids[followup_sheet_name]\n",
    "\n",
    "try:\n",
    "    # Step 1: Get sheet data to find the last column\n",
    "    spreadsheet = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().get,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        ranges=[f\"{followup_sheet_name}!A1:ZZ1\"],\n",
    "        includeGridData=True\n",
    "    )\n",
    "\n",
    "    sheet_data = next((s for s in spreadsheet['sheets'] if s['properties']['sheetId'] == sheet_id), None)\n",
    "    if not sheet_data:\n",
    "        raise Exception(f\"Sheet '{followup_sheet_name}' not found in spreadsheet.\")\n",
    "\n",
    "    new_column_index = 10  # Column K\n",
    "    new_column_letter = 'K'\n",
    "\n",
    "    # Step 2: Insert a new column\n",
    "    insert_column_request = {\n",
    "        'requests': [{\n",
    "            'insertDimension': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': new_column_index,\n",
    "                    'endIndex': new_column_index + 1\n",
    "                },\n",
    "                'inheritFromBefore': True\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=insert_column_request\n",
    "    )\n",
    "    logging.info(f\"Inserted new column at index {new_column_index} ({new_column_letter}) in '{followup_sheet_name}'.\")\n",
    "\n",
    "    # Step 3: Set the header to \"Comments\"\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().values().update,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        range=f\"{followup_sheet_name}!{new_column_letter}1\",\n",
    "        valueInputOption='RAW',\n",
    "        body={'values': [['Comments']]}\n",
    "    )\n",
    "    logging.info(f\"Set header 'Comments' in column {new_column_letter} of '{followup_sheet_name}'.\")\n",
    "\n",
    "    # Step 4: Get formatting from column F (entire column)\n",
    "    formatting_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().get,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        ranges=[f\"{followup_sheet_name}!F:F\"],\n",
    "        fields='sheets.data.rowData.values.userEnteredFormat'\n",
    "    )\n",
    "\n",
    "    format_data = formatting_response['sheets'][0]['data'][0].get('rowData', [])\n",
    "    if not format_data:\n",
    "        logging.warning(f\"No formatting data found for column F in '{followup_sheet_name}'. Applying default formatting.\")\n",
    "        column_format = {}\n",
    "    else:\n",
    "        column_format = format_data[0]['values'][0].get('userEnteredFormat', {}) if format_data[0].get('values') else {}\n",
    "\n",
    "    formatting_requests = []\n",
    "    # Copy formatting for each row in column F\n",
    "    for row_index, row in enumerate(format_data):\n",
    "        cell = row.get('values', [])\n",
    "        row_format = cell[0].get('userEnteredFormat', column_format) if cell else column_format\n",
    "\n",
    "        # Prepare the request to copy complete formatting including borders\n",
    "        formatting_requests.append({\n",
    "            'repeatCell': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'startRowIndex': row_index,\n",
    "                    'endRowIndex': row_index + 1,\n",
    "                    'startColumnIndex': new_column_index,\n",
    "                    'endColumnIndex': new_column_index + 1\n",
    "                },\n",
    "                'cell': {\n",
    "                    'userEnteredFormat': row_format\n",
    "                },\n",
    "                'fields': 'userEnteredFormat(backgroundColor,textFormat,borders,padding,horizontalAlignment,verticalAlignment,wrapStrategy,numberFormat)'\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Apply all formatting requests in one batch\n",
    "    if formatting_requests:\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "            body={'requests': formatting_requests}\n",
    "        )\n",
    "        logging.info(f\"Applied complete formatting including borders from column F to column K in '{followup_sheet_name}'.\")\n",
    "    else:\n",
    "        logging.warning(\"No formatting data found in column F.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to add Comments column to '{followup_sheet_name}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5165e",
   "metadata": {},
   "source": [
    "2025-05-21 15:47:49,858 - INFO - Inserted new column at index 10 (K) in '24 Apr - 30 Apr'.\n",
    "2025-05-21 15:47:50,912 - INFO - Set header 'Comments' in column K of '24 Apr - 30 Apr'.\n",
    "2025-05-21 15:47:54,406 - INFO - Applied complete formatting including borders from column F to column K in '24 Apr - 30 Apr'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab164689",
   "metadata": {},
   "source": [
    "## hide other sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde10bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hide_requests(worksheet_ids: Dict[str, int], sheets_to_keep_visible: List[str], spreadsheet_id: str) -> List[Dict[str, Any]]:\n",
    "    gc, sheets_service = authenticate_gsheets() #build('sheets', 'v4', credentials=creds)\n",
    "    \n",
    "    try:\n",
    "        metadata = execute_with_backoff(\n",
    "            sheets_service.spreadsheets().get,\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            fields='sheets.properties'\n",
    "        )\n",
    "        logging.info(f\"Metadata retrieved: {metadata.keys()}\")\n",
    "        current_sheets = metadata.get('sheets', [])\n",
    "\n",
    "        hide_requests = []\n",
    "        for sheet in current_sheets:\n",
    "            properties = sheet.get('properties', {})\n",
    "            title = properties.get('title')\n",
    "            sheet_id = properties.get('sheetId')\n",
    "            is_hidden = properties.get('hidden', False)\n",
    "\n",
    "            if not title or sheet_id is None:\n",
    "                logging.warning(f\"Skipping invalid sheet: {properties}\")\n",
    "                continue\n",
    "\n",
    "            if title not in sheets_to_keep_visible and not is_hidden:\n",
    "                hide_requests.append({\n",
    "                    \"updateSheetProperties\": {\n",
    "                        \"properties\": {\n",
    "                            \"sheetId\": sheet_id,\n",
    "                            \"hidden\": True\n",
    "                        },\n",
    "                        \"fields\": \"hidden\"\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        total_sheets = len(current_sheets)\n",
    "        if len(hide_requests) >= total_sheets:\n",
    "            logging.warning(f\"Prevented hiding all sheets in spreadsheet {spreadsheet_id}.\")\n",
    "            hide_requests = hide_requests[:-1] if hide_requests else []\n",
    "\n",
    "        return hide_requests\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch or process current sheet visibility: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ce624",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_visible_sheets = [next_sheet_name]\n",
    "currentworksheet_requests = filter_hide_requests(current_worksheet_ids, current_visible_sheets, CURRENT_SPREADSHEET_ID)\n",
    "\n",
    "if currentworksheet_requests :\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "        body={\"requests\": currentworksheet_requests }\n",
    "    )\n",
    "    logging.info(f\"Hid {len(currentworksheet_requests )} sheets in current spreadsheet.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d20672",
   "metadata": {},
   "source": [
    "2025-05-21 15:51:08,167 - INFO - Metadata retrieved: dict_keys(['sheets'])\n",
    "2025-05-21 15:51:08,735 - INFO - Hid 1 sheets in current spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_visible_sheets = [followup_sheet_name]\n",
    "followupworksheet_requests = filter_hide_requests(followup_worksheet_ids, followup_visible_sheets, FOLLOWUP_SPREADSHEET_ID)\n",
    "\n",
    "\n",
    "if followupworksheet_requests :\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body={\"requests\": followupworksheet_requests }\n",
    "    )\n",
    "    logging.info(f\"Hid {len(followupworksheet_requests )} sheets in followup spreadsheet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd8e9f",
   "metadata": {},
   "source": [
    "2025-05-21 15:51:08,765 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-21 15:51:10,584 - INFO - Metadata retrieved: dict_keys(['sheets'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
