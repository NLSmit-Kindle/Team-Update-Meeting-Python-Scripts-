{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f70b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from sqlalchemy import create_engine\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from googleapiclient.errors import HttpError\n",
    "from gspread_formatting import CellFormat, TextFormat, Color, Borders, Border, set_column_width, format_cell_range\n",
    "from gspread_formatting import batch_updater\n",
    "from googleapiclient.http import HttpRequest\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import base64\n",
    "from email.message import EmailMessage\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import matplotlib.pyplot as plt\n",
    "from googleapiclient.http import MediaFileUpload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531e86f",
   "metadata": {},
   "source": [
    "# ENABLE DYNAMIC SCRIPT DIRECTORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()\n",
    "print(script_dir)\n",
    "\n",
    "#background_data_dir = script_dir + '\\\\' + 'Background Data Folder'\n",
    "\"\"\"os.path.join adds the appropriate path separator (\\ or /) based on the operating system\"\"\"\n",
    "background_data_dir = os.path.join(script_dir, 'Background Data Folder')\n",
    "if not os.path.exists(background_data_dir):\n",
    "    os.makedirs(background_data_dir)\n",
    "\n",
    "#report_dir = script_dir + '\\\\' + 'Reports Folder'\n",
    "report_dir = os.path.join(script_dir, 'Reports Folder') \n",
    "if not os.path.exists(report_dir):\n",
    "    os.makedirs(report_dir)\n",
    "\n",
    "data_supplied_dir = os.path.join(script_dir, 'Data Supplied Folder')\n",
    "if not os.path.exists(data_supplied_dir):\n",
    "    os.makedirs(data_supplied_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc24fe",
   "metadata": {},
   "source": [
    "c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66230e98",
   "metadata": {},
   "source": [
    "## SPECIFY DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7948af",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data_dir = os.path.join(background_data_dir, 'master_data.db')\n",
    "if not os.path.exists(master_data_dir):\n",
    "    os.makedirs(master_data_dir)\n",
    "\n",
    "previous_data_dir = os.path.join(background_data_dir, 'previous_errors_n_reponse_times_data.db')\n",
    "if not os.path.exists(previous_data_dir):\n",
    "    os.makedirs(previous_data_dir)\n",
    "\n",
    "\n",
    "gmail_auth_dir = os.path.join(background_data_dir, 'GmailAuth.json')\n",
    "if not os.path.exists(gmail_auth_dir):\n",
    "    os.makedirs(gmail_auth_dir)\n",
    "\n",
    "gmail_token_dir = os.path.join(background_data_dir, 'GmailToken.json')\n",
    "if not os.path.exists(gmail_token_dir):\n",
    "    os.makedirs(gmail_token_dir)\n",
    "\n",
    "gdrive_auth_dir = os.path.join(background_data_dir, 'GoogleAuth.json')\n",
    "if not os.path.exists(gdrive_auth_dir):\n",
    "    os.makedirs(gdrive_auth_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab72f2",
   "metadata": {},
   "source": [
    "# GET MASTER DATA \n",
    "## REPORTING DATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(master_data_dir) \n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT DISTINCT ReportingDate FROM MasterData\")   \n",
    "periods = [row[0] for row in cursor.fetchall()]\n",
    "conn.close()\n",
    "\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=periods,\n",
    "    description='Reporting Date:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global selected_reporting_date\n",
    "        selected_reporting_date = change['new']\n",
    "        with out:\n",
    "            clear_output()\n",
    "            print(f\"Selected Period: {selected_reporting_date}\")\n",
    "\n",
    "dropdown.observe(on_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with out:\n",
    "    display(dropdown)\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7828332",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_reporting_date "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9e3d7",
   "metadata": {},
   "source": [
    "'02 May 2025'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ReportingDate = selected_reporting_date\n",
    "datetime_ReportingDate = datetime.strptime(selected_reporting_date, '%d %b %Y').date()\n",
    "formatted_ReportingDate = datetime_ReportingDate.strftime('%d %m %Y')\n",
    "\n",
    "RunDate = datetime_ReportingDate - timedelta(days=1)\n",
    "formatted_RunDate = f\"{RunDate .day:02d} {RunDate .month:02d} {RunDate .year}\"\n",
    "#formatted_RunDate = RunDate.strftime('%d %m %Y')\n",
    "\n",
    "Designated_Filename = f\"KIT 3 Online Reporting Data {formatted_RunDate}.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f249e98",
   "metadata": {},
   "source": [
    "1. Using f-string with manual formatting: \n",
    "\n",
    "    formatted_RunDate = f\"{RunDate.day:02d} {RunDate.month:02d} {RunDate.year}\"\n",
    "\n",
    "        RunDate.day:02d formats the day with two digits (e.g., 01, 09, 23).\n",
    "\n",
    "        RunDate.month:02d does the same for the month.\n",
    "\n",
    "        RunDate.year just inserts the full year (e.g., 2025).\n",
    "\n",
    "    This approach is explicit and lets you control each component individually.\n",
    "\n",
    "2. Using .strftime()\n",
    "\n",
    "    formatted_RunDate = RunDate.strftime('%d %m %Y')\n",
    "\n",
    "        %d formats the day as a two-digit number.\n",
    "\n",
    "        %m formats the month as a two-digit number.\n",
    "\n",
    "        %Y formats the year as a four-digit number.\n",
    "\n",
    "    This method is cleaner and more concise, especially useful when dealing with more complex date formats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c69a6",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_ReportingDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bacca2",
   "metadata": {},
   "source": [
    "datetime.date(2025, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2762bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_ReportingDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8d3be",
   "metadata": {},
   "source": [
    "'02 05 2025'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f86656",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4bc15",
   "metadata": {},
   "source": [
    "datetime.date(2025, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_RunDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfbe39",
   "metadata": {},
   "source": [
    "'01 05 2025'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Designated_Filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f5377",
   "metadata": {},
   "source": [
    "'KIT 3 Online Reporting Data 01 05 2025.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3146ed",
   "metadata": {},
   "source": [
    "## CORRESPONDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(master_data_dir) \n",
    "cursor = conn.cursor() ## SOLUTION TO ProgramError: cannot operate on a closed database connection\n",
    "cursor.execute(\"SELECT * FROM MasterData WHERE ReportingDate = ?\", (str_ReportingDate,))\n",
    "row = cursor.fetchone()\n",
    "\n",
    "if row:\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    \n",
    "    for key, value in zip(column_names, row):\n",
    "        if key in [\"StartDate\", \"EndDate\", \"ReportingDate\"] and isinstance(value, str):\n",
    "            # Convert string to datetime.date\n",
    "            try:\n",
    "                # For StartDate and EndDate, use ISO format (YYYY-MM-DD)\n",
    "                if key in [\"StartDate\", \"EndDate\"]:\n",
    "                    value = datetime.strptime(value, \"%Y-%m-%d\").date()\n",
    "                else:\n",
    "                    # ReportingDate is in 'dd MMM YYYY' format\n",
    "                    value = datetime.strptime(value, \"%d %b %Y\").date()\n",
    "            except ValueError:\n",
    "                value = None  # Handle parsing error gracefully\n",
    "        \n",
    "    \n",
    "        globals()[key] = value\n",
    "        \n",
    "        print(f\"{key} = {value} ({type(value).__name__})\")\n",
    "else:\n",
    "    print(f\"No row found for ReportingDate = {formatted_ReportingDate}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64ffa3",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------\n",
    "ProgrammingError                                                   Traceback (most recent call last)\n",
    "Cell In[20], line 3\n",
    "      1 conn = sqlite3.connect(master_data_dir) \n",
    "----> 3 cursor.execute(\"SELECT * FROM MasterData WHERE ReportingDate = ?\", (str_ReportingDate,))\n",
    "      4 row = cursor.fetchone()\n",
    "      6 if row:\n",
    "\n",
    "ProgrammingError: Cannot operate on a closed database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f570faa",
   "metadata": {},
   "source": [
    "id = 17 (int)\n",
    "StartDate = 2025-04-24 (date)\n",
    "EndDate = 2025-05-01 (date)\n",
    "WeekNumber = 17 (int)\n",
    "YearWeek = 202517 (int)\n",
    "CurrentSheetName = 24 Apr - 30 Apr (str)\n",
    "CurrentPeriod = 24 Apr 2025 - 01 May 2025 (str)\n",
    "PeriodName = 202517 - 24 Apr 2025 - 01 May 2025 (str)\n",
    "PreviousSheetName = 17 Apr - 23 Apr (str)\n",
    "NextSheetName = 01 May - 07 May (str)\n",
    "ReportingDate = 2025-05-02 (date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "YeakWeek = YearWeek\n",
    "WeekNumber = WeekNumber\n",
    "CurrentSheetName = CurrentSheetName\n",
    "CurrentPeriod = CurrentPeriod\n",
    "PeriodName = PeriodName\n",
    "PreviousSheetName = PreviousSheetName\n",
    "NextSheetName = NextSheetName\n",
    "ReportingDate = ReportingDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f08167",
   "metadata": {},
   "source": [
    "# CHECK CONNECTION TO GMAIL AND GOOGLE DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "###\n",
    "\n",
    "GOOGLE_SERVICE_ACCOUNT_FILE = os.getenv('SERVICE_ACCOUNT_FILE',gdrive_auth_dir)\n",
    "\n",
    "logging.info(f\"Attempting to use service account file: {GOOGLE_SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "if not os.path.exists(GOOGLE_SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {GOOGLE_SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {GOOGLE_SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "logging.info(\"File found, proceeding with authentication...\")\n",
    "\n",
    "###\n",
    "\n",
    "GMAIL_SERVICE_ACCOUNT_FILE = os.getenv('SERVICE_ACCOUNT_FILE', gmail_auth_dir)\n",
    "\n",
    "logging.info(f\"Attempting to use service account file: {GMAIL_SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "if not os.path.exists(GMAIL_SERVICE_ACCOUNT_FILE):\n",
    "    logging.error(f\"Service account file not found: {GMAIL_SERVICE_ACCOUNT_FILE}\")\n",
    "    raise FileNotFoundError(f\"Service account file not found: {GMAIL_SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "logging.info(\"File found, proceeding with authentication...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04837af8",
   "metadata": {},
   "source": [
    "2025-05-23 08:03:04,748 - INFO - Attempting to use service account file: c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Data Supplied Folder\\GoogleAuth.json\n",
    "2025-05-23 08:03:04,748 - INFO - File found, proceeding with authentication...\n",
    "2025-05-23 08:03:04,748 - INFO - Attempting to use service account file: c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Data Supplied Folder\\GmailAuth.json\n",
    "2025-05-23 08:03:04,748 - INFO - File found, proceeding with authentication..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6940a1",
   "metadata": {},
   "source": [
    "# DEFINED FUCNTIONS\n",
    "## google sheets and drive authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_gsheets(return_gspread=False):\n",
    "    \"\"\"\n",
    "    Authenticate using google-auth for gspread or Google Sheets API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        SCOPES = [\n",
    "            \"https://spreadsheets.google.com/feeds\",\n",
    "            \"https://www.googleapis.com/auth/drive\",\n",
    "            \"https://www.googleapis.com/auth/spreadsheets\"\n",
    "        ]\n",
    "        creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        if return_gspread:\n",
    "            gc = gspread.authorize(creds)\n",
    "            logging.debug(\"gspread authentication successful\")\n",
    "            return creds, gc\n",
    "        else:\n",
    "            sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "            logging.debug(\"Sheets API authentication successful\")\n",
    "            return creds, sheets_service\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to authenticate with Google Sheets: {e}\")\n",
    "        raise\n",
    "\n",
    "def authenticate_gdrive():\n",
    "    \"\"\"\n",
    "    Authenticate and return Google Drive API service.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "        creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        gc = gspread.authorize(creds)\n",
    "        drive_service = build('drive', 'v3', credentials=creds)\n",
    "        logging.debug(\"Drive API authentication successful\")\n",
    "        return creds, gc, drive_service\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to authenticate with Google Drive: {e}\")\n",
    "        raise\n",
    "\n",
    "def execute_with_backoff(func,*args,max_retries=5,**kwargs):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result=func(*args,**kwargs)\n",
    "            if isinstance(result,HttpRequest):\n",
    "                result=result.execute()\n",
    "            return result\n",
    "        except(HttpError,gspread.exceptions.APIError) as e:\n",
    "            if 'Quota exceeded' in str(e) or getattr(e,'status',0)==429:\n",
    "                if attempt==max_retries-1:\n",
    "                    logging.error(f\"Max retries reached for {func.__name__}: {e}\")\n",
    "                    raise\n",
    "                sleep_time=(2**attempt)+(random.randint(0,1000)/1000)\n",
    "                logging.warning(f\"Quota exceeded, retrying in {sleep_time:.2f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error in {func.__name__}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f291dd",
   "metadata": {},
   "source": [
    "## FETCH DATA FROM DRIVE\n",
    "### current weeks error count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd89691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_current_errors():\n",
    "    try:\n",
    "        _, gc, _ = authenticate_gdrive()\n",
    "        spreadsheet = gc.open('ERROR REPORT Current Week')\n",
    "        current_sheet = spreadsheet.worksheet(CurrentSheetName)\n",
    "        data = current_sheet.get_all_records()\n",
    "        return pd.DataFrame(data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4f9a4",
   "metadata": {},
   "source": [
    "### client names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c000bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_client_names():\n",
    "    try:\n",
    "        _, gc, _ = authenticate_gdrive()\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Client Names Sheet')\n",
    "        data = sheet.get_all_records()\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df[~df['Other1'].isnull() & (df['Other1'].str.strip() != '')]  #ignore those for which no alternative name is listed\n",
    "        ClientNames_dict = clients_df_to_custom_dict(df)\n",
    "        return ClientNames_dict # return as dictionary \n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Client Names Sheet: {e}\")\n",
    "        raise   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33ea0a",
   "metadata": {},
   "source": [
    "#### standardize client names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "\n",
    "def extract_dates(text):\n",
    "    match = re.search(r'(\\d{1,2} \\w{3}) \\d{4} - (\\d{1,2} \\w{3}) \\d{4}', text)\n",
    "    if match:\n",
    "        return f\"{match.group(1)} - {match.group(2)}\"\n",
    "    return None  # or return text if no match is found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clients_df_to_custom_dict(df):\n",
    "    result = {}\n",
    "    for _, row in df.iterrows():\n",
    "        values = []\n",
    "        # Append non-null, non-empty values from Other1, Other2, and Other3\n",
    "        for col in ['Other1', 'Other2', 'Other3']:\n",
    "            if pd.notnull(row[col]) and row[col] != '':\n",
    "                values.append(row[col])\n",
    "        result[row['Client Name']] = values\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d53396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_client_names(df, df_name):\n",
    "\n",
    "    ClientNames_dict = fetch_client_names()\n",
    "    Nonstrd_ClientNames_list = [item for value in ClientNames_dict.values() for item in (value if isinstance(value, (list, tuple)) else [value])]\n",
    "\n",
    "    Nonstrd_ClientNames_list = [clean_string(i) for i in Nonstrd_ClientNames_list]\n",
    "    def check_match(text):\n",
    "        if pd.isna(text):  # Handle NaN/None values\n",
    "            return False\n",
    "        cleaned_text = clean_string(text)\n",
    "        return cleaned_text in Nonstrd_ClientNames_list\n",
    "    \n",
    "    orginal_colname = df.columns[0]\n",
    "    orginal_cols = list(df.columns)\n",
    "\n",
    "    # Standardize column\n",
    "    df = df.rename(columns={df.columns[0]: 'temp col'})\n",
    "    # Add a new column 'Match' to the DataFrame\n",
    "    df['Match'] = df['temp col'].apply(check_match)\n",
    "\n",
    "    value_map = {}\n",
    "    for key, values in ClientNames_dict.items():\n",
    "        for value in values:\n",
    "            value_map[clean_string(value)] = key\n",
    "    \n",
    "    # Function to apply to each row\n",
    "    def get_standard_value(row):\n",
    "        if row['Match']:  # If Match is True\n",
    "            cleaned_col = clean_string(row['temp col'])\n",
    "            return value_map.get(cleaned_col, row['temp col'])  # Return key if found, else original\n",
    "        return row['temp col']  # Return original if Match is False\n",
    "    \n",
    "    # Add 'standard' column to DataFrame\n",
    "    df['standard'] = df.apply(get_standard_value, axis=1)\n",
    "    df = df.drop(columns=['temp col'])\n",
    "    df = df.rename(columns={'standard': orginal_colname})\n",
    "    df = df.reindex(columns=orginal_cols) # reorder\n",
    "    print('Client Names standardized for {}'.format(df_name))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f4a24",
   "metadata": {},
   "source": [
    "### fetch leave and kindle employees assigned to client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_leave(reporting_date=ReportingDate):\n",
    "    reporting_date = pd.Timestamp(reporting_date)\n",
    "    try:\n",
    "        _, gc, _ = authenticate_gdrive()\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Leave Sheet')\n",
    "        data = pd.DataFrame(sheet.get_all_records())\n",
    "\n",
    "        data['Leave - Start Date'] = pd.to_datetime(data['Leave - Start Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "        data['Leave - End Date'] = pd.to_datetime(data['Leave - End Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "        data['On Leave'] = False\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            start = row['Leave - Start Date']\n",
    "            end = row['Leave - End Date']\n",
    "\n",
    "            if pd.notnull(start) and start < reporting_date <= end:\n",
    "                data.at[index, 'On Leave'] = True\n",
    "            elif pd.isnull(start) and pd.notnull(end) and end > reporting_date:\n",
    "                data.at[index, 'On Leave'] = True\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Leave Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kindle_employees():\n",
    "    try:\n",
    "        _, gc, _ = authenticate_gdrive()\n",
    "        spreadsheet = gc.open('Additional Data')\n",
    "        sheet = spreadsheet.worksheet('Person Responsible Sheet')\n",
    "        data = sheet.get_all_records()\n",
    "        data = pd.DataFrame(data)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Person Responsible Sheet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37a92e",
   "metadata": {},
   "source": [
    "#### determine individuals repsonsible for reporting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_person_responsible(reporting_date=ReportingDate):\n",
    "    reporting_date = pd.Timestamp(reporting_date)\n",
    "    Employee_data = fetch_kindle_employees()\n",
    "    LeaveStatus_data = fetch_leave(reporting_date)\n",
    "\n",
    "    def determine_devs_turn(row):\n",
    "        rotation_freq = row['Rotation Frequency (Weeks)']\n",
    "        if pd.isna(rotation_freq) or rotation_freq == '':\n",
    "        # Return Developer 1's name if valid, otherwise return empty string\n",
    "            dev1 = row['Developer 1']\n",
    "            if dev1 and dev1 != '?' and not pd.isna(dev1):\n",
    "                return dev1.strip()\n",
    "            return ''\n",
    "    \n",
    "        # Convert rotation_freq to an integer\n",
    "        try:\n",
    "            rotation_freq = int(rotation_freq)\n",
    "        except (ValueError, TypeError):\n",
    "            return ''\n",
    "    \n",
    "        if rotation_freq <= 0:\n",
    "            return ''\n",
    "\n",
    "        # Determine which developer columns to consider (Developer 1 to Developer n)\n",
    "        developer_columns = ['Developer 1', 'Developer 2', 'Developer 3', 'Developer 4'][:rotation_freq]\n",
    "    \n",
    "        # Get the list of developers from the relevant columns\n",
    "        developers = []\n",
    "        for col in developer_columns:\n",
    "            dev = row[col]\n",
    "            if dev and dev != '?' and not pd.isna(dev):\n",
    "            # Handle multiple developers in the same column (e.g., \"Joel;Brendan\")\n",
    "                developers.extend([d.strip() for d in dev.split(';') if d.strip()])\n",
    "    \n",
    "        if not developers:\n",
    "            return ''\n",
    "    \n",
    "        if not isinstance(WeekNumber, int):\n",
    "            raise ValueError(f\"current_week_number must be an integer, got {type(WeekNumber)}: {WeekNumber}\")\n",
    "    \n",
    "        developer_index = (WeekNumber % rotation_freq) % len(developers)\n",
    "    \n",
    "        if not isinstance(developer_index, int):\n",
    "            raise ValueError(f\"developer_index is not an integer, got {type(developer_index)}: {developer_index}\")\n",
    "    \n",
    "        return developers[developer_index]\n",
    "\n",
    "    Employee_data['Developer'] = Employee_data.apply(determine_devs_turn, axis=1)\n",
    "\n",
    "    on_leave_list = LeaveStatus_data.loc[LeaveStatus_data['On Leave'] == True, 'Kindle Employee'].dropna().tolist()\n",
    "\n",
    "    def assign_functionalerror_reporter(row):\n",
    "        dev = row['Developer']\n",
    "        dev2 = row['Developer 2']\n",
    "        dev3 = row['Developer 3']\n",
    "        dev4 = row['Developer 4']\n",
    "        emergency = row['In Case of Emergency']\n",
    "    \n",
    "        dev_on_leave = pd.notna(dev) and dev in on_leave_list\n",
    "        dev2_on_leave = pd.notna(dev2) and dev2 in on_leave_list\n",
    "        dev3_on_leave = pd.notna(dev3) and dev3 in on_leave_list\n",
    "        dev4_on_leave = pd.notna(dev4) and dev4 in on_leave_list\n",
    "    \n",
    "        if pd.notna(dev) and not dev_on_leave:\n",
    "            return dev\n",
    "        elif pd.notna(dev2) and not dev2_on_leave:\n",
    "            return dev2\n",
    "        elif pd.notna(dev3) and not dev3_on_leave:\n",
    "            return dev3\n",
    "        elif pd.notna(dev4) and not dev4_on_leave:\n",
    "            return dev4\n",
    "        elif pd.notna(emergency):\n",
    "            return emergency\n",
    "        else:\n",
    "            return None  # or raise an error if no responsible person can be assigned\n",
    "        \n",
    "    Employee_data['Functional Errors Reporter'] = Employee_data.apply(assign_functionalerror_reporter, axis=1)\n",
    "\n",
    "\n",
    "    def assign_clientmovement_reporter(row):\n",
    "        if row['Designated Reporter on Client Movement'] == 'Developer':\n",
    "            return assign_functionalerror_reporter(row)\n",
    "\n",
    "        if row['Designated Reporter on Client Movement'] == 'Business Analyst':\n",
    "            ba1 = row['Business Analyst 1']\n",
    "            ba2 = row['Business Analyst 2']\n",
    "            emergency = row['In Case of Emergency']\n",
    "\n",
    "            ba1_on_leave = pd.notna(ba1) and ba1 in on_leave_list\n",
    "            ba2_on_leave = pd.notna(ba2) and ba2 in on_leave_list\n",
    "\n",
    "            if pd.notna(ba1) and not ba1_on_leave:\n",
    "                return ba1\n",
    "            elif pd.notna(ba2) and not ba2_on_leave:\n",
    "                return ba2\n",
    "            elif pd.notna(emergency):\n",
    "                return emergency\n",
    "            else:\n",
    "                return None  # or raise an error if no responsible person can be assigned\n",
    "\n",
    "        return None  # If \"Designated Reporter on Client Movement\" is neither BA nor Developer\n",
    "\n",
    "    \n",
    "    Employee_data['Client Movement Reporter'] = Employee_data.apply(assign_clientmovement_reporter, axis=1)\n",
    "\n",
    "    PersonsResponsible_data = Employee_data.drop(['Designated Reporter on Client Movement', 'Business Analyst 1', 'Business Analyst 2', 'Key Resource',\n",
    "       'Developer 1', 'Developer 2', 'Developer 3', 'Developer 4',\n",
    "       'Rotation Frequency (Weeks)', 'In Case of Emergency', 'Developer'], axis=1)\n",
    "    \n",
    "    PersonsResponsible_data = standardize_client_names(PersonsResponsible_data, 'PersonsResponsible_data')\n",
    "    \n",
    "    return PersonsResponsible_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3778f",
   "metadata": {},
   "source": [
    "#### apply to client movement and non-/functional error count\n",
    "##### fetch client movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_client_movement():\n",
    "    \"\"\"\n",
    "    remove all rows after the row where 'Tenant Name' has 'Total'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        _, gc, _ = authenticate_gdrive()\n",
    "        spreadsheet = gc.open('Weekly Movement')\n",
    "        sheet = spreadsheet.worksheet('Export')\n",
    "        \n",
    "        data = sheet.get_all_records()\n",
    "        \n",
    "        ClientMovement_data = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "        if 'Tenant Name' in ClientMovement_data.columns:\n",
    "            total_index = ClientMovement_data[ClientMovement_data['Tenant Name'].str.contains('Total', na=False)].index\n",
    "            if not total_index.empty:\n",
    "                ClientMovement_data = ClientMovement_data.loc[:total_index[0]-1]\n",
    "\n",
    "        ClientMovement_data = standardize_client_names(ClientMovement_data, 'ClientMovement_data')\n",
    "\n",
    "        return ClientMovement_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch Client Movement: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85c555",
   "metadata": {},
   "source": [
    "##### determine error count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e376b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_error_count():\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    \n",
    "    Current_Error_data = fetch_current_errors()\n",
    "    Current_Error_data['Client'] = Current_Error_data['Client'].str.title()\n",
    "    Current_Error_data['Client'] = Current_Error_data['Client'].replace('', np.nan)\n",
    "    Current_Error_data['Period'] = Current_Error_data['Period'].replace('', np.nan)\n",
    "    Current_Error_data['Client'] = Current_Error_data['Client'].ffill().bfill()\n",
    "    Current_Error_data['Period'] = Current_Error_data['Period'].ffill().bfill()\n",
    "    Current_Error_data = Current_Error_data.infer_objects(copy=False)\n",
    "    Current_Error_data['Functional Error Y/N'] = Current_Error_data['Functional Error Y/N'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    Current_Error_grouped = Current_Error_data.groupby(['Client', 'Period', 'Functional Error Y/N'], as_index=False)['No of times error occurred'].sum()\n",
    "    Current_Error_grouped.rename(columns={'No of times error occurred': 'Total Errors'}, inplace=True)\n",
    "\n",
    "    Current_Error_grouped.rename(columns={'Functional Error Y/N': 'FunctionalError_Y/N'}, inplace=True)\n",
    "\n",
    "    Current_ErrorCount_data = Current_Error_grouped.pivot_table(\n",
    "        index=['Client', 'Period'],\n",
    "        columns='FunctionalError_Y/N',\n",
    "        values='Total Errors',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "        ).reset_index()\n",
    "    Current_ErrorCount_data = Current_ErrorCount_data.infer_objects(copy=False)\n",
    "    \n",
    "    Current_ErrorCount_data.columns.name = None  # Remove the \"FunctionalError_Y/N\" header\n",
    "    Current_ErrorCount_data = Current_ErrorCount_data.drop(columns=[''])\n",
    "\n",
    "    Current_ErrorCount_data.rename(columns={'Y': 'FunctionalErrors', 'N': 'NonFunctionalErrors'}, inplace=True)\n",
    "\n",
    "    Current_ErrorCount_data = standardize_client_names(Current_ErrorCount_data, 'Current_ErrorCount_data')\n",
    "\n",
    "    return Current_ErrorCount_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30229985",
   "metadata": {},
   "outputs": [],
   "source": [
    "PersonsResponsible_data = determine_person_responsible()\n",
    "BAResponsible_data = PersonsResponsible_data[['Clients','Client Movement Reporter']].rename(columns={'Clients':'Tenant Name'})\n",
    "DEVResponsible_data = PersonsResponsible_data[['Clients','Functional Errors Reporter']].rename(columns={'Clients':'Company'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79354d",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "PermissionError                                                    Traceback (most recent call last)\n",
    "Cell In[21], line 1\n",
    "----> 1 PersonsResponsible_data = determine_person_responsible()\n",
    "      2 BAResponsible_data = PersonsResponsible_data[['Clients','Client Movement Reporter']].rename(columns={'Clients':'Tenant Name'})\n",
    "      3 DEVResponsible_data = PersonsResponsible_data[['Clients','Functional Errors Reporter']].rename(columns={'Clients':'Company'})\n",
    "\n",
    "Cell In[18], line 3, in determine_person_responsible(reporting_date)\n",
    "      1 def determine_person_responsible(reporting_date=ReportingDate):\n",
    "      2     reporting_date = pd.Timestamp(reporting_date)\n",
    "----> 3     Employee_data = fetch_kindle_employees()\n",
    "      4     LeaveStatus_data = fetch_leave(reporting_date)\n",
    "      6     def determine_devs_turn(row):\n",
    "\n",
    "Cell In[17], line 3, in fetch_kindle_employees()\n",
    "      1 def fetch_kindle_employees():\n",
    "      2     try:\n",
    "----> 3         _, gc, _ = authenticate_gdrive()\n",
    "      4         spreadsheet = gc.open('Additional Data')\n",
    "      5         sheet = spreadsheet.worksheet('Person Responsible Sheet')\n",
    "\n",
    "Cell In[10], line 30, in authenticate_gdrive()\n",
    "     28 try:\n",
    "     29     SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "---> 30     creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "     31     gc = gspread.authorize(creds)\n",
    "     32     drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\google\\oauth2\\service_account.py:260, in Credentials.from_service_account_file(cls, filename, **kwargs)\n",
    "    248 @classmethod\n",
    "    249 def from_service_account_file(cls, filename, **kwargs):\n",
    "    250     \"\"\"Creates a Credentials instance from a service account json file.\n",
    "    251 \n",
    "    252     Args:\n",
    "   (...)    258             credentials.\n",
    "    259     \"\"\"\n",
    "--> 260     info, signer = _service_account_info.from_filename(\n",
    "    261         filename, require=[\"client_email\", \"token_uri\"]\n",
    "    262     )\n",
    "    263     return cls._from_signer_and_info(signer, info, **kwargs)\n",
    "\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\google\\auth\\_service_account_info.py:78, in from_filename(filename, require, use_rsa_signer)\n",
    "     64 def from_filename(filename, require=None, use_rsa_signer=True):\n",
    "     65     \"\"\"Reads a Google service account JSON file and returns its parsed info.\n",
    "     66 \n",
    "     67     Args:\n",
    "   (...)     76             info and a signer instance.\n",
    "     77     \"\"\"\n",
    "---> 78     with io.open(filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "     79         data = json.load(json_file)\n",
    "     80         return data, from_dict(data, require=require, use_rsa_signer=use_rsa_signer)\n",
    "\n",
    "PermissionError: [Errno 13] Permission denied: 'c:\\\\Users\\\\nicola\\\\Desktop\\\\VisualCode Workspace\\\\Team Meeting Update\\\\Data Supplied Folder\\\\GoogleAuth.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b50e2",
   "metadata": {},
   "source": [
    "###### TROUBLESHOOTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_SERVICE_ACCOUNT_FILE = r\"c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Data Supplied Folder\\GoogleAuth.json\"\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "try:\n",
    "    print(f\"File exists: {os.path.exists(GOOGLE_SERVICE_ACCOUNT_FILE)}\")\n",
    "    print(f\"File readable: {os.access(GOOGLE_SERVICE_ACCOUNT_FILE, os.R_OK)}\")\n",
    "    creds = Credentials.from_service_account_file(GOOGLE_SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    logging.debug(\"Authentication successful\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Authentication failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e35c9",
   "metadata": {},
   "source": [
    "2025-05-23 08:48:01,014 - ERROR - Authentication failed: [Errno 13] Permission denied: 'c:\\\\Users\\\\nicola\\\\Desktop\\\\VisualCode Workspace\\\\Team Meeting Update\\\\Data Supplied Folder\\\\GoogleAuth.json'\n",
    "File exists: True\n",
    "File readable: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af362893",
   "metadata": {},
   "source": [
    "NOT USING <strong>from google.oauth2.credentials import Credentials</strong>\n",
    "\n",
    "    google.oauth2.service_account.Credentials \n",
    "\n",
    "is designed for service account authentication, which uses a JSON key file (like GoogleAuth.json) to authenticate programmatically without user interaction. This is what you need for server-to-server interactions with Google APIs, such as accessing Google Drive or Sheets.\n",
    "\n",
    "\n",
    "    google.oauth2.credentials.Credentials \n",
    "\n",
    "is for OAuth 2.0 user authentication, typically used for scenarios where a user authorizes access via a browser-based flow. It requires a client ID, client secret, and access/refresh tokens, not a service account JSON file. It does not have a from_service_account_file method, so using it would break your code and is not suitable for your automated script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9e5a2",
   "metadata": {},
   "source": [
    "Reproduce the Error with Debug Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea6623",
   "metadata": {},
   "source": [
    "###### Test File Accessibility Explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51024674",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "GOOGLE_SERVICE_ACCOUNT_FILE = r\"c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Data Supplied Folder\\GoogleAuth.json\"\n",
    "\n",
    "try:\n",
    "    logging.debug(f\"Attempting to read file: {GOOGLE_SERVICE_ACCOUNT_FILE}\")\n",
    "    with open(GOOGLE_SERVICE_ACCOUNT_FILE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        logging.debug(f\"File contents: {data}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to read file: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b108e",
   "metadata": {},
   "source": [
    "2025-05-23 08:53:52,418 - ERROR - Failed to read file: [Errno 13] Permission denied: 'c:\\\\Users\\\\nicola\\\\Desktop\\\\VisualCode Workspace\\\\Team Meeting Update\\\\Data Supplied Folder\\\\GoogleAuth.json'\n",
    "Traceback (most recent call last):\n",
    "  File \"C:\\Users\\nicola\\AppData\\Local\\Temp\\ipykernel_9920\\3547980247.py\", line 6, in <module>\n",
    "    with open(GOOGLE_SERVICE_ACCOUNT_FILE, 'r', encoding='utf-8') as f:\n",
    "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 325, in _modified_open\n",
    "    return io_open(file, *args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "PermissionError: [Errno 13] Permission denied: 'c:\\\\Users\\\\nicola\\\\Desktop\\\\VisualCode Workspace\\\\Team Meeting Update\\\\Data Supplied Folder\\\\GoogleAuth.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0507cf4",
   "metadata": {},
   "source": [
    "###### Re-Test with Explict Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861967d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "GOOGLE_SERVICE_ACCOUNT_FILE = r\"c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\Team Meeting Update\\Data Supplied Folder\\GoogleAuth.json\"\n",
    "\n",
    "try:\n",
    "    logging.debug(f\"File path: {GOOGLE_SERVICE_ACCOUNT_FILE}\")\n",
    "    logging.debug(f\"File exists: {os.path.exists(GOOGLE_SERVICE_ACCOUNT_FILE)}\")\n",
    "    logging.debug(f\"File readable: {os.access(GOOGLE_SERVICE_ACCOUNT_FILE, os.R_OK)}\")\n",
    "    logging.debug(\"Attempting to read file\")\n",
    "    with open(GOOGLE_SERVICE_ACCOUNT_FILE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        logging.debug(f\"File contents loaded successfully\")\n",
    "except PermissionError as e:\n",
    "    logging.error(f\"Permission error: {e}\", exc_info=True)\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to read file: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f7ca2",
   "metadata": {},
   "source": [
    "2025-05-23 08:55:55,263 - ERROR - Permission error: [Errno 13] Permission denied: 'c:\\\\Users\\\\nicola\\\\Desktop\\\\VisualCode Workspace\\\\Team Meeting Update\\\\Data Supplied Folder\\\\GoogleAuth.json'\n",
    "Traceback (most recent call last):\n",
    "  File \"C:\\Users\\nicola\\AppData\\Local\\Temp\\ipykernel_9920\\2185016895.py\", line 9, in <module>\n",
    "    with open(GOOGLE_SERVICE_ACCOUNT_FILE, 'r', encoding='utf-8') as f:\n",
    "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 325, in _modified_open\n",
    "    return io_open(file, *args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "PermissionError: [Errno 13] Permission denied: 'c:\\\\Users\\\\nicola\\\\Desktop\\\\VisualCode Workspace\\\\Team Meeting Update\\\\Data Supplied Folder\\\\GoogleAuth.json'"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "640dbaff",
   "metadata": {},
   "source": [
    "NOTICE wrong directory specified when attempting to locate file such that folders created with specified names \n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "gmail_auth_dir = os.path.join(data_supplied_dir, 'GmailAuth.json')\n",
    "\n",
    "gmail_token_dir = os.path.join(data_supplied_dir, 'GmailToken.json')\n",
    "\n",
    "gdrive_auth_dir = os.path.join(data_supplied_dir, 'GoogleAuth.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4c65b",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb93f22",
   "metadata": {},
   "source": [
    "2025-05-23 09:15:30,366 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:15:36,903 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:15:40,055 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Client Names standardized for PersonsResponsible_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d1832",
   "metadata": {},
   "source": [
    "##### client movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd985bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClientMovement_data = fetch_client_movement()\n",
    "Onlines_data = pd.merge(ClientMovement_data, BAResponsible_data, on='Tenant Name', how='left')\n",
    "Onlines_data.loc[Onlines_data['Difference'] <= 0, 'Client Movement Reporter'] = np.nan\n",
    "\n",
    "try:\n",
    "    _, gc, _ = authenticate_gdrive()\n",
    "    spreadsheet = gc.open('Persons Responsible')\n",
    "\n",
    "    worksheet_title = 'Onlines'\n",
    "    try:\n",
    "        sheet = spreadsheet.worksheet(worksheet_title)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        sheet = spreadsheet.add_worksheet(title=worksheet_title, rows=100, cols=26)\n",
    "        logging.info(f\"Created new worksheet: {worksheet_title}\")\n",
    "\n",
    "    data = [Onlines_data.columns.tolist()] + Onlines_data.astype(object).where(pd.notnull(Onlines_data), None).values.tolist()\n",
    "\n",
    "    # Clear sheet data    \n",
    "    execute_with_backoff(sheet.clear)\n",
    "    execute_with_backoff(sheet.update, data, 'A1', value_input_option='RAW')\n",
    "\n",
    "    # Format the data as a table\n",
    "    set_column_width(sheet, 'A', 250)  \n",
    "    set_column_width(sheet, 'F', 100) \n",
    "    set_column_width(sheet, 'G', 150) \n",
    "\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(data[0])\n",
    "    range_notation = f'A1:{chr(65 + num_cols - 1)}{num_rows}'\n",
    "\n",
    "    # Set header row height using batch update\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateDimensionProperties\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet.id,\n",
    "                            \"dimension\": \"ROWS\",\n",
    "                            \"startIndex\": 0,  # Row 1 (0-based index)\n",
    "                            \"endIndex\": 1     # Only the first row\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"pixelSize\": 30  # Set height to 30 pixels\n",
    "                        },\n",
    "                        \"fields\": \"pixelSize\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Bold headers and format\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            f'A1:{chr(65 + num_cols - 1)}1',\n",
    "            CellFormat(\n",
    "                textFormat=TextFormat(bold=True),\n",
    "                horizontalAlignment='CENTER',\n",
    "                verticalAlignment='MIDDLE',\n",
    "                backgroundColor=Color(0.9, 0.9, 0.9)  # Light gray background for headers\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add borders to create a table effect\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            range_notation,\n",
    "            CellFormat(\n",
    "                borders=Borders(\n",
    "                    top=Border('SOLID'),\n",
    "                    bottom=Border('SOLID'),\n",
    "                    left=Border('SOLID'),\n",
    "                    right=Border('SOLID')\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add filter buttons to the table\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"setBasicFilter\": {\n",
    "                        \"filter\": {\n",
    "                            \"range\": {\n",
    "                                \"sheetId\": sheet.id,\n",
    "                                \"startRowIndex\": 0,  # Start at row 1\n",
    "                                \"endRowIndex\": num_rows,  # End at last row\n",
    "                                \"startColumnIndex\": 0,  # Start at column A\n",
    "                                \"endColumnIndex\": num_cols  # End at last column\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Data successfully added to {worksheet_title} worksheet of Persons Responsible workbook\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to write DataFrame to sheet, apply formatting, or add filter: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e4dcd",
   "metadata": {},
   "source": [
    "2025-05-23 09:16:54,748 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:16:59,315 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:17:02,593 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Client Names standardized for ClientMovement_data\n",
    "2025-05-23 09:17:13,560 - INFO - Data successfully added to Onlines worksheet of Persons Responsible workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d83782",
   "metadata": {},
   "source": [
    "##### functional errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64657788",
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_ErrorCount_data = determine_error_count()\n",
    "Current_ErrorCount_data = Current_ErrorCount_data.rename(columns={'Client': 'Company'}) \n",
    "Current_ErrorCount_data = Current_ErrorCount_data.drop(['Period'], axis=1)\n",
    "FunctionalErrors_data = pd.merge(Current_ErrorCount_data, DEVResponsible_data, on='Company', how='left')\n",
    "FunctionalErrors_data.loc[FunctionalErrors_data['FunctionalErrors'] > 0, 'Functional Errors Reporter'] = np.nan\n",
    "\n",
    "try:\n",
    "    _, gc, _ = authenticate_gdrive()\n",
    "    spreadsheet = gc.open('Persons Responsible')\n",
    "\n",
    "    # Check if worksheet exists, create if it doesn't\n",
    "    worksheet_title = 'Functional Errors'\n",
    "    try:\n",
    "        sheet = spreadsheet.worksheet(worksheet_title)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        sheet = spreadsheet.add_worksheet(title=worksheet_title, rows=100, cols=26)\n",
    "        logging.info(f\"Created new worksheet: {worksheet_title}\")\n",
    "\n",
    "    data = [FunctionalErrors_data.columns.tolist()] + FunctionalErrors_data.astype(object).where(pd.notnull(FunctionalErrors_data), None).values.tolist()\n",
    "        \n",
    "    # Clear the sheet and write the data\n",
    "    execute_with_backoff(sheet.clear)\n",
    "    execute_with_backoff(sheet.update, data, 'A1', value_input_option='RAW')\n",
    "\n",
    "    # Format the data as a table\n",
    "    set_column_width(sheet, 'A', 300)\n",
    "    set_column_width(sheet, 'B', 50)  \n",
    "    set_column_width(sheet, 'C', 50) \n",
    "    set_column_width(sheet, 'D', 150)\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(data[0])\n",
    "    range_notation = f'A1:{chr(65 + num_cols - 1)}{num_rows}'\n",
    "\n",
    "    # Set header row height using batch update\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateDimensionProperties\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet.id,\n",
    "                            \"dimension\": \"ROWS\",\n",
    "                            \"startIndex\": 0,  # Row 1 (0-based index)\n",
    "                            \"endIndex\": 1     # Only the first row\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"pixelSize\": 30  # Set height to 30 pixels\n",
    "                        },\n",
    "                        \"fields\": \"pixelSize\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Bold headers and format\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            f'A1:{chr(65 + num_cols - 1)}1',\n",
    "            CellFormat(\n",
    "                textFormat=TextFormat(bold=True),\n",
    "                horizontalAlignment='CENTER',\n",
    "                verticalAlignment='MIDDLE',\n",
    "                backgroundColor=Color(0.9, 0.9, 0.9)  # Light gray background for headers\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add borders to create a table effect\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            range_notation,\n",
    "            CellFormat(\n",
    "                borders=Borders(\n",
    "                    top=Border('SOLID'),\n",
    "                    bottom=Border('SOLID'),\n",
    "                    left=Border('SOLID'),\n",
    "                    right=Border('SOLID')\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    # Add filter buttons to the table\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"setBasicFilter\": {\n",
    "                        \"filter\": {\n",
    "                            \"range\": {\n",
    "                                \"sheetId\": sheet.id,\n",
    "                                \"startRowIndex\": 0,  # Start at row 1\n",
    "                                \"endRowIndex\": num_rows,  # End at last row\n",
    "                                \"startColumnIndex\": 0,  # Start at column A\n",
    "                                \"endColumnIndex\": num_cols  # End at last column\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Data sucessfully added to {worksheet_title} worksheet of Persons Responsible workbook\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to write DataFrame to sheet, apply formatting, or add filter: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a48ede",
   "metadata": {},
   "source": [
    "2025-05-23 09:20:36,833 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:20:41,363 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Client Names standardized for Current_ErrorCount_data\n",
    "2025-05-23 09:20:52,934 - INFO - Data sucessfully added to Functional Errors worksheet of Persons Responsible workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chart(df, show=False):\n",
    "\n",
    "    os.makedirs('Reports Folder', exist_ok=True)\n",
    "    chart_name = f'Bar Graph for {PeriodName}.png'\n",
    "    \n",
    "    try:\n",
    "        x = np.arange(len(df.iloc[:,0]))   ## SOLUTION df['Client'] altered to df.iloc[:,0]\n",
    "        width = 0.35\n",
    "        fig, ax = plt.subplots(figsize=(20, 15))  \n",
    "        ax.bar(x - width/2, df['NonFunctionalErrors'], width, label='Non-Functional Errors')\n",
    "        ax.bar(x + width/2, df['FunctionalErrors'], width, label='Functional Errors')\n",
    "\n",
    "        ax.set_xlabel('Client')\n",
    "        ax.set_ylabel('Error Count')\n",
    "        ax.set_title(f'Functional vs Non-Functional Errors for {PeriodName}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(df.iloc[:,0], rotation=45, ha='right')  ## SOLTION df['Client'] altered to df.iloc[:,0]\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        chart_path = os.path.join('Reports Folder', chart_name)\n",
    "        plt.savefig(chart_path, dpi=900, bbox_inches='tight')  \n",
    "        plt.close(fig)\n",
    "        logging.debug(f\"Chart saved to {chart_path} with high resolution (DPI=900)\")\n",
    "\n",
    "        if not os.path.exists(chart_path):\n",
    "            raise FileNotFoundError(f\"Chart file {chart_name} was not saved correctly\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate or save chart: {e}\")\n",
    "        raise\n",
    "    \n",
    "    if show: \n",
    "        return plt.show()\n",
    "    \n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9292375",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_chart(Current_ErrorCount_data, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875062c",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805, in Index.get_loc(self, key)\n",
    "   3804 try:\n",
    "-> 3805     return self._engine.get_loc(casted_key)\n",
    "   3806 except KeyError as err:\n",
    "\n",
    "File index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "File index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
    "\n",
    "File pandas\\\\_libs\\\\hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "File pandas\\\\_libs\\\\hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
    "\n",
    "KeyError: 'Client'\n",
    "\n",
    "The above exception was the direct cause of the following exception:\n",
    "\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[26], line 1\n",
    "----> 1 generate_chart(Current_ErrorCount_data, show=False)\n",
    "\n",
    "Cell In[25], line 7, in generate_chart(df, show)\n",
    "      4 chart_name = f'Bar Graph for {PeriodName}.png'\n",
    "      6 try:\n",
    "----> 7     x = np.arange(len(df['Client']))\n",
    "      8     width = 0.35\n",
    "      9     fig, ax = plt.subplots(figsize=(20, 15))  \n",
    "\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102, in DataFrame.__getitem__(self, key)\n",
    "   4100 if self.columns.nlevels > 1:\n",
    "   4101     return self._getitem_multilevel(key)\n",
    "-> 4102 indexer = self.columns.get_loc(key)\n",
    "   4103 if is_integer(indexer):\n",
    "   4104     indexer = [indexer]\n",
    "\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812, in Index.get_loc(self, key)\n",
    "   3807     if isinstance(casted_key, slice) or (\n",
    "   3808         isinstance(casted_key, abc.Iterable)\n",
    "   3809         and any(isinstance(x, slice) for x in casted_key)\n",
    "   3810     ):\n",
    "   3811         raise InvalidIndexError(key)\n",
    "-> 3812     raise KeyError(key) from err\n",
    "   3813 except TypeError:\n",
    "   3814     # If we have a listlike key, _check_indexing_error will raise\n",
    "   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
    "   3816     #  the TypeError.\n",
    "   3817     self._check_indexing_error(key)\n",
    "\n",
    "KeyError: 'Client'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd0179",
   "metadata": {},
   "source": [
    "###### TROUBLESHOOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f38df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_ErrorCount_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARING OUTPUT \n",
    "Current_ErrorCount_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0774b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Current_ErrorCount_data['Company']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54705c",
   "metadata": {},
   "source": [
    "## FETCH DATA FROM LOCAL DEVICE \n",
    "### previous error and response times data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e70af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_previous(YearWeek=YearWeek): \n",
    "    engine = create_engine(f'sqlite:///{previous_data_dir}')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM PreviousErrorData\n",
    "        WHERE CAST(SUBSTR(Period, 1, 6) AS INTEGER) < {YearWeek}\n",
    "            \"\"\"\n",
    "    Previous_Errors_n_ResponseTimes_data = pd.read_sql(query, engine)\n",
    "    Previous_Errors_n_ResponseTimes_data = standardize_client_names(Previous_Errors_n_ResponseTimes_data,'Previous_Errors_n_ResponseTimes_data')\n",
    "    \n",
    "    return Previous_Errors_n_ResponseTimes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0cfa50",
   "metadata": {},
   "source": [
    "### current response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e39093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_response_times(data_folder = data_supplied_dir): \n",
    "\n",
    "    files = os.listdir(data_folder)\n",
    "    matched_file = None\n",
    "    for file in files:\n",
    "        if clean_string(file) == clean_string(Designated_Filename):\n",
    "            matched_file = file\n",
    "            break\n",
    "\n",
    "    if not matched_file:\n",
    "        available_files = \", \".join(files)\n",
    "        raise FileNotFoundError(f\"File '{Designated_Filename}' not found. Available files: {available_files}\")\n",
    "\n",
    "    file_path = os.path.join(data_folder, matched_file)\n",
    "\n",
    "    try:\n",
    "        xl = pd.ExcelFile(file_path)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"File found but couldn't be accessed: {str(e)}\")\n",
    "\n",
    "    sheet_name = \"This week response times\"\n",
    "    if sheet_name not in xl.sheet_names:\n",
    "        sheet_name = xl.sheet_names[1]  # Default to last sheet\n",
    "\n",
    "    try:\n",
    "        data = xl.parse(sheet_name, header=0)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"No valid data found in the '{sheet_name}' sheet: {str(e)}\")\n",
    "\n",
    "    Current_ResponseTimes_data  = standardize_client_names(data, 'Current_ResponseTimes_data')\n",
    "    Current_ResponseTimes_data = Current_ResponseTimes_data.drop(['FunctionalError', 'NonFunctionalError'], axis=1)\n",
    "    \n",
    "    #Current_ResponseTimes_data.to_excel(\"ResponseTimesQuery.xlsx\", index=False)\n",
    "\n",
    "    return Current_ResponseTimes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5f097",
   "metadata": {},
   "source": [
    "## CONCATENATE PREVIOUS WITH CURRENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_errors_n_responsetimes():\n",
    "    \"\"\"\n",
    "    remove total row of response times\n",
    "    \"\"\"\n",
    "    Current_ResponseTimes_data = fetch_response_times()\n",
    "    Current_ResponseTimes_data = Current_ResponseTimes_data[~Current_ResponseTimes_data['Period'].isna()] \n",
    "    \n",
    "    Current_ErrorCount_data = determine_error_count()\n",
    "\n",
    "    Current_ErrorCount_data = Current_ErrorCount_data.rename(columns={'Client': 'Company'}) \n",
    "    Current_ErrorCount_data = Current_ErrorCount_data.drop(['Period'], axis=1)\n",
    "\n",
    "    Current_Errors_n_ResponseTimes_data = pd.merge(Current_ResponseTimes_data, Current_ErrorCount_data, on=['Company'], how='left')\n",
    "    \n",
    "    columns_to_convert = ['FunctionalErrors', 'NonFunctionalErrors']\n",
    "    for col in columns_to_convert:\n",
    "        Current_Errors_n_ResponseTimes_data[col] = pd.to_numeric(Current_Errors_n_ResponseTimes_data[col], errors='coerce').astype('Int64')\n",
    "    \n",
    "\n",
    "    if Current_Errors_n_ResponseTimes_data.duplicated(subset=['Period', 'Company']).any():\n",
    "        print(\"Warning: Removing internal duplicates based on Period and Company.\")\n",
    "        Current_Errors_n_ResponseTimes_data = Current_Errors_n_ResponseTimes_data.drop_duplicates(subset=['Period', 'Company'])\n",
    "\n",
    "    engine = create_engine(f'sqlite:///{previous_data_dir}')\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Get the last entry's Period from PreviousErrorData (most recent by Period)\n",
    "            last_entry = pd.read_sql(\"SELECT Period FROM PreviousErrorData ORDER BY Period DESC LIMIT 1\", conn)\n",
    "        \n",
    "            if last_entry.empty:\n",
    "                print(\"Warning: PreviousErrorData table is empty. Cannot verify Period.\")\n",
    "            else:\n",
    "                last_period = last_entry['Period'].iloc[0]\n",
    "                last_period_prefix = int(last_period[:6])\n",
    "\n",
    "                # Get the Period prefix from ErrorReport_data (assuming all rows have the same prefix)\n",
    "                new_period_prefix = int(Current_ErrorCount_data['Period'].iloc[0][:6])  # First 6 characters\n",
    "\n",
    "                # Verify that the new year is exactly one more than the last year\n",
    "                if new_period_prefix == last_period_prefix + 1:\n",
    "                    print(f\"Verification successful: New Period ({new_period_prefix}) is one week after last Period ({last_period_prefix}).\")\n",
    "                else:\n",
    "                    print(f\"Verification failed: New Period ({new_period_prefix}) is not one more than last Period ({last_period_prefix}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {e}\")\n",
    "\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            existing_records = pd.read_sql(\"SELECT Period, Company FROM PreviousErrorData\", conn)\n",
    "\n",
    "            # Ensure consistent data types for comparison\n",
    "            for df in [Current_Errors_n_ResponseTimes_data, existing_records]:\n",
    "                df['Period'] = df['Period'].astype(str)\n",
    "                df['Company'] = df['Company'].astype(str)\n",
    "\n",
    "            new_records = Current_Errors_n_ResponseTimes_data.merge(\n",
    "                existing_records,\n",
    "                on=['Period', 'Company'],\n",
    "                how='left',\n",
    "                indicator=True\n",
    "            ).query('_merge == \"left_only\"').drop(columns='_merge')\n",
    "\n",
    "            if new_records.empty:\n",
    "                print(\"No new records to append; all records already exist in PreviousErrorData.\")\n",
    "            else:\n",
    "                new_records.to_sql('PreviousErrorData', con=engine, if_exists='append', index=False)\n",
    "                print(f\"Successfully appended {len(new_records)} new records to PreviousErrorData table.\")\n",
    "                print(\"Appended records:\")\n",
    "                print(new_records[['Period', 'Company']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking/appending data: {e}\")\n",
    "\n",
    "    Previous_Errors_n_ResponseTimes_data = fetch_previous()\n",
    "\n",
    "    Updated_Errors_n_ResponseTimes_data = pd.concat([Previous_Errors_n_ResponseTimes_data, Current_Errors_n_ResponseTimes_data], ignore_index=True)\n",
    "\n",
    "    return Updated_Errors_n_ResponseTimes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c1557",
   "metadata": {},
   "source": [
    "# WRITE DATA TO REPORT SHEETS\n",
    "## complete table of errors and response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Updated_Errors_n_ResponseTimes_data = concat_errors_n_responsetimes()\n",
    "Updated_Errors_n_ResponseTimes_data = Updated_Errors_n_ResponseTimes_data.where(pd.notnull(Updated_Errors_n_ResponseTimes_data), None)\n",
    "\n",
    "ClientNames_dict = fetch_client_names()\n",
    "\n",
    "Nonstrd_ClientNames_list = [item for value in ClientNames_dict.values() for item in (value if isinstance(value, (list, tuple)) else [value])]\n",
    "if not Updated_Errors_n_ResponseTimes_data['Company'].isin(Nonstrd_ClientNames_list).any():\n",
    "    print(\"No non-standard company names found.\")\n",
    "else:\n",
    "    print(\"There is at least one non-standard company name present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1268ba9",
   "metadata": {},
   "source": [
    "Client Names standardized for Current_ErrorCount_data\n",
    "Error during verification: (sqlite3.OperationalError) unable to open database file\n",
    "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
    "Error checking/appending data: (sqlite3.OperationalError) unable to open database file\n",
    "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
    "2025-05-23 09:30:47,477 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Client Names standardized for Previous_Errors_n_ResponseTimes_data\n",
    "No non-standard company names found.\n",
    "\n",
    "<strong> file path not correctly specified </strong> once corrected\n",
    "\n",
    "Client Names standardized for Current_ErrorCount_data\n",
    "Error during verification: 'Period'\n",
    "No new records to append; all records already exist in PreviousErrorData.\n",
    "2025-05-23 09:34:43,678 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Client Names standardized for Previous_Errors_n_ResponseTimes_data\n",
    "No non-standard company names found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def write_Errors_n_ResponseTimes_to_sheet(append=False): ## NOT NECCESSARY TO DEFINE AS FUNCTION since used once\n",
    "try:\n",
    "    _, gc, _ = authenticate_gdrive()\n",
    "    spreadsheet = gc.open('KIT 3 Online Reporting Data')\n",
    "\n",
    "    worksheet_title = 'Data Errors and Reponse Times'\n",
    "    try:\n",
    "        sheet = spreadsheet.worksheet(worksheet_title)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        sheet = spreadsheet.add_worksheet(title=worksheet_title, rows=100, cols=26)  \n",
    "        logging.info(f\"Created new worksheet: {worksheet_title}\")\n",
    "\n",
    "    data_filled = Updated_Errors_n_ResponseTimes_data.copy()\n",
    "\n",
    "    for col in data_filled.columns:\n",
    "        if data_filled[col].dtype == 'Float64':\n",
    "            data_filled[col] = data_filled[col].fillna(0)\n",
    "        else:\n",
    "            data_filled[col] = data_filled[col].fillna('')\n",
    "\n",
    "        # Convert DataFrame to list for writing to sheet\n",
    "    data = [data_filled.columns.tolist()] + data_filled.astype(object).where(pd.notnull(data_filled), None).values.tolist()\n",
    "\n",
    "        # Clear the sheet and write the data\n",
    "    execute_with_backoff(sheet.clear)\n",
    "    execute_with_backoff(sheet.update, data, 'A1', value_input_option='RAW')\n",
    "\n",
    "        # Format the data as a table\n",
    "    num_rows = len(data)\n",
    "    num_cols = len(data[0])\n",
    "    range_notation = f'A1:{chr(65 + num_cols - 1)}{num_rows}'\n",
    "\n",
    "    # Set header row height using batch update\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateDimensionProperties\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet.id,\n",
    "                            \"dimension\": \"ROWS\",\n",
    "                            \"startIndex\": 0,  # Row 1 (0-based index)\n",
    "                            \"endIndex\": 1     # Only the first row\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"pixelSize\": 30  # Set height to 30 pixels\n",
    "                        },\n",
    "                        \"fields\": \"pixelSize\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Bold headers and format\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            f'A1:{chr(65 + num_cols - 1)}1',\n",
    "            CellFormat(\n",
    "                textFormat=TextFormat(bold=True),\n",
    "                horizontalAlignment='CENTER',\n",
    "                verticalAlignment='MIDDLE',\n",
    "                backgroundColor=Color(0.9, 0.9, 0.9)  # Light gray background for headers\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add borders to create a table effect\n",
    "    format_cell_range(\n",
    "            sheet,\n",
    "            range_notation,\n",
    "            CellFormat(\n",
    "                borders=Borders(\n",
    "                    top=Border('SOLID'),\n",
    "                    bottom=Border('SOLID'),\n",
    "                    left=Border('SOLID'),\n",
    "                    right=Border('SOLID')\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Auto-resize columns for better readability\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"autoResizeDimensions\": {\n",
    "                        \"dimensions\": {\n",
    "                            \"sheetId\": sheet.id,\n",
    "                            \"dimension\": \"COLUMNS\",\n",
    "                            \"startIndex\": 0,\n",
    "                            \"endIndex\": num_cols\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Add filter buttons to the table\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"setBasicFilter\": {\n",
    "                        \"filter\": {\n",
    "                            \"range\": {\n",
    "                                \"sheetId\": sheet.id,\n",
    "                                \"startRowIndex\": 0,  # Start at row 1\n",
    "                                \"endRowIndex\": num_rows,  # End at last row\n",
    "                                \"startColumnIndex\": 0,  # Start at column A\n",
    "                                \"endColumnIndex\": num_cols  # End at last column\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Overwrote {len(Updated_Errors_n_ResponseTimes_data)} rows in {sheet}, formatted as a table, and added filter buttons\")\n",
    "        #return\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to write DataFrame to sheet, apply formatting, or add filter: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2511423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_Errors_n_ResponseTimes_to_sheet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205fdfb",
   "metadata": {},
   "source": [
    "2025-05-23 09:35:35,699 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:35:48,443 - INFO - Overwrote 5056 rows in <Worksheet 'Data Errors and Reponse Times' id:1289794990>, formatted as a table, and added filter buttons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42c78c",
   "metadata": {},
   "source": [
    "### create accompanying pivot table and graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb33225",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _, gc, _ = authenticate_gdrive()\n",
    "    spreadsheet = gc.open('KIT 3 Online Reporting Data')\n",
    "\n",
    "    data_sheet = spreadsheet.worksheet('Data Errors and Reponse Times')\n",
    "\n",
    "    pivot_sheet_title = 'Errors Report'\n",
    "    try:\n",
    "        pivot_sheet = spreadsheet.worksheet(pivot_sheet_title)\n",
    "        execute_with_backoff(pivot_sheet.clear)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        pivot_sheet = spreadsheet.add_worksheet(title=pivot_sheet_title, rows=100, cols=26)\n",
    "        logging.info(f\"Created new worksheet: {pivot_sheet_title}\")\n",
    "\n",
    "    _ , sheets_service = authenticate_gsheets()\n",
    "    #sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "    sheet_id = spreadsheet.id\n",
    "    pivot_sheet_id = pivot_sheet.id\n",
    "\n",
    "    headers = data_sheet.row_values(1)  \n",
    "    company_idx = headers.index('Company') if 'Company' in headers else 0\n",
    "    functional_error_idx = headers.index('Functional Error Count') if 'Functional Error Count' in headers else 1\n",
    "    non_functional_error_idx = headers.index('Non Functional Error Count') if 'Non Functional Error Count' in headers else 2\n",
    "    period_idx = headers.index('Period') if 'Period' in headers else 3\n",
    "\n",
    "    # Define the pivot table with filter for PeriodName\n",
    "    pivot_table_request = {\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateCells\": {\n",
    "                        \"rows\": [\n",
    "                            {\n",
    "                                \"values\": [\n",
    "                                    {\n",
    "                                        \"pivotTable\": {\n",
    "                                            \"source\": {\n",
    "                                                \"sheetId\": data_sheet.id,\n",
    "                                                \"startRowIndex\": 0,\n",
    "                                                \"startColumnIndex\": 0,\n",
    "                                                \"endRowIndex\": len(data_sheet.get_all_values()),\n",
    "                                                \"endColumnIndex\": len(data_sheet.get_all_values()[0])\n",
    "                                            },\n",
    "                                            \"rows\": [\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": company_idx,\n",
    "                                                    \"showTotals\": True,\n",
    "                                                    \"sortOrder\": \"ASCENDING\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"values\": [\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": functional_error_idx,\n",
    "                                                    \"summarizeFunction\": \"SUM\",\n",
    "                                                    \"name\": \"Functional Error Count\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": non_functional_error_idx,\n",
    "                                                    \"summarizeFunction\": \"SUM\",\n",
    "                                                    \"name\": \"Non Functional Error Count\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"criteria\": {\n",
    "                                                str(period_idx): {\n",
    "                                                    \"visibleValues\": [PeriodName]\n",
    "                                                }\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        ],\n",
    "                        \"start\": {\n",
    "                            \"sheetId\": pivot_sheet_id,\n",
    "                            \"rowIndex\": 0,\n",
    "                            \"columnIndex\": 0\n",
    "                        },\n",
    "                        \"fields\": \"*\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # Execute pivot table creation\n",
    "    sheets_service.spreadsheets().batchUpdate(spreadsheetId=sheet_id, body=pivot_table_request).execute()\n",
    "    logging.info(\"Created pivot table in 'Errors Report' sheet\")\n",
    "\n",
    "    chart_request = {\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"addChart\": {\n",
    "                        \"chart\": {\n",
    "                            \"spec\": {\n",
    "                                \"basicChart\": {\n",
    "                                    \"chartType\": \"COLUMN\",\n",
    "                                    \"legendPosition\": \"BOTTOM_LEGEND\",\n",
    "                                    \"axis\": [\n",
    "                                        {\n",
    "                                            \"position\": \"BOTTOM_AXIS\",\n",
    "                                            \"title\": \"Company\"\n",
    "                                        },\n",
    "                                        {\n",
    "                                            \"position\": \"LEFT_AXIS\",\n",
    "                                            \"title\": \"Error Count\"\n",
    "                                        }\n",
    "                                    ],\n",
    "                                    \"domains\": [\n",
    "                                        {\n",
    "                                            \"domain\": {\n",
    "                                                \"sourceRange\": {\n",
    "                                                    \"sources\": [\n",
    "                                                        {\n",
    "                                                            \"sheetId\": pivot_sheet_id,\n",
    "                                                            \"startRowIndex\": 1,\n",
    "                                                            \"startColumnIndex\": 0,\n",
    "                                                            \"endColumnIndex\": 1\n",
    "                                                        }\n",
    "                                                    ]\n",
    "                                                }\n",
    "                                            }\n",
    "                                        }\n",
    "                                    ],\n",
    "                                    \"series\": [\n",
    "                                        {\n",
    "                                            \"series\": {\n",
    "                                                \"sourceRange\": {\n",
    "                                                    \"sources\": [\n",
    "                                                        {\n",
    "                                                            \"sheetId\": pivot_sheet_id,\n",
    "                                                            \"startRowIndex\": 1,\n",
    "                                                            \"startColumnIndex\": 1,\n",
    "                                                            \"endColumnIndex\": 2\n",
    "                                                        }\n",
    "                                                    ]\n",
    "                                                }\n",
    "                                            }\n",
    "                                        },\n",
    "                                        {\n",
    "                                            \"series\": {\n",
    "                                                \"sourceRange\": {\n",
    "                                                    \"sources\": [\n",
    "                                                        {\n",
    "                                                            \"sheetId\": pivot_sheet_id,\n",
    "                                                            \"startRowIndex\": 1,\n",
    "                                                            \"startColumnIndex\": 2,\n",
    "                                                            \"endColumnIndex\": 3\n",
    "                                                        }\n",
    "                                                    ]\n",
    "                                                }\n",
    "                                            }\n",
    "                                        }\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"title\": \"Error Counts by Company\"\n",
    "                            },\n",
    "                            \"position\": {\n",
    "                                \"overlayPosition\": {\n",
    "                                    \"anchorCell\": {\n",
    "                                        \"sheetId\": pivot_sheet_id,\n",
    "                                        \"rowIndex\": 0,\n",
    "                                        \"columnIndex\": 5\n",
    "                                    },\n",
    "                                    \"offsetXPixels\": 0,\n",
    "                                    \"offsetYPixels\": 0,\n",
    "                                    \"widthPixels\": 600,\n",
    "                                    \"heightPixels\": 400\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    # Execute chart creation\n",
    "    sheets_service.spreadsheets().batchUpdate(spreadsheetId=sheet_id, body=chart_request).execute()\n",
    "    logging.info(\"Added grouped bar chart to 'Errors Report' sheet\")\n",
    "\n",
    "    # Auto-resize columns in the pivot sheet\n",
    "    spreadsheet.batch_update({\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"autoResizeDimensions\": {\n",
    "                        \"dimensions\": {\n",
    "                            \"sheetId\": pivot_sheet_id,\n",
    "                            \"dimension\": \"COLUMNS\",\n",
    "                            \"startIndex\": 0,\n",
    "                            \"endIndex\": 10\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create pivot table, slicers, or chart: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39343fb8",
   "metadata": {},
   "source": [
    "2025-05-23 09:36:30,312 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:36:35,614 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:36:41,910 - INFO - Created pivot table in 'Errors Report' sheet\n",
    "2025-05-23 09:36:43,059 - INFO - Added grouped bar chart to 'Errors Report' sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cf97a",
   "metadata": {},
   "source": [
    "NOTICE <strong> failure </strong>\n",
    "    although graph correctly created in google sheet - distorts once downloaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e21eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _, gc, _ = authenticate_gdrive()\n",
    "    spreadsheet = gc.open('KIT 3 Online Reporting Data')\n",
    "\n",
    "    data_sheet = spreadsheet.worksheet('Data Errors and Reponse Times')\n",
    "\n",
    "    pivot_sheet_title = 'Errors Report'\n",
    "\n",
    "    try:\n",
    "        pivot_sheet = spreadsheet.worksheet(pivot_sheet_title)\n",
    "        execute_with_backoff(pivot_sheet.clear)\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        pivot_sheet = spreadsheet.add_worksheet(title=pivot_sheet_title, rows=100, cols=26)\n",
    "        logging.info(f\"Created new worksheet: {pivot_sheet_title}\")\n",
    "\n",
    "    _ , sheets_service = authenticate_gsheets()\n",
    "    \n",
    "    sheet_id = spreadsheet.id\n",
    "    pivot_sheet_id = pivot_sheet.id\n",
    "\n",
    "    headers = data_sheet.row_values(1)  \n",
    "    company_idx = headers.index('Company') if 'Company' in headers else 0\n",
    "    functional_error_idx = headers.index('Functional Error Count') if 'Functional Error Count' in headers else 1\n",
    "    non_functional_error_idx = headers.index('Non Functional Error Count') if 'Non Functional Error Count' in headers else 2\n",
    "    period_idx = headers.index('Period') if 'Period' in headers else 3\n",
    "\n",
    "    # Define the pivot table with filter for PeriodName\n",
    "    pivot_table_request = {\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateCells\": {\n",
    "                        \"rows\": [\n",
    "                            {\n",
    "                                \"values\": [\n",
    "                                    {\n",
    "                                        \"pivotTable\": {\n",
    "                                            \"source\": {\n",
    "                                                \"sheetId\": data_sheet.id,\n",
    "                                                \"startRowIndex\": 0,\n",
    "                                                \"startColumnIndex\": 0,\n",
    "                                                \"endRowIndex\": len(data_sheet.get_all_values()),\n",
    "                                                \"endColumnIndex\": len(data_sheet.get_all_values()[0])\n",
    "                                            },\n",
    "                                            \"rows\": [\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": company_idx,\n",
    "                                                    \"showTotals\": True,\n",
    "                                                    \"sortOrder\": \"ASCENDING\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"values\": [\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": functional_error_idx,\n",
    "                                                    \"summarizeFunction\": \"SUM\",\n",
    "                                                    \"name\": \"Functional Error Count\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"sourceColumnOffset\": non_functional_error_idx,\n",
    "                                                    \"summarizeFunction\": \"SUM\",\n",
    "                                                    \"name\": \"Non Functional Error Count\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"criteria\": {\n",
    "                                                str(period_idx): {\n",
    "                                                    \"visibleValues\": [PeriodName]\n",
    "                                                }\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        ],\n",
    "                        \"start\": {\n",
    "                            \"sheetId\": pivot_sheet_id,\n",
    "                            \"rowIndex\": 0,\n",
    "                            \"columnIndex\": 0\n",
    "                        },\n",
    "                        \"fields\": \"*\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # Execute pivot table creation\n",
    "    sheets_service.spreadsheets().batchUpdate(spreadsheetId=sheet_id, body=pivot_table_request).execute()\n",
    "    logging.info(\"Created pivot table in 'Errors Report' sheet\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to create pivot table & slicers: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c17c63",
   "metadata": {},
   "source": [
    "2025-05-23 09:36:59,133 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:37:04,541 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:37:10,152 - INFO - Created pivot table in 'Errors Report' sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a664ec",
   "metadata": {},
   "source": [
    "#### upload graph as image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaed8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_insert_image(cell_range='G1:L28', sheet_name='Errors Report', spreadsheet_id = '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI', PeriodName=PeriodName):\n",
    "\n",
    "    image_path = f'Reports Folder/Bar Graph for {PeriodName}.png'\n",
    "\n",
    "    _, gc, drive_service = authenticate_gdrive()\n",
    "    spreadsheet = gc.open('KIT 3 Online Reporting Data')\n",
    "\n",
    "    _, sheets_service = authenticate_gsheets()\n",
    "\n",
    "    try:\n",
    "        # Step 1: Upload image to Google Drive\n",
    "        if not os.path.exists(image_path):\n",
    "            logging.error(f\"Image file not found: {image_path}\")\n",
    "            raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "        file_metadata = {\n",
    "            'name': os.path.basename(image_path),\n",
    "            'mimeType': 'image/png',\n",
    "            'parents': []  # Optionally specify a folder ID here if needed\n",
    "        }\n",
    "        media = MediaFileUpload(image_path, mimetype='image/png')\n",
    "        file = execute_with_backoff(drive_service.files().create,\n",
    "                                    body=file_metadata,\n",
    "                                    media_body=media,\n",
    "                                    fields='id,webContentLink')\n",
    "        logging.info(f\"Uploaded image to Drive, file ID: {file['id']}\")\n",
    "\n",
    "        # Step 2: Set permissions to make the image publicly accessible\n",
    "        permission = {'type': 'anyone', 'role': 'reader'}\n",
    "        execute_with_backoff(drive_service.permissions().create,\n",
    "                             fileId=file['id'],\n",
    "                             body=permission)\n",
    "        logging.info(f\"Set image permissions to public\")\n",
    "\n",
    "        # Step 3: Get sheet metadata to find sheetId\n",
    "        spreadsheet = execute_with_backoff(sheets_service.spreadsheets().get,\n",
    "                                          spreadsheetId=spreadsheet_id)\n",
    "        sheet_id = None\n",
    "        for sheet in spreadsheet['sheets']:\n",
    "            if sheet['properties']['title'] == sheet_name:\n",
    "                sheet_id = sheet['properties']['sheetId']\n",
    "                break\n",
    "        if not sheet_id:\n",
    "            logging.error(f\"Sheet '{sheet_name}' not found in spreadsheet\")\n",
    "            raise ValueError(f\"Sheet '{sheet_name}' not found\")\n",
    "\n",
    "        \n",
    "        start_cell = cell_range.split(':')[0]  \n",
    "        match = re.match(r'([A-Z]+)(\\d+)', start_cell)\n",
    "        if not match:\n",
    "            logging.error(f\"Invalid cell reference: {start_cell}\")\n",
    "            raise ValueError(f\"Invalid cell reference: {start_cell}\")\n",
    "        \n",
    "        column_letters, row_number = match.groups()\n",
    "        column_index = sum((ord(c.upper()) - ord('A') + 1) * (26 ** i) for i, c in enumerate(reversed(column_letters))) - 1\n",
    "        row_index = int(row_number) - 1\n",
    "\n",
    "        # Step 5: Update the top-left cell with =IMAGE() formula\n",
    "        image_url = file['webContentLink']\n",
    "        formula = f'=IMAGE(\"{image_url}\")'\n",
    "        request = {\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateCells\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet_id,\n",
    "                            \"startRowIndex\": row_index,\n",
    "                            \"endRowIndex\": row_index + 1,\n",
    "                            \"startColumnIndex\": column_index,\n",
    "                            \"endColumnIndex\": column_index + 1\n",
    "                        },\n",
    "                        \"rows\": [\n",
    "                            {\n",
    "                                \"values\": [\n",
    "                                    {\n",
    "                                        \"userEnteredValue\": {\n",
    "                                            \"formulaValue\": formula\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        ],\n",
    "                        \"fields\": \"userEnteredValue\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        execute_with_backoff(sheets_service.spreadsheets().batchUpdate,\n",
    "                            spreadsheetId=spreadsheet_id,\n",
    "                            body=request)\n",
    "        logging.info(f\"Set =IMAGE() formula in '{sheet_name}' at cell {start_cell} for range {cell_range}\")\n",
    "\n",
    "        # Step 6: Adjust column width and row heights for better image display\n",
    "        dimension_request = {\n",
    "            \"requests\": [\n",
    "                {\n",
    "                    \"updateDimensionProperties\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet_id,\n",
    "                            \"dimension\": \"COLUMNS\",\n",
    "                            \"startIndex\": column_index,\n",
    "                            \"endIndex\": column_index + 1\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"pixelSize\": 200  # Adjust width as needed\n",
    "                        },\n",
    "                        \"fields\": \"pixelSize\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"updateDimensionProperties\": {\n",
    "                        \"range\": {\n",
    "                            \"sheetId\": sheet_id,\n",
    "                            \"dimension\": \"ROWS\",\n",
    "                            \"startIndex\": 0,\n",
    "                            \"endIndex\": 28\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"pixelSize\": 30  # Adjust height as needed\n",
    "                        },\n",
    "                        \"fields\": \"pixelSize\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        execute_with_backoff(sheets_service.spreadsheets().batchUpdate,\n",
    "                            spreadsheetId=spreadsheet_id,\n",
    "                            body=dimension_request)\n",
    "        logging.info(f\"Adjusted column width and row heights for range {cell_range}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload and insert image: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e873bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_insert_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a63b5",
   "metadata": {},
   "source": [
    "2025-05-23 09:38:12,320 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:38:14,618 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:38:20,722 - INFO - Uploaded image to Drive, file ID: 1dCoh-_uKqbGIsFALG2dSS78rGvRmmOeH\n",
    "2025-05-23 09:38:21,525 - INFO - Set image permissions to public\n",
    "2025-05-23 09:38:25,077 - INFO - Set =IMAGE() formula in 'Errors Report' at cell G1 for range G1:L28\n",
    "2025-05-23 09:38:26,132 - INFO - Adjusted column width and row heights for range G1:L28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56666df7",
   "metadata": {},
   "source": [
    "### copy data from Current Errors and Follow Up sheets and add to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREADSHEET_ID = os.getenv('SPREADSHEET_ID', '1m49V3mKZiuTatraSSGqo1xHLIhcQRmJZO90S4Y72vcc') # CURRENT\n",
    "DEST_SPREADSHEET_ID = os.getenv('DEST_SPREADSHEET_ID', '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI') # KIT 3 ONLINE REPORT\n",
    "\n",
    "_, gc = authenticate_gsheets(return_gspread=True)\n",
    "_, sheets_service = authenticate_gsheets()\n",
    "\n",
    "spreadsheet = execute_with_backoff(gc.open_by_key, SPREADSHEET_ID)\n",
    "dest_spreadsheet = execute_with_backoff(gc.open_by_key, DEST_SPREADSHEET_ID)\n",
    "\n",
    "source_worksheets = execute_with_backoff(spreadsheet.worksheets)\n",
    "dest_worksheets = execute_with_backoff(dest_spreadsheet.worksheets)\n",
    "\n",
    "source_worksheet_ids = {sheet.title: sheet.id for sheet in source_worksheets}\n",
    "dest_worksheet_ids = {sheet.title: sheet.id for sheet in dest_worksheets}\n",
    "\n",
    "source_sheet_name = CurrentSheetName  \n",
    "dest_sheet_name = 'Current Week Error'\n",
    "\n",
    "if source_sheet_name in source_worksheet_ids and source_sheet_name not in dest_worksheet_ids:\n",
    "    \n",
    "    copy_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().sheets().copyTo,\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        sheetId=source_worksheet_ids[source_sheet_name],\n",
    "        body={'destinationSpreadsheetId': DEST_SPREADSHEET_ID}\n",
    "    )\n",
    "    copied_sheet_id = copy_response['sheetId']\n",
    "    logging.info(f\"Copied sheet '{source_sheet_name}' to destination.\")\n",
    "\n",
    "    # Delete existing destination sheet if it exists\n",
    "    if dest_sheet_name in dest_worksheet_ids:\n",
    "        delete_request = {\n",
    "            'requests': [{\n",
    "                'deleteSheet': {\n",
    "                    'sheetId': dest_worksheet_ids[dest_sheet_name]\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "            body=delete_request\n",
    "        )\n",
    "        logging.info(f\"Deleted existing sheet '{dest_sheet_name}' from destination.\")\n",
    "\n",
    "    # Rename the copied sheet and ensure it's not hidden\n",
    "    rename_and_unhide_request = {\n",
    "    'requests': [{\n",
    "        'updateSheetProperties': {\n",
    "            'properties': {\n",
    "                'sheetId': copied_sheet_id,\n",
    "                'title': dest_sheet_name,\n",
    "                'hidden': False\n",
    "            },\n",
    "            'fields': 'title,hidden'\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    execute_with_backoff(\n",
    "    sheets_service.spreadsheets().batchUpdate,\n",
    "    spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "    body=rename_and_unhide_request\n",
    "    )\n",
    "    logging.info(f\"Renamed copied sheet to '{dest_sheet_name}' and ensured it is visible.\")\n",
    "\n",
    "\n",
    "    # Adjust column widths\n",
    "    resize_columns_request = {\n",
    "    'requests': [\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 0,  # Column A\n",
    "                    'endIndex': 1\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 170  # Slightly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 1,  # Column B\n",
    "                    'endIndex': 2\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 120  # Slightly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 2,  # Column C\n",
    "                    'endIndex': 3\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 400  # Significantly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "        body=resize_columns_request\n",
    "    )\n",
    "    logging.info(\"Adjusted column widths for columns A, B (slightly), and C (significantly).\")\n",
    "\n",
    "else:\n",
    "    logging.info(f\"No sheet copied. Either '{source_sheet_name}' doesn't exist in source or '{dest_sheet_name}' already exists in destination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742495a8",
   "metadata": {},
   "source": [
    "2025-05-23 09:38:37,246 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:38:44,316 - INFO - Copied sheet '24 Apr - 30 Apr' to destination.\n",
    "2025-05-23 09:38:44,902 - INFO - Deleted existing sheet 'Current Week Error' from destination.\n",
    "2025-05-23 09:38:46,428 - INFO - Renamed copied sheet to 'Current Week Error' and ensured it is visible.\n",
    "2025-05-23 09:38:47,441 - INFO - Adjusted column widths for columns A, B (slightly), and C (significantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPREADSHEET_ID = os.getenv('SPREADSHEET_ID', '13IpNi1rFg8luMX8t_OlhCbAQcEhkZkZUmwFWP3C_8N8') # FOLLOW UP\n",
    "DEST_SPREADSHEET_ID = os.getenv('DEST_SPREADSHEET_ID', '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI') # KIT3 REPORT\n",
    "\n",
    "_, gc = authenticate_gsheets(return_gspread=True)\n",
    "_, sheets_service =  authenticate_gsheets()\n",
    "\n",
    "spreadsheet = execute_with_backoff(gc.open_by_key, SPREADSHEET_ID)\n",
    "dest_spreadsheet = execute_with_backoff(gc.open_by_key, DEST_SPREADSHEET_ID)\n",
    "\n",
    "source_worksheets = execute_with_backoff(spreadsheet.worksheets)\n",
    "dest_worksheets = execute_with_backoff(dest_spreadsheet.worksheets)\n",
    "\n",
    "source_worksheet_ids = {sheet.title: sheet.id for sheet in source_worksheets}\n",
    "dest_worksheet_ids = {sheet.title: sheet.id for sheet in dest_worksheets}\n",
    "\n",
    "source_sheet_name = PreviousSheetName\n",
    "dest_sheet_name = 'Feedback on Previous Week Error'\n",
    "\n",
    "if source_sheet_name in source_worksheet_ids and source_sheet_name not in dest_worksheet_ids:\n",
    "   \n",
    "    copy_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().sheets().copyTo,\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        sheetId=source_worksheet_ids[source_sheet_name],\n",
    "        body={'destinationSpreadsheetId': DEST_SPREADSHEET_ID}\n",
    "    )\n",
    "\n",
    "    copied_sheet_id = copy_response['sheetId']\n",
    "    logging.info(f\"Copied sheet '{source_sheet_name}' to destination.\")\n",
    "\n",
    "    # Delete existing destination sheet if it exists\n",
    "    if dest_sheet_name in dest_worksheet_ids:\n",
    "        delete_request = {\n",
    "            'requests': [{\n",
    "                'deleteSheet': {\n",
    "                    'sheetId': dest_worksheet_ids[dest_sheet_name]\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "            body=delete_request\n",
    "        )\n",
    "        logging.info(f\"Deleted existing sheet '{dest_sheet_name}' from destination.\")\n",
    "\n",
    "    # Rename the copied sheet and ensure it's not hidden\n",
    "    rename_and_unhide_request = {\n",
    "    'requests': [{\n",
    "        'updateSheetProperties': {\n",
    "            'properties': {\n",
    "                'sheetId': copied_sheet_id,\n",
    "                'title': dest_sheet_name,\n",
    "                'hidden': False\n",
    "            },\n",
    "            'fields': 'title,hidden'\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "        body=rename_and_unhide_request\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Renamed copied sheet to '{dest_sheet_name}' and ensured it is visible.\")\n",
    "\n",
    "    # Adjust column widths\n",
    "    resize_columns_request = {\n",
    "    'requests': [\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 0,  # Column A\n",
    "                    'endIndex': 1\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 170  # Slightly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 1,  # Column B\n",
    "                    'endIndex': 2\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 120  # Slightly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': 2,  # Column C\n",
    "                    'endIndex': 3\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': 400  # Significantly increased\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=DEST_SPREADSHEET_ID,\n",
    "        body=resize_columns_request\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Adjusted column widths for columns A, B (slightly), and C (significantly).\")\n",
    "\n",
    "else:\n",
    "    logging.info(f\"No sheet copied. Either '{source_sheet_name}' doesn't exist in source or '{dest_sheet_name}' already exists in destination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc38f1",
   "metadata": {},
   "source": [
    "2025-05-23 09:39:10,901 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:39:20,519 - INFO - Copied sheet '17 Apr - 23 Apr' to destination.\n",
    "2025-05-23 09:39:21,617 - INFO - Deleted existing sheet 'Feedback on Previous Week Error' from destination.\n",
    "2025-05-23 09:39:22,720 - INFO - Renamed copied sheet to 'Feedback on Previous Week Error' and ensured it is visible.\n",
    "2025-05-23 09:39:23,815 - INFO - Adjusted column widths for columns A, B (slightly), and C (significantly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd104921",
   "metadata": {},
   "source": [
    "# DISTRIBUTE VIA GMAIL \n",
    "using specific name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ea661",
   "metadata": {},
   "outputs": [],
   "source": [
    "GMAIL_SCOPES = ['https://www.googleapis.com/auth/drive.readonly','https://www.googleapis.com/auth/gmail.send']\n",
    "\n",
    "flow = InstalledAppFlow.from_client_secrets_file(gmail_auth_dir, GMAIL_SCOPES)\n",
    "creds = flow.run_local_server(port=0)\n",
    "\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "# Spreadsheet ID\n",
    "SPREADSHEET_ID = '1oc1zL7BdlRFFT68cZV46ctvHi5q-ZosaPk79HvireWI'\n",
    "TO_EMAIL = 'nicola@kindle.co.za'\n",
    "\n",
    "# Export Google Sheet as Excel\n",
    "request = drive_service.files().export_media(fileId=SPREADSHEET_ID, mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')\n",
    "\n",
    "file_path = f'Reports Folder/{Designated_Filename}'\n",
    "fh = io.FileIO(file_path, 'wb')\n",
    "downloader = MediaIoBaseDownload(fh, request)\n",
    "done = False\n",
    "while not done:\n",
    "    status, done = downloader.next_chunk()\n",
    "\n",
    "# Read file for attachment\n",
    "with open(file_path, 'rb') as f:\n",
    "    file_data = f.read()\n",
    "\n",
    "# Build Email\n",
    "message = EmailMessage()\n",
    "message['To'] = TO_EMAIL\n",
    "message['From'] = 'me'  \n",
    "message['Subject'] = 'KIT3 Weekly Report {}'.format(formatted_ReportingDate)\n",
    "message.set_content(\"Hi All,\\n\\n Please find attached.\\n\\n Regards\")\n",
    "\n",
    "message.add_attachment(\n",
    "    file_data,\n",
    "    maintype='application',\n",
    "    subtype='vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "    filename= Designated_Filename\n",
    ")\n",
    "\n",
    "# Encode and send\n",
    "encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n",
    "gmail_service.users().messages().send(\n",
    "    userId='me',\n",
    "    body={'raw': encoded_message}\n",
    ").execute()\n",
    "\n",
    "print(\"Email sent successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e6959",
   "metadata": {},
   "source": [
    "2025-05-23 09:40:00,953 - INFO - \"GET /?state=O7pSbeazFxcYJGvIsbyUllS7ZekljW&code=4/0AUJR-x5apv0JMqNYcz-bT960tAbAJm9LDyuTQek-YVkN-bp43VpfgO7sglNITRgbM68DFA&scope=https://www.googleapis.com/auth/drive.readonly%20https://www.googleapis.com/auth/gmail.send HTTP/1.1\" 200 65\n",
    "2025-05-23 09:40:01,743 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:40:01,759 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "Email sent successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69aba4",
   "metadata": {},
   "source": [
    "<strong> FAILURE </strong> need to re-authenticate each time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e9161",
   "metadata": {},
   "source": [
    "# CREATE NEW SHEETS IN CURRENT and FOLLOWUP WORKBOOKS \n",
    "add new column to Follow Up workbook\n",
    "and hide other sheets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_WIDTHS = {\n",
    "    0: 200,  # Column A\n",
    "    1: 150,  # Column B\n",
    "    2: 300,  # Column C\n",
    "    3: 150,  # Column D\n",
    "    4: 150, \n",
    "    5: 150, \n",
    "    6: 150, \n",
    "    7: 150, \n",
    "    8: 150,\n",
    "    9: 150\n",
    "}\n",
    "\n",
    "CURRENT_SPREADSHEET_ID = os.getenv('CURRENT_SPREADSHEET_ID', '1m49V3mKZiuTatraSSGqo1xHLIhcQRmJZO90S4Y72vcc')  # CURRENT\n",
    "FOLLOWUP_SPREADSHEET_ID = os.getenv('FOLLOWUP_SPREADSHEET_ID', '13IpNi1rFg8luMX8t_OlhCbAQcEhkZkZUmwFWP3C_8N8')  # FOLLOW UP\n",
    "TEMPLATE_NAME = 'TEMPLATE - Do not edit'\n",
    "\n",
    "_, gc = authenticate_gsheets(return_gspread=True)\n",
    "_, sheets_service = authenticate_gsheets()\n",
    "\n",
    "# Open spreadsheets\n",
    "current_spreadsheet = execute_with_backoff(gc.open_by_key, CURRENT_SPREADSHEET_ID)\n",
    "template_sheet = current_spreadsheet.get_worksheet_by_id(552319826)\n",
    "followup_spreadsheet = execute_with_backoff(gc.open_by_key, FOLLOWUP_SPREADSHEET_ID)\n",
    "\n",
    "# Cache worksheets to reduce API calls\n",
    "current_worksheets = execute_with_backoff(current_spreadsheet.worksheets)\n",
    "followup_worksheets = execute_with_backoff(followup_spreadsheet.worksheets)\n",
    "\n",
    "current_worksheet_ids = {sheet.title: sheet.id for sheet in current_worksheets}\n",
    "followup_worksheet_ids = {sheet.title: sheet.id for sheet in followup_worksheets}\n",
    "\n",
    "# Define sheet names \n",
    "next_sheet_name = NextSheetName  \n",
    "followup_sheet_name = CurrentSheetName  \n",
    "\n",
    "# Step 1: Unhide the template sheet\n",
    "if TEMPLATE_NAME in current_worksheet_ids:\n",
    "    template_sheet_id = current_worksheet_ids[TEMPLATE_NAME]\n",
    "    unhide_request = {\n",
    "        'requests': [{\n",
    "            'updateSheetProperties': {\n",
    "                'properties': {\n",
    "                    'sheetId': template_sheet_id,\n",
    "                    'hidden': False\n",
    "                },\n",
    "                'fields': 'hidden'\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    try:\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "            body=unhide_request\n",
    "        )\n",
    "        logging.info(f\"Unhid template sheet '{TEMPLATE_NAME}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error unhiding template sheet '{TEMPLATE_NAME}': {e}\")\n",
    "        raise\n",
    "else:\n",
    "    logging.error(f\"Template sheet '{TEMPLATE_NAME}' not found.\")\n",
    "    raise Exception(f\"Template sheet '{TEMPLATE_NAME}' not found.\")\n",
    "\n",
    "if next_sheet_name in current_worksheet_ids:\n",
    "    logging.info(f\"Worksheet '{next_sheet_name}' already exists. Deleting it.\")\n",
    "    sheet_id = current_worksheet_ids[next_sheet_name]\n",
    "    delete_request = {\n",
    "        'requests': [{\n",
    "            'deleteSheet': {\n",
    "                'sheetId': sheet_id\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "        body=delete_request\n",
    "    )\n",
    "    \n",
    "    try:   \n",
    "        del current_worksheet_ids[next_sheet_name]\n",
    "        time.sleep(1)\n",
    "        updated_current_worksheets = execute_with_backoff(current_spreadsheet.worksheets)\n",
    "        updated_current_worksheet_ids = {sheet.title: sheet.id for sheet in updated_current_worksheets}\n",
    "        \n",
    "        if next_sheet_name in updated_current_worksheet_ids:\n",
    "            raise Exception(f\"Failed to delete worksheet '{next_sheet_name}'.\")\n",
    "        logging.info(f\"Deleted worksheet '{next_sheet_name}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error deleting worksheet '{next_sheet_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    new_sheet = execute_with_backoff(\n",
    "        current_spreadsheet.duplicate_sheet,\n",
    "        source_sheet_id=template_sheet.id,\n",
    "        new_sheet_name=next_sheet_name)\n",
    "    \n",
    "    current_worksheet_ids[next_sheet_name] = new_sheet.id\n",
    "    \n",
    "    logging.info(f\"Created worksheet '{next_sheet_name}'.\")\n",
    "\n",
    "    # Set column widths for the new sheet\n",
    "    width_requests = [\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': new_sheet.id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': col_index,\n",
    "                    'endIndex': col_index + 1\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': width\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        } for col_index, width in COLUMN_WIDTHS.items()\n",
    "    ]\n",
    "    if width_requests:\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "            body={'requests': width_requests}\n",
    "        )\n",
    "        logging.info(f\"Set column widths for '{next_sheet_name}'.\")\n",
    "\n",
    "    # Update 'Period' column\n",
    "    values = execute_with_backoff(new_sheet.get_all_values)\n",
    "    header = values[0]\n",
    "    try:\n",
    "        period_col_index = header.index(\"Period\")\n",
    "        period_col_letter = chr(ord('A') + period_col_index)\n",
    "    except ValueError:\n",
    "        raise Exception(\"Column 'Period' not found.\")\n",
    "    updates = [\n",
    "        {'range': f\"{period_col_letter}{row_idx}\", 'values': [[next_sheet_name]]}\n",
    "        for row_idx, row in enumerate(values[1:], start=2)\n",
    "        if len(row) > period_col_index and row[period_col_index].strip()\n",
    "    ]\n",
    "    if updates:\n",
    "        execute_with_backoff(new_sheet.batch_update, updates)\n",
    "        logging.info(f\"Updated {len(updates)} 'Period' cells.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating worksheet '{next_sheet_name}': {e}\")\n",
    "    raise\n",
    "\n",
    "if followup_sheet_name in current_worksheet_ids and followup_sheet_name in followup_worksheet_ids:\n",
    "    logging.info(f\"Worksheet '{followup_sheet_name}' already exists. Deleting it.\")\n",
    "    sheet_id = followup_worksheet_ids[followup_sheet_name]\n",
    "    delete_request = {\n",
    "        'requests': [{\n",
    "            'deleteSheet': {\n",
    "                'sheetId': sheet_id\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=delete_request\n",
    "    )\n",
    "    \n",
    "    try:   \n",
    "        del followup_worksheet_ids[followup_sheet_name]\n",
    "        time.sleep(1)\n",
    "        updated_followup_worksheets = execute_with_backoff(followup_spreadsheet.worksheets)\n",
    "        updated_followup_worksheet_ids = {sheet.title: sheet.id for sheet in updated_followup_worksheets}\n",
    "        \n",
    "        if followup_sheet_name in updated_followup_worksheet_ids:\n",
    "            raise Exception(f\"Failed to delete worksheet '{followup_sheet_name}'.\")\n",
    "        logging.info(f\"Deleted worksheet '{followup_sheet_name}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error deleting worksheet '{followup_sheet_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    copy_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().sheets().copyTo,\n",
    "        spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "        sheetId=current_worksheet_ids[followup_sheet_name],\n",
    "        body={'destinationSpreadsheetId': FOLLOWUP_SPREADSHEET_ID})\n",
    "        \n",
    "    copied_sheet_id = copy_response['sheetId']\n",
    "    rename_request = {\n",
    "        'requests': [{\n",
    "            'updateSheetProperties': {\n",
    "                'properties': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'title': followup_sheet_name\n",
    "                },\n",
    "                'fields': 'title'\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    # Add column width updates to the rename request\n",
    "    width_requests = [\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': col_index,\n",
    "                    'endIndex': col_index + 1\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': width\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        } for col_index, width in COLUMN_WIDTHS.items()\n",
    "    ]\n",
    "    rename_request['requests'].extend(width_requests)\n",
    "    \n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=rename_request\n",
    "    )\n",
    "    \n",
    "    followup_worksheet_ids[followup_sheet_name] = copied_sheet_id\n",
    "\n",
    "    logging.info(f\"Copied, renamed, and set column widths for sheet '{followup_sheet_name}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error copying sheet '{followup_sheet_name}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c8cc0",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "HttpError                                 Traceback (most recent call last)\n",
    "Cell In[52], line 161\n",
    "    153 sheet_id = followup_worksheet_ids[followup_sheet_name]\n",
    "    154 delete_request = {\n",
    "    155     'requests': [{\n",
    "    156         'deleteSheet': {\n",
    "   (...)    159     }]\n",
    "    160 }\n",
    "--> 161 execute_with_backoff(\n",
    "    162     sheets_service.spreadsheets().batchUpdate,\n",
    "    163     spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "    164     body=delete_request\n",
    "    165 )\n",
    "    167 try:   \n",
    "    168     del followup_worksheet_ids[followup_sheet_name]\n",
    "\n",
    "Cell In[9], line 44, in execute_with_backoff(func, max_retries, *args, **kwargs)\n",
    "     42     result=func(*args,**kwargs)\n",
    "     43     if isinstance(result,HttpRequest):\n",
    "---> 44         result=result.execute()\n",
    "     45     return result\n",
    "     46 except(HttpError,gspread.exceptions.APIError) as e:\n",
    "\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\googleapiclient\\_helpers.py:130, in positional.<locals>.positional_decorator.<locals>.positional_wrapper(*args, **kwargs)\n",
    "    128     elif positional_parameters_enforcement == POSITIONAL_WARNING:\n",
    "    129         logger.warning(message)\n",
    "--> 130 return wrapped(*args, **kwargs)\n",
    "\n",
    "File c:\\Users\\nicola\\Desktop\\VisualCode Workspace\\.venv\\Lib\\site-packages\\googleapiclient\\http.py:938, in HttpRequest.execute(self, http, num_retries)\n",
    "    936     callback(resp)\n",
    "    937 if resp.status >= 300:\n",
    "--> 938     raise HttpError(resp, content, uri=self.uri)\n",
    "    939 return self.postproc(resp, content)\n",
    "\n",
    "HttpError: <HttpError 400 when requesting https://sheets.googleapis.com/v4/spreadsheets/13IpNi1rFg8luMX8t_OlhCbAQcEhkZkZUmwFWP3C_8N8:batchUpdate?alt=json returned \"Invalid requests[0].deleteSheet: You can't remove all the visible sheets in a document.\". Details: \"Invalid requests[0].deleteSheet: You can't remove all the visible sheets in a document.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff42338",
   "metadata": {},
   "source": [
    "## TROUBLESHOOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f37b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NEWLY ADDED CODE\"\"\"\n",
    "\n",
    "# Fetch spreadsheet metadata\n",
    "spreadsheet = sheets_service.spreadsheets().get(spreadsheetId=FOLLOWUP_SPREADSHEET_ID).execute()\n",
    "sheets = spreadsheet.get('sheets', [])\n",
    "\n",
    "# Step 1: Unhide all hidden sheets\n",
    "unhide_requests = []\n",
    "for sheet in sheets:\n",
    "    sheet_properties = sheet['properties']\n",
    "    if sheet_properties.get('hidden', False):  # Check if sheet is hidden\n",
    "        unhide_requests.append({\n",
    "            'updateSheetProperties': {\n",
    "                'properties': {\n",
    "                    'sheetId': sheet_properties['sheetId'],\n",
    "                    'hidden': False\n",
    "                },\n",
    "                'fields': 'hidden'\n",
    "            }\n",
    "        })\n",
    "\n",
    "if unhide_requests:\n",
    "    unhide_request_body = {'requests': unhide_requests}\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=unhide_request_body\n",
    "    )\n",
    "    print(f\"Unhid {len(unhide_requests)} sheet(s).\")\n",
    "\n",
    "# Step 2: Check the number of visible sheets\n",
    "visible_sheets = [sheet for sheet in sheets if not sheet['properties'].get('hidden', False)]\n",
    "if len(visible_sheets) <= 1:\n",
    "    # Create a new sheet to ensure at least one visible sheet remains after deletion\n",
    "    create_request = {\n",
    "        'requests': [{\n",
    "            'addSheet': {\n",
    "                'properties': {\n",
    "                    'title': 'Temporary Sheet'\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=create_request\n",
    "    )\n",
    "    print(\"Created a new sheet to ensure at least one visible sheet remains.\")\n",
    "\n",
    "\n",
    "\"\"\"END OF NEWLY ADDED CODE\"\"\"\n",
    "\n",
    "spreadsheet = sheets_service.spreadsheets().get(spreadsheetId=FOLLOWUP_SPREADSHEET_ID).execute()\n",
    "sheets = spreadsheet.get('sheets', [])\n",
    "visible_sheets = [sheet for sheet in sheets if not sheet['properties'].get('hidden', False)]\n",
    "\n",
    "if len(visible_sheets) > 1:\n",
    "    delete_request = {\n",
    "        'requests': [{\n",
    "            'deleteSheet': {\n",
    "                'sheetId': followup_worksheet_ids[followup_sheet_name]\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=delete_request\n",
    "    )\n",
    "    del followup_worksheet_ids[followup_sheet_name]\n",
    "else:\n",
    "    print(f\"Cannot delete sheet '{followup_sheet_name}' as it is the last visible sheet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976eba9",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[62], line 62\n",
    "     56 visible_sheets = [sheet for sheet in sheets if not sheet['properties'].get('hidden', False)]\n",
    "     58 if len(visible_sheets) > 1:\n",
    "     59     delete_request = {\n",
    "     60         'requests': [{\n",
    "     61             'deleteSheet': {\n",
    "---> 62                 'sheetId': followup_worksheet_ids[followup_sheet_name]\n",
    "     63             }\n",
    "     64         }]\n",
    "     65     }\n",
    "     66     execute_with_backoff(\n",
    "     67         sheets_service.spreadsheets().batchUpdate,\n",
    "     68         spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "     69         body=delete_request\n",
    "     70     )\n",
    "     71     del followup_worksheet_ids[followup_sheet_name]\n",
    "\n",
    "KeyError: '24 Apr - 30 Apr'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e86d3",
   "metadata": {},
   "source": [
    "Cannot delete sheet '24 Apr - 30 Apr' as it is the last visible sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433ac01",
   "metadata": {},
   "source": [
    "<strong> adding a step to first unhide all sheets </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Fetch spreadsheet metadata\n",
    "spreadsheet = sheets_service.spreadsheets().get(spreadsheetId=FOLLOWUP_SPREADSHEET_ID).execute()\n",
    "sheets = spreadsheet.get('sheets', [])\n",
    "\n",
    "# Step 1: Unhide all hidden sheets\n",
    "unhide_requests = []\n",
    "for sheet in sheets:\n",
    "    sheet_properties = sheet['properties']\n",
    "    if sheet_properties.get('hidden', False):  # Check if sheet is hidden\n",
    "        unhide_requests.append({\n",
    "            'updateSheetProperties': {\n",
    "                'properties': {\n",
    "                    'sheetId': sheet_properties['sheetId'],\n",
    "                    'hidden': False\n",
    "                },\n",
    "                'fields': 'hidden'\n",
    "            }\n",
    "        })\n",
    "\n",
    "if unhide_requests:\n",
    "    unhide_request_body = {'requests': unhide_requests}\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=unhide_request_body\n",
    "    )\n",
    "    print(f\"Unhid {len(unhide_requests)} sheet(s).\")\n",
    "\n",
    "    # Refresh spreadsheet metadata after update\n",
    "    spreadsheet = sheets_service.spreadsheets().get(spreadsheetId=FOLLOWUP_SPREADSHEET_ID).execute()\n",
    "    sheets = spreadsheet.get('sheets', [])\n",
    "\n",
    "# Step 2: Ensure there's at least one visible sheet\n",
    "visible_sheets = [sheet for sheet in sheets if not sheet['properties'].get('hidden', False)]\n",
    "if len(visible_sheets) <= 1:\n",
    "    # Create a uniquely named temporary sheet\n",
    "    temp_sheet_title = f\"Temporary Sheet {uuid.uuid4().hex[:6]}\"\n",
    "    create_request = {\n",
    "        'requests': [{\n",
    "            'addSheet': {\n",
    "                'properties': {\n",
    "                    'title': temp_sheet_title\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=create_request\n",
    "    )\n",
    "    print(f\"Created a new sheet '{temp_sheet_title}' to ensure at least one visible sheet remains.\")\n",
    "\n",
    "    # Refresh spreadsheet metadata again\n",
    "    spreadsheet = sheets_service.spreadsheets().get(spreadsheetId=FOLLOWUP_SPREADSHEET_ID).execute()\n",
    "    sheets = spreadsheet.get('sheets', [])\n",
    "\n",
    "# Step 3: Attempt to delete the follow-up sheet\n",
    "visible_sheets = [sheet for sheet in sheets if not sheet['properties'].get('hidden', False)]\n",
    "\n",
    "if followup_sheet_name not in followup_worksheet_ids:\n",
    "    print(f\"Sheet '{followup_sheet_name}' not found in worksheet IDs. Aborting deletion.\")\n",
    "else:\n",
    "    if len(visible_sheets) > 1:\n",
    "        delete_request = {\n",
    "            'requests': [{\n",
    "                'deleteSheet': {\n",
    "                    'sheetId': followup_worksheet_ids[followup_sheet_name]\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "            body=delete_request\n",
    "        )\n",
    "        del followup_worksheet_ids[followup_sheet_name]\n",
    "        print(f\"Deleted sheet '{followup_sheet_name}'.\")\n",
    "    else:\n",
    "        print(f\"Cannot delete sheet '{followup_sheet_name}' as it is the last visible sheet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da697ca2",
   "metadata": {},
   "source": [
    "Sheet '24 Apr - 30 Apr' not found in worksheet IDs. Aborting deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080120f",
   "metadata": {},
   "source": [
    "<strong>CONTINUE WITH CODE to copy from current and add comments column</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea39b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if followup_sheet_name in current_worksheet_ids and followup_sheet_name in followup_worksheet_ids:\n",
    "    logging.info(f\"Worksheet '{followup_sheet_name}' already exists. Deleting it.\")\n",
    "    sheet_id = followup_worksheet_ids[followup_sheet_name]\n",
    "    delete_request = {\n",
    "        'requests': [{\n",
    "            'deleteSheet': {\n",
    "                'sheetId': sheet_id\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=delete_request\n",
    "    )\n",
    "    \n",
    "    try:   \n",
    "        del followup_worksheet_ids[followup_sheet_name]\n",
    "        time.sleep(1)\n",
    "        updated_followup_worksheets = execute_with_backoff(followup_spreadsheet.worksheets)\n",
    "        updated_followup_worksheet_ids = {sheet.title: sheet.id for sheet in updated_followup_worksheets}\n",
    "        \n",
    "        if followup_sheet_name in updated_followup_worksheet_ids:\n",
    "            raise Exception(f\"Failed to delete worksheet '{followup_sheet_name}'.\")\n",
    "        logging.info(f\"Deleted worksheet '{followup_sheet_name}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error deleting worksheet '{followup_sheet_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    copy_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().sheets().copyTo,\n",
    "        spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "        sheetId=current_worksheet_ids[followup_sheet_name],\n",
    "        body={'destinationSpreadsheetId': FOLLOWUP_SPREADSHEET_ID})\n",
    "        \n",
    "    copied_sheet_id = copy_response['sheetId']\n",
    "    rename_request = {\n",
    "        'requests': [{\n",
    "            'updateSheetProperties': {\n",
    "                'properties': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'title': followup_sheet_name\n",
    "                },\n",
    "                'fields': 'title'\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    # Add column width updates to the rename request\n",
    "    width_requests = [\n",
    "        {\n",
    "            'updateDimensionProperties': {\n",
    "                'range': {\n",
    "                    'sheetId': copied_sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': col_index,\n",
    "                    'endIndex': col_index + 1\n",
    "                },\n",
    "                'properties': {\n",
    "                    'pixelSize': width\n",
    "                },\n",
    "                'fields': 'pixelSize'\n",
    "            }\n",
    "        } for col_index, width in COLUMN_WIDTHS.items()\n",
    "    ]\n",
    "    rename_request['requests'].extend(width_requests)\n",
    "    \n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=rename_request\n",
    "    )\n",
    "    \n",
    "    followup_worksheet_ids[followup_sheet_name] = copied_sheet_id\n",
    "\n",
    "    logging.info(f\"Copied, renamed, and set column widths for sheet '{followup_sheet_name}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error copying sheet '{followup_sheet_name}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c9149",
   "metadata": {},
   "source": [
    "2025-05-23 10:00:50,315 - INFO - Copied, renamed, and set column widths for sheet '24 Apr - 30 Apr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec722369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_index_to_letter(index):\n",
    "    result = ''\n",
    "    index += 1\n",
    "    while index:\n",
    "        index, rem = divmod(index - 1, 26)\n",
    "        result = chr(65 + rem) + result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id = followup_worksheet_ids[followup_sheet_name]\n",
    "\n",
    "try:\n",
    "    # Step 1: Get sheet data to find the last column\n",
    "    spreadsheet = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().get,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        ranges=[f\"{followup_sheet_name}!A1:ZZ1\"],\n",
    "        includeGridData=True\n",
    "    )\n",
    "\n",
    "    sheet_data = next((s for s in spreadsheet['sheets'] if s['properties']['sheetId'] == sheet_id), None)\n",
    "    if not sheet_data:\n",
    "        raise Exception(f\"Sheet '{followup_sheet_name}' not found in spreadsheet.\")\n",
    "\n",
    "    new_column_index = 10  # Column K\n",
    "    new_column_letter = 'K'\n",
    "\n",
    "    # Step 2: Insert a new column\n",
    "    insert_column_request = {\n",
    "        'requests': [{\n",
    "            'insertDimension': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'dimension': 'COLUMNS',\n",
    "                    'startIndex': new_column_index,\n",
    "                    'endIndex': new_column_index + 1\n",
    "                },\n",
    "                'inheritFromBefore': True\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body=insert_column_request\n",
    "    )\n",
    "    logging.info(f\"Inserted new column at index {new_column_index} ({new_column_letter}) in '{followup_sheet_name}'.\")\n",
    "\n",
    "    # Step 3: Set the header to \"Comments\"\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().values().update,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        range=f\"{followup_sheet_name}!{new_column_letter}1\",\n",
    "        valueInputOption='RAW',\n",
    "        body={'values': [['Comments']]}\n",
    "    )\n",
    "    logging.info(f\"Set header 'Comments' in column {new_column_letter} of '{followup_sheet_name}'.\")\n",
    "\n",
    "    # Step 4: Get formatting from column F (entire column)\n",
    "    formatting_response = execute_with_backoff(\n",
    "        sheets_service.spreadsheets().get,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        ranges=[f\"{followup_sheet_name}!F:F\"],\n",
    "        fields='sheets.data.rowData.values.userEnteredFormat'\n",
    "    )\n",
    "\n",
    "    format_data = formatting_response['sheets'][0]['data'][0].get('rowData', [])\n",
    "    if not format_data:\n",
    "        logging.warning(f\"No formatting data found for column F in '{followup_sheet_name}'. Applying default formatting.\")\n",
    "        column_format = {}\n",
    "    else:\n",
    "        column_format = format_data[0]['values'][0].get('userEnteredFormat', {}) if format_data[0].get('values') else {}\n",
    "\n",
    "    formatting_requests = []\n",
    "    # Copy formatting for each row in column F\n",
    "    for row_index, row in enumerate(format_data):\n",
    "        cell = row.get('values', [])\n",
    "        row_format = cell[0].get('userEnteredFormat', column_format) if cell else column_format\n",
    "\n",
    "        # Prepare the request to copy complete formatting including borders\n",
    "        formatting_requests.append({\n",
    "            'repeatCell': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'startRowIndex': row_index,\n",
    "                    'endRowIndex': row_index + 1,\n",
    "                    'startColumnIndex': new_column_index,\n",
    "                    'endColumnIndex': new_column_index + 1\n",
    "                },\n",
    "                'cell': {\n",
    "                    'userEnteredFormat': row_format\n",
    "                },\n",
    "                'fields': 'userEnteredFormat(backgroundColor,textFormat,borders,padding,horizontalAlignment,verticalAlignment,wrapStrategy,numberFormat)'\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Apply all formatting requests in one batch\n",
    "    if formatting_requests:\n",
    "        execute_with_backoff(\n",
    "            sheets_service.spreadsheets().batchUpdate,\n",
    "            spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "            body={'requests': formatting_requests}\n",
    "        )\n",
    "        logging.info(f\"Applied complete formatting including borders from column F to column K in '{followup_sheet_name}'.\")\n",
    "    else:\n",
    "        logging.warning(\"No formatting data found in column F.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to add Comments column to '{followup_sheet_name}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a40d0a",
   "metadata": {},
   "source": [
    "2025-05-23 09:41:39,066 - INFO - Inserted new column at index 10 (K) in '24 Apr - 30 Apr'.\n",
    "2025-05-23 09:41:40,196 - INFO - Set header 'Comments' in column K of '24 Apr - 30 Apr'.\n",
    "2025-05-23 09:41:43,781 - INFO - Applied complete formatting including borders from column F to column K in '24 Apr - 30 Apr'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5987177",
   "metadata": {},
   "source": [
    "## hide sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde10bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hide_requests(worksheet_ids: Dict[str, int], sheets_to_keep_visible: List[str], spreadsheet_id: str) -> List[Dict[str, Any]]:\n",
    "    gc, _ = authenticate_gsheets(return_gspread=True) \n",
    "    _, sheets_service = authenticate_gsheets() #build('sheets', 'v4', credentials=creds)\n",
    "    \n",
    "    try:\n",
    "        metadata = execute_with_backoff(\n",
    "            sheets_service.spreadsheets().get,\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            fields='sheets.properties'\n",
    "        )\n",
    "        logging.info(f\"Metadata retrieved: {metadata.keys()}\")\n",
    "        current_sheets = metadata.get('sheets', [])\n",
    "\n",
    "        hide_requests = []\n",
    "        for sheet in current_sheets:\n",
    "            properties = sheet.get('properties', {})\n",
    "            title = properties.get('title')\n",
    "            sheet_id = properties.get('sheetId')\n",
    "            is_hidden = properties.get('hidden', False)\n",
    "\n",
    "            if not title or sheet_id is None:\n",
    "                logging.warning(f\"Skipping invalid sheet: {properties}\")\n",
    "                continue\n",
    "\n",
    "            if title not in sheets_to_keep_visible and not is_hidden:\n",
    "                hide_requests.append({\n",
    "                    \"updateSheetProperties\": {\n",
    "                        \"properties\": {\n",
    "                            \"sheetId\": sheet_id,\n",
    "                            \"hidden\": True\n",
    "                        },\n",
    "                        \"fields\": \"hidden\"\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        total_sheets = len(current_sheets)\n",
    "        if len(hide_requests) >= total_sheets:\n",
    "            logging.warning(f\"Prevented hiding all sheets in spreadsheet {spreadsheet_id}.\")\n",
    "            hide_requests = hide_requests[:-1] if hide_requests else []\n",
    "\n",
    "        return hide_requests\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch or process current sheet visibility: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ce624",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_visible_sheets = [next_sheet_name]\n",
    "currentworksheet_requests = filter_hide_requests(current_worksheet_ids, current_visible_sheets, CURRENT_SPREADSHEET_ID)\n",
    "\n",
    "if currentworksheet_requests :\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=CURRENT_SPREADSHEET_ID,\n",
    "        body={\"requests\": currentworksheet_requests }\n",
    "    )\n",
    "    logging.info(f\"Hid {len(currentworksheet_requests )} sheets in current spreadsheet.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14188e92",
   "metadata": {},
   "source": [
    "2025-05-23 09:42:35,568 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:42:38,080 - INFO - Metadata retrieved: dict_keys(['sheets'])\n",
    "2025-05-23 09:42:39,178 - INFO - Hid 1 sheets in current spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_visible_sheets = [followup_sheet_name]\n",
    "followupworksheet_requests = filter_hide_requests(followup_worksheet_ids, followup_visible_sheets, FOLLOWUP_SPREADSHEET_ID)\n",
    "\n",
    "\n",
    "if followupworksheet_requests :\n",
    "    execute_with_backoff(\n",
    "        sheets_service.spreadsheets().batchUpdate,\n",
    "        spreadsheetId=FOLLOWUP_SPREADSHEET_ID,\n",
    "        body={\"requests\": followupworksheet_requests }\n",
    "    )\n",
    "    logging.info(f\"Hid {len(followupworksheet_requests )} sheets in followup spreadsheet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1159e",
   "metadata": {},
   "source": [
    "2025-05-23 09:42:39,235 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
    "2025-05-23 09:42:41,772 - INFO - Metadata retrieved: dict_keys(['sheets'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
